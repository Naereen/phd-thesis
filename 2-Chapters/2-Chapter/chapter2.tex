%!TEX root = ../PhD_thesis__Lilian_Besson

% First chapter begins here
\chapter{The Single Player and Stationary Multi-Armed Bandits Model}
\label{chapter:2}
\minitoc


\TODOL{L'abstract ici est trop long, mais je l'ai écrit avant de commencer le chapitre, pour avoir une idée de ce que je devais faire ! Je couperai à la fin !}

\paragraph{Abstract.}
%
In this chapter, we present the first model of multi-armed bandit (MAB) studied in this thesis,
the single-player and stationary stochastic MAB model.
MAB models a decision-making game, where a \emph{player} has to sequentially select one action in a (usually finite) set of actions, and only receives a (random) feedback about the action she selected, also called \emph{reward}.
Her goal is to maximize its sum of received rewards, from time $t=1$ to $t=T$ if the game is played at an horizon $T$. The difficulty comes from balancing the trade-off between \emph{exploration}, because she has to observe as much as possible all the arms to get more knowledge about the distributions of their rewards, and \emph{exploitation}, because she also has to select as much as possible the best arm.
%
We focus on the model with finite arms, and binary or real-valued stochastic rewards.
After introducing formally the notations used in all this thesis, for arms, distributions, rewards etc, we review shortly various possible applications of multi-armed bandits.
The focus of this work is on cognitive radio and IoT networks, where arms can represent wireless orthogonal channels, but more generally any resource characterizing the communication of a device to a gateway (\eg, spreading factor for LoRa, power allocation for NOMA etc).

We explain the measure of performance used in this thesis, defined as the regret and written $R_T^{\cA}$ for algorithm $\cA$ and horizon $T$. The regret compares the performance of the decision-making player and the oracle who knows in advance the best arm and always selects it.
Other measures of performance, such as best-arm identification, are not used later on in this document, but they are also shortly discussed.
%
We give an overview of the main families of algorithms designed to tackle the stochastic MAB model, and we mainly focus on the \UCB{} and \klUCB{} algorithms, as they are the two we used in the next chapters.
Part of the work presented in Chapter~\ref{chapter:4} also used the Thompson sampling algorithm, but no theoretical analyses were done about it.
When presenting our library SMPyBandits in Chapter~\ref{chapter:3} our library SMPyBandits, designed to perform numerical simulations of MAB problem, we use it to compare the empirical performances of the most famous and most efficient algorithms, on a few different problems.
%
In the point-of-view of practical simulations, one must also take care of two other measures of performance: time and memory complexity, which can vary greatly between different families of algorithms.
We also give an empirical comparison of different algorithms for these two measures in Chapter~\ref{chapter:3}.

We then review the main theoretical results from famous previous works, that give a lower-bound on the regret of any algorithm, essentially showing that we cannot find an algorithm achieving a sub-logarithmic regret, in this stochastic MAB model.
We also gives regret upper-bounds of the two main algorithms used in this thesis, \UCB{} and \klUCB, to highlight the difference between their finite-time regret upper-bounds. They both attain a logarithmic regret, proving their order optimality (\ie, their regret upper-bound asymptotically matches the lower-bound), moreover \klUCB{} is known to be optimal for a wide range of arms distributions.
A sketch of the proof of both results is given in Appendix.

Finally, we explore the problem of algorithm selection: as there is many different algorithms, all having different qualities and properties, how one can decide which algorithm to use for a particular application?
We present one of the first contribution made during my PhD \cite{Besson2018WCNC}, consisting of an efficient algorithm for aggregating algorithms. We illustrate its performance empirically, but no satisfactory theoretical result is given, as the bound we hoped for have been proved unachievable since then.


\paragraph{Publications.}
%
Section~\ref{sec:2:chooseYourPreferredBanditAlgorithm} in this chapter is based on the following publication: \cite{Besson2018WCNC}.
The notations used in this thesis are the usual notations in all the MAB literature, and we follow closely the formalism of \cite{Kaufmann12PhD}, as well as \cite{Slivkins2019,LattimoreBanditAlgorithmsBook,Bubeck12}.


\paragraph{Main references for the curious reader.}
%
For more details and a more pedagogical introduction to multi-armed bandits, we suggest to the interested reader to read the short text-book \cite{Slivkins2019}.
Another excellent but reasonably short survey is \cite{Bubeck12}, while the more recent book \cite{LattimoreBanditAlgorithmsBook} is the most complete resource about bandit algorithms.


% \newpage
% Write miniTOC just after the title
\graphicspath{{2-Chapters/2-Chapter/Images/}}

% ----------------------------------------------------------------------------
\section{Stationary MAB models}
\label{sec:2:notations}

% ----------------------------------------------------------------------------
\subsection{Finite arms and stochastic rewards}

In this thesis, we only consider models with finite arms.
No linear bandits, no contextual bandits etc.

\TODOL{Write this!}

For now, we only consider stochastic rewards. Piece-wise stochastic models are studied in Chapter~\ref{chapter:6}.


% ----------------------------------------------------------------------------
\subsection{Different hypotheses on rewards distributions}

Everything is implemented and documented at
https://smpybandits.github.io/docs/Arms.html

Bernoulli, Gaussian, sub-Bernoulli, sub-Gaussian, Exponential, sub-Exponential...

One dimensional exponential family...

Just bounded, in $[a,b]$ but without loss of generality in $[0,1]$ etc.

Markov models: presented later in Section~\ref{sec:3:markovModels}.


% ----------------------------------------------------------------------------
\section{Applications of stationary MAB}
\label{sec:2:applicationsofStationaryMAB}

\subsection{Various possible applications}

- clinical trial
- A/B testing of websites
- online content recommandation
- wireless communication, lots of possibilities, one is frequency/channel selection in the OSA paradigm, detailed after
- hyper-parameters tuning of deep neural networks etc


Citation: \cite{bouneffouf2019survey}


\subsection{Application for Opportunistic Spectrum Access}

Detail the analogy arm = channel in a licensed spectrum, rewards $r_k(t)$ = 1 if no Primary User or 0 if channel $k$ is busy at time $t$.

Detail the previous work from our team SCEE on bandits + OSA : Wassim, Navik


% ----------------------------------------------------------------------------
\section{Review of MAB algorithms}
\label{sec:2:famousMABalgorithms}

Everything is implemented and documented at
https://smpybandits.github.io/docs/Policies.html


\subsubsection{Why do we need smart algorithms?}
Why Follow-the-Leader (EmpiricalMeans https://smpybandits.github.io/docs/Policies.EmpiricalMeans.html) don't work, example.

\subsection{Simple but efficient algorithms when well tuned: $\varepsilon$-greedy and Explore-then-Commit}

See
https://smpybandits.github.io/docs/Policies.EpsilonGreedy.html
https://smpybandits.github.io/docs/Policies.ExploreThenCommit.html


% ----------------------------------------------------------------------------
\subsection{Index policies: UCB and others}

\subsubsection{UCB}
Optimism in face of uncertainty, for UCB etc.


\subsubsection{klUCB}
Optimal upper confidence bounds based on Kullback-Leibler divergence and klUCB indexes

\subsubsection{Other policies inspired by UCB}

- UCB-H and klUCB-H
- UCB-V
- UCB-dagger
- UCBoost
- UCB+ and klUCB+


% ----------------------------------------------------------------------------
\subsection{Bayesian policies: Thompson Sampling and others}

Idea behind Thompson sampling and Bayesian philosophy.

The Beta posterior and others (https://smpybandits.github.io/docs/Policies.Posterior.html) and Thompson Sampling.


% ----------------------------------------------------------------------------
\subsection{Other policies: BESA and others}

It's a good opportunity to show off and say that I worked on my own on some non-standard policies.

- One of the first paper I read in 2016 was defining AdBandits, https://smpybandits.github.io/docs/Policies.AdBandits.html, a mixture between Thompson sampling and BayesUCB (very efficient in practice)
- I especially love the BESA algorithm by [? and ? and Maillard].


% ----------------------------------------------------------------------------
\subsection{Policies for adversarial bandits: Exp3 and others}

Exp3, Exp3++ etc
https://smpybandits.github.io/docs/Policies.Exp3.html
https://smpybandits.github.io/docs/Policies.Exp3PlusPlus.html

Softmax: is it the same, but written differently?
https://smpybandits.github.io/docs/Policies.Softmax.html


% ----------------------------------------------------------------------------
\subsection{Hybrid policies: MOSS, klUCB++ and klUCB-Switch and others}

Recent work.
https://smpybandits.github.io/docs/Policies.MOSS.html
https://smpybandits.github.io/docs/Policies.IMED.html
https://smpybandits.github.io/docs/Policies.klUCBPlusPlus.html
https://smpybandits.github.io/docs/Policies.klUCBSwitch.html

Talk about these?
https://smpybandits.github.io/docs/Policies.OCUCB.html
https://smpybandits.github.io/docs/Policies.OSSB.html

With perturbation: maybe we don't need optimism in face of uncertainty, we just need uniform sampling!
Randomized Confidence Bound (RCB) with uniform perturbations performs as well as UCB!
https://smpybandits.github.io/docs/Policies.RandomizedIndexPolicy.html


\subsection{Summary of classification of the most famous algorithms}

A huge table in one page in portrait mode, giving:
- name
- reference, year
- family, classification
- time/memory complexity ?
- regret bound for stochastic regret

of about 20/30 algorithms?


% ----------------------------------------------------------------------------
\section{Lower and upper bounds on regret}
\label{sec:2:lowerUpperBoundsRegret}


\subsection{Measuring performance with the (mean) regret}
Explain what is regret


\subsection{Regret lower bounds: do not detail too much, don't explain the tools behind the results}

- [Lai and Robbins] lower-bound in $\Omega(\log(T))$
- Worst case lower-bound in $\Omega(\sqrt{T})$
- Adversarial lower-bound in $\Omega(\sqrt{T})$ (also useful for piece-wise stationary models)


\subsection{Regret upper bounds, tools and different kinds of bounds}

- useful decomposition, with number of samples (see Claire's PhD)
- different kinds of bounds, problem-dependent with $\bigO{C \log(T)}$ with $C$ a measure of the problem complexity, or problem-independent with $\bigO{\sqrt{KT}}$ ?
- tools to show regret upper-bounds, concentration inequalities,
- short proof of UCB or klUCB? just to start to show the tools


\subsection{Other measures of performances}

- Quantile regret or just histogram of regrets
- Worst case regret (max $R_T^{r}$ for $r$ index of Monte-Carlo simulation?)
- Fairness


% ----------------------------------------------------------------------------
\section{Different approaches on algorithm selection}
\label{sec:2:chooseYourPreferredBanditAlgorithm}

\subsection{Naive approach: use UCB or Thompson sampling, and forget about the rest!}
First solution: be naive, only use UCB everywhere

\subsection{Use prior knowledge about the future application to select beforehand the chosen algorithm}
Second solution, if one has some prior knowledge about the domain or setting for which the learning algorithms will be deployed, she can run synthetic or real-world simulations, compare many algorithms before deployment, and select the best algorithm!

\subsection{Online algorithm selection with expert aggregation}

\input{2-Chapters/2-Chapter/IEEE_WCNC_2018.git/IEEE_WCNC__2018__Paper__Lilian_Besson__07-17.tex}

% A last solution is online algorithm selection, inspired from expert aggregation.
% Include here the discussion about expert aggregation and my \textbf{Aggregator} algorithm, see https://hal.inria.fr/hal-01705292



% ----------------------------------------------------------------------------
\section{Conclusion}
\label{sec:2:conclusion}

In this chapter, presented 

\TODOL{Write this!}



% ----------------------------------------------------------------------------
\section{Appendix}
\label{sec:2:appendix}

\TODOL{Write this!}

\subsection{Efficient numerical computation of Kullback-Leibler divergences and \klUCB{} indexes}

I want to detail the maths behind an implementation of klUCB, see https://smpybandits.github.io/docs/Policies.kullback.html and all what is detailed there.

How to write a fast version of klBern and klucbBern (the two most used functions), using C or Numba or Cython, maybe I can include the speed up simulations here? First piece of code, before really presenting SMPyBandits ?

My work for this part is also published on https://github.com/Naereen/Kullback-Leibler-divergences-and-kl-UCB-indexes
(in Python) and https://github.com/Naereen/KullbackLeibler.jl (in Julia).

