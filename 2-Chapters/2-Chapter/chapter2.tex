%!TEX root = ../PhD_thesis__Lilian_Besson

% First chapter begins here
\chapter{Stochastic Multi-Armed Bandits Models}
\label{chapter:2}

\TODOL{Abstract ici et plus court, et petite introduction !}

\minitoc
% Write miniTOC just after the title
\graphicspath{{2-Chapters/2-Chapter/Images/}}

\newpage

% \TODOL{L'abstract ici est trop long, mais je l'ai écrit avant de commencer le chapitre, pour avoir une idée de ce que je devais faire ! Je couperai à la fin !}

\paragraph{Abstract.}
%
In this chapter, we present the common base of all the models studied in this thesis:
the multi-armed bandit (MAB) model,
restricting to the single-player and stationary stochastic case.
We focus on decision-making models with a finite number of resources, called arms, and on stochastic models, where an arm is associated with a one-dimensional distribution.
In all this thesis, we only consider one-dimensional exponential families, and in particular Bernoulli distributions.
%
We quickly review the possible applications of bandits, that range from recommender systems and information retrieval to healthcare and finance. In the rest of this thesis, we focus on cognitive radio, an important field of research where bandits has already been applied with success in the last ten years.

We then explain the main measure of performance used in this thesis, defined as the regret and written $R_T^{\cA}$ for algorithm $\cA$ and for horizon $T$. The regret compares the performance of the decision-making player and the oracle who knows in advance the best arm and always selects it.
Other measures of performance, such as best-arm identification, are not used later on in this document.
% , but they are also shortly discussed.
%
We explain the main theoretical result from previous works: a lower-bound on the regret of any algorithm, essentially showing that no algorithm can achieve sub-logarithmic regret on all problems, in this stochastic MAB model (\ie, $R_T^{\cA} = \Omega(\log(T))$).

We give an overview of the main families of algorithms designed to tackle stochastic MAB problems, and we mainly focus on the \UCB, \klUCB{} and Thompson sampling algorithms, as they are used in the rest of the manuscript.
% We present SMPyBandits in the next Chapter~\ref{chapter:3}, a Python open-source library, designed to run numerical simulations of MAB problems, one of the main contributions of this thesis.
% We use it to compare the empirical performances of the most famous and most efficient algorithms, in terms of regret and by evaluating them on different problems.
% %
% In the point-of-view of numerical simulations or real-world applications of bandits on embedded systems, one must also take care of two other measures of performance: time and memory complexities, which can vary greatly between different families of algorithms.
% We also empirically compare different algorithms for these two measures in Section~\ref{sec:3:timeAndMemoryCosts}.
%
We then give regret upper-bounds of the two main algorithms used in this thesis, \UCB{} and \klUCB, to highlight the difference between their finite-time regret upper-bounds. They both attain a logarithmic regret, proving their order optimality (\ie, their regret upper-bound asymptotically matches the lower-bound up-to a constant factor).
% Moreover, \klUCB{} is known to be optimal for a wide range of arms distributions.
% A sketch of the proof of both results is given in Appendix.

Finally, we explore the problem of algorithm selection: as there are many different algorithms, all having different qualities and properties, how one can decide which algorithm to use for a particular application?
We present one of the first contributions made during my PhD \cite{Besson2018WCNC}, consisting of an efficient algorithm for aggregating algorithms,
and we illustrate its performance empirically.
% , but no satisfactory theoretical result is given, as the bound we had hoped for has been proved unachievable since then.


\paragraph{Publication.}
%
The Section~\ref{sec:2:chooseYourPreferredBanditAlgorithm} in this chapter is based on the article \cite{Besson2018WCNC}.


\paragraph{Main references for the curious reader.}
%
For more details and a more formal introduction to multi-armed bandits, we suggest to the interested reader to work on a very recent text-book by Slivkins \cite{Slivkins2019}.
Another excellent but reasonably short survey is the book by Bubeck and Cesa-Bianchi \cite{Bubeck12}, while the more recent book by Lattimore and Szepesv{\'a}ri \cite{LattimoreBanditAlgorithmsBook} is the most complete resource about bandit algorithms.
Finally, we recommend \cite{bouneffouf2019survey} for a short but good survey on applications of MAB.

The notations used in this thesis are the usual notations in all the MAB literature, and we follow closely the formalism of \cite{Kaufmann12PhD}, as well as \cite{Slivkins2019,LattimoreBanditAlgorithmsBook,Bubeck12}.
The Nomenclature includes a summary of the notations, see Page~\pageref{chapter:nomenclature}.


% \newpage


% ----------------------------------------------------------------------------
\section{The stochastic Multi-Armed Bandit models}
\label{sec:2:notations}


Multi-Armed Bandits (MAB) models were introduced by Thompson as early as in 1933 \cite{Thompson33}, and later studied from the 1950s by Robbins \cite{Robbins52} and others.
This family of models was first proposed for clinical trials, and later applied to a wide range of different problems.
Their name refers to one-armed bandits found in casinos, as showed in Figure~\ref{fig:2:example_of_a_5_arm_bandit_problem} below.
% The seminal paper by Lai and Robbins \cite{LaiRobbins85} was the first to introduce the measure of performance widely used in the literature, the notion of \emph{regret}, and to analyze a lower-bound on the performance of any algorithm against a fixed problem.
% Since then, many efficient algorithms were proposed, XXX


A MAB is a model of a decision-making game, where a \emph{player} has to sequentially select one action in a (usually finite) set of actions, and only receives a (random) feedback about the selected action, also called a \emph{reward}.
We consider $K\in\N, K\geq2$ arms, and discrete times $t\in\N^*$.
When the player decides at time $t$ to play (or pull) the arm $A(t)\in\{1,\dots,K\}=[K]$, she receives a reward $r_{A(t)}(t)\in\R$, and so on for a finite number of steps $t$, from $t=1$ until $t=T$ for an horizon $T$.
%
A commonly studied goal for the player is to maximize its sum of received rewards, $\sum_{t=1}^T r_{A(t)}(t)$
(or its mean in stochastic and stationary models).


The difficulty of this decision-making game comes from balancing the trade-off between \emph{exploration}, because the player has to observe as much as possible all the arms to get more knowledge about the distributions of their rewards, and \emph{exploitation}, because she also has to select as much as possible the best arm.
The MAB model is a famous example of a reinforcement learning model, where the decision-making process has to adapt to an unknown environment using noisy observations and a discrete time, alternating between decisions and observations.
We illustrate this cycle in Figure~\ref{fig:2:ReinforcementLearningCycleMABmodel} below.
For more details on reinforcement learning, in particular about more generic models, we refer to the famous book \cite{SuttonBarto2018}.


\tikzstyle{block} = [align=center, draw, fill=gray!25, rectangle, minimum height=3em, minimum width=6em]
\begin{figure}[h!]
    \centering
    \resizebox{0.50\textwidth}{!}{
        \begin{tikzpicture}[auto, node distance=5cm, >=latex]
            %
            % We start by placing the blocks
            \node [block] (player) at (0,0) {Player};
            % We draw an edge between the player and system block to
            \node [block] (environment) at (4,0) {MAB problem};
            % Once the nodes are placed, connecting them is easy.
            \draw [->] (player) to[bend left=90] node[pos=0.5] {Chooses a discrete action $k=A(t) \in\{1,\dots,K\}$} (environment);
            \draw [->] (environment) to[bend left=90] node[pos=0.5] {Observes a real-valued reward $r_k(t) \in \R$} (player);
            %
        \end{tikzpicture}
    }
\caption{Reinforcement learning cycle in a MAB model, for time steps $t=1,\dots,T$.}
\label{fig:2:ReinforcementLearningCycleMABmodel}
\end{figure}


This quantity $\sum_{t=1}^T r_{A(t)}(t)$ can indeed be random, and usually depends on two aspects.
First, the rewards may depend on the randomness of the environment, \ie, the unknown and unpredictable values $r_k(t)$.
Then this quantity also depends on the player's decision-making policy, \ie, the choices $A(t)$, that are based on (all) the past observations, \ie, $A(1)$, $r_{A(1)}(1)$, $\dots$, $A(t-1)$, $r_{A(t-1)}(t-1)$, and possibly on an external source of randomness.
%
Without loss of generality, it can be given by a sequence $U_0,U_1,\dots$ of \iid{} random values, uniformly distributed on $[0,1]$, and that has to be independent from the rewards $r_k(t)$.
External randomness is used in most MAB algorithms, \eg, to break ties uniformly in an index policy.


% ----------------------------------------------------------------------------
\subsection{Finite arms and stochastic rewards}


In all this thesis, we focus on the model with finite arms, and binary or real-valued stochastic rewards, that is $A(t)\in\{1,\dots,K\}$ for a fixed and known number of arms $K\in\N,K\geq2$, and $r_{A(t)}(t)\in\R$.
%
% Restricting to models with a finite number of arms
% rules out an entire domain of the recent research on MAB, in particular we do not consider linear nor contextual bandits (for more details, see Parts V and VI of \cite{LattimoreBanditAlgorithmsBook}).
%
We also restrict to \emph{stochastic} rewards, that is we associate a real-valued distribution to each arm, denoted $\nu_k$ for arm $k\in\{1,\dots,\}=[K]$.
Rewards are stationary, meaning that $(r_k(t))_{t\in\N^*}$ is independent and identically distributed (\iid), $r_k(t) \sim \nu_k$ for any $t\geq1$.

The most common objective for the player is to maximize its expected rewards $\E[\sum_{t=1}^T Y_t] = \sum_{t=1}^T \E[r_{A(t)}(t)]$.
The expectancy is taken on the rewards $(r_k(t))_{k,t}$, and the extern random variables $(U_s)_s$.
%
Other common objectives include best-arm identification (BAI) \cite{audibert2010best}.


\paragraph{An interactive demo to discover the MAB problem (for the novice).}
\label{par:2:interactiveDemoDiscoverMAB}
%
If you are discovering the concept of bandit model here, I would like to recommend you to go online and play a few times with an interactive demonstration.
On this demo, you will be facing a MAB problem with $K=5$ arms, and you have $T=100$ decisions to make.
The demo is hosted on my website, at \href{https://perso.crans.org/besson/phd/MAB\_interactive\_demo/}{\texttt{perso.crans.org/besson/phd/MAB\_interactive\_demo/}}, and illustrated below.

% \begin{figure}[h!]  % [htbp]
%     \centering
%     \includegraphics[width=0.75\linewidth]{2-Chapters/2-Chapter/Images/example_of_a_5_arm_bandit_problem__step0.png}
%     \caption{See }
%     \label{fig:2:example_of_a_5_arm_bandit_problem__step0}
% \end{figure}

\begin{figure}[h!]  % [htbp]
    \centering
    \includegraphics[width=0.85\linewidth]{2-Chapters/2-Chapter/Images/example_of_a_5_arm_bandit_problem.png}
    \caption{Screenshot of the demonstration available online on my website, for a current step of $t=24$.}
    \label{fig:2:example_of_a_5_arm_bandit_problem}
\end{figure}


The webpage looks like Figure~\ref{fig:2:example_of_a_5_arm_bandit_problem} below.
The arms follow Bernoulli distributions, \ie, $\nu_k = \mathcal{B}(\mu_k)$, of unknown means $\mu_k\in[0,1]$, and your goal in this interactive demonstration is to obtain the highest possible cumulated reward in $100$ steps, $\sum_{t=1}^{100} r_{A(t)}(t)$.
Your decisions have to be made sequentially: at time $t$, you pick one of the arms, $A(t) \in\{1,2,3,4,5\}$, then the demo shows the random reward obtained from this (virtual) casino machine (in \textcolor{gold}{yellow}), \ie, the binary value $r_{A(t)}(t)\in\{0,1\}$, that is sampled \iid{} from $\nu_k$.
%
The UI of the demo also shows the current value of $t$ (``total plays'') and $\sum_{s=1}^t r_{A(s)}(s)$ the ``total reward''.
For each arm, we show the sum of rewards obtained from that arm, \ie, $X_k(t) = \sum_{s=1}^t r_k(s) \mathbbm{1}(A(s) = k)$, in the ``Rewards'' line, and the number of pulls of that arm, \ie, $N_k(t) = \sum_{s=1}^t \mathbbm{1}(A(s) = k)$ in the ``Pulls'' line.
%
The demo also shows the estimated probability of each arm, that is $\widehat{\mu_k}(t) = X_k(t) / N_k(t)$ (when $N_k(t)>0$), in the ``Estimated probs'' line.

In the first Figure~\ref{fig:2:example_of_a_5_arm_bandit_problem}, the current state of the game is shown at time $t=24$.
% Currently at time $t=24$ (out of $T=100$ total time steps),
At this step, the player has collected a sum of rewards of $14$, by observing $X(t) = [6,2,2,2,2]$ rewards of value $1$ in the $K=5$ different arms. Arms were sampled $N(t) = [8,4,4,4,4]$ times, meaning that the value $0$ was seen respectively $[2,2,2,2,2]$ times, and currently arm $1$ appears to be the best one. The true means of the arms are $\bm{\mu}=[0.6, 0.2, 0.55, 0.7, 0.5]$, and (much) more samples are needed before the player can accurately identify arm $4$ as the best arm.
%
In the second Figure~\ref{fig:2:example_of_a_5_arm_bandit_problem__step100} below, we display the result of an example of run, when the player was following the \UCB{} algorithm from \cite{Auer02}.
After $T=100$ steps, the player obtained a cumulated reward of $56$, by playing mostly arms $4,3,5,1,0$ (in decreasing order of number of plays). The empirical means $\widehat{\mu_k}(T)$ correctly identify the best arm (arm $4$), but do not correctly rank the arms as arms $1$ and $3$ obtained means of $\widehat{\mu_1}(T) = 0.5 < \widehat{\mu_3}(T)=0.6$ but the true means are $\mu_3 = 0.55 < \mu_1 = 0.6$.

\begin{figure}[h!]  % [htbp]
    \centering
    \includegraphics[width=0.85\linewidth]{2-Chapters/2-Chapter/Images/example_of_a_5_arm_bandit_problem__step100.png}
    \caption{Screenshot of the demonstration, after the end of the game, after $t=T=100$ steps.  See \href{https://perso.crans.org/besson/phd/MAB\_interactive\_demo/}{\texttt{perso.crans.org/besson/phd/MAB\_interactive\_demo/}}}
    \label{fig:2:example_of_a_5_arm_bandit_problem__step100}
\end{figure}


\paragraph{Decisions can use the past observations.}

A bandit algorithm $\cA$, also referred to as a strategy or a policy, and sometimes denoted by $\pi$ or $\rho$, selects an arm $A(t)$ at time $t$ by using the past observations and the past external randomness.
More formally, one could state that an algorithm is a sequence of measurable functions $(\cA_t)_{t\geq1}$,
where $\cA_t$ maps the past observations $\cO_t = (U_0, Y_1, U_1, \dots, Y_{t-1}, U_{t-1})$
to an arm $\cA_t(\cO_t) = A(t) \in[K]$,
if we denote $Y_s = r_{A(s)}(s)$ the $s$-th reward.
The initial information is reduced to $\cO_1 = (U_0)$, and the first decision is $A(1) = \cA_1(\cO_1)$. Usually, most algorithms starts by selecting $A(1)=1,\dots,A(K)=K$ (or a permutation of the $K$ arms) in the $K$ first steps.
%
An algorithm is said to be deterministic if it does not depend on the external randomness $U_0,U_1,\dots$, but in this thesis we only use non-deterministic algorithms.


% ----------------------------------------------------------------------------
\subsection{Hypotheses on rewards distributions}

In the example above, we consider Bernoulli distributions, but other distributions have been studied in the literature.
From now on until the last chapter of this thesis, we only consider stochastic rewards. The piece-wise stationary model is studied in Chapter~\ref{chapter:6}.
%
% Everything is implemented and documented at
% https://smpybandits.github.io/docs/Arms.html
We also focus only on real-valued rewards, meaning that $r_k(t)\in\R$ for all $k$ and $t$.

An important hypothesis is whether rewards are bounded or not,
and whether the player knows if they are bounded or not before starting the bandit game.
Moreover, if rewards are known to be bounded, let say in an interval $[a,b]$, another important hypothesis is whether the player knows the values of $a$ and $b$.
%
Intuitively, the bandit game is easier if the player knows the support of the distributions, and we restrict to this case in all the thesis.
Most of the algorithms proposed in the literature follow this hypothesis as well.
%
The mostly used
infinitely supported distributions are Gaussian, exponential and Poisson,
while in the literature, the Bernoulli distribution is the most common case of finitely supported distributions.
Continuous distributions with finite support also included truncated versions of infinitely supported distributions, in particular truncated Gaussian are used for numerical experiments in lots of research articles.

% Just bounded, in $[a,b]$ but without loss of generality in $[0,1]$ etc.
\paragraph{The normalization trick.}\label{par:2:normalizationTrick}
%
If the player knows that reward are bounded in an interval $[a,b]$, and if she knows $a$ and $b$ ($a<b$), then with no loss of generality we can restrict to the interval $[0,1]$, as if $r\in[a,b]$, the player can instead consider the normalized reward $r' = $ that lies in $[0,1]$.
Note that this ``normalization trick'' is implemented for any policy in SMPyBandits, with the \texttt{lower} and \texttt{amplitude} optional arguments, respectively representing $a$ the lower-bound on rewards and $b-a$ the length of the interval of possible values of rewards.

% Bernoulli, Gaussian, sub-Bernoulli, sub-Gaussian, Exponential, sub-Exponential...

\paragraph{We restrict to one-dimensional exponential family.}
%
% On one-dimensional exponential family...

In all this thesis we restrict to distributions that have a certain form, and focus on real-valued distributions lying in an exponential families.
One-dimensional exponential families include Bernoulli and Poisson distributions, as well as Gaussian distributions with a fixed variance.
Our main interest is Bernoulli distributions, but we prefer to present the notations of exponential families in one dimension.
% We follow the notations from the course on statistical learning (Stat 260) taught in 2010 by Michael Jordan at the University of Berkeley \cite{JordanCourseStatBerkeley}, that are also the notations used for instance in Emilie Kaufmann's PhD in 2014 \cite{Kaufmann12PhD}.

Given a measure $\lambda$, that is usually the natural Lebesgue measure on $\R$, an exponential family of probability distributions is defined as the distributions whose density (relative to $\lambda$) can be written as
% \begin{equation}\label{eq:2:exponentialFamily}
$ \Pr(x | \lambda) = h(x) \exp \left( \eta x - A(\eta) \right)$,
% \end{equation}
for a parameter vector $\eta$ (the canonical parameter), and a sufficient statistic that we restrict to be $T(x)=x$, and a function $h$.
$A(\eta)$ is the cumulant function, and it is entirely determined by the $\eta$ and $h$:
$A(\eta) = \log \left( \int h(x) \exp(\eta x) \lambda(\mathrm{d} x) \right)$.
%
The natural parameter space is the set of values of $\eta$ such that this integral $A(\eta)$ is finite,
and usually the literature focusses on regular and minimal exponential families (when the natural parameter space is an non-empty open set in $\R$).

-- An important result on exponential families is that \emph{a distribution in such family is entirely characterized by its parameter $\eta$}.
That is why such one-dimensional distributions are entirely characterized by their mean, as $\mu = \E_{\eta}[X]$.

-- A second important result is a simplified form for the \emph{Kullback-Leibler divergence} \cite{KullbackLeibler51}, for two distributions lying in the same exponential family.
The KL divergence is also called the relative entropy is a measure of how one probability distribution is different from a second, reference probability distribution.

\begin{defn}[Kullback-Leibler divergence]\label{def:2:KLDivergence}
    The KL divergence between two distributions $d_1$ and $d_2$ is defined as
    \begin{equation}\label{eq:2:Kullback-LeiblerDivergenceExpFamily1}
        \KL\left(d_1, d_2\right) = \int d_1 \log\left(\frac{d_2}{d_1}\right) \lambda(\mathrm{d}x) = \E_{d_1} \left[ \log\left(\frac{d_2}{d_1}\right) \right].
    \end{equation}
\end{defn}

Following this Definition~\ref{def:2:KLDivergence} for two distributions
$d_1=\Pr(x|\eta_1)$ and $d_2=\Pr(x|\eta_2)$,
we can write $\KL( \eta_1, \eta_2 ) = \KL(\Pr(x|\eta_1), \Pr(x|\eta_2))$, which can be simplified to use only the two parameters $\eta_1$ and $\eta_2$,
and $\mu_1 = \E_{\eta_1}[X]$ the mean of distribution $d_1$:
%
\begin{equation}\label{eq:2:Kullback-LeiblerDivergenceExpFamily2}
    \KL\left( \eta_1, \eta_2 \right) = \E_{d_1}[ (\eta_1 - \eta_2) X ] - A(\eta_1) + A(\eta_2) = (\eta_1 - \eta_2) \mu_1 - A(\eta_1) + A(\eta_2),
\end{equation}


Without diving more in the details of exponential families,
it is interesting to illustrate this definition and the notations with two important examples:
%
\begin{itemize}
    \item
    \textbf{Bernoulli distributions} can be seen as an exponential family with $h(x) = 1$,
    and for a Bernoulli distribution of mean $\mu\in[0,1]$, denoted $B(\mu)$,
    the parameter is $\eta = \mu / (1 - \mu)$, giving $A(\eta) = \log(1 + \mathrm{e}^{\eta})$
    (with the limit behavior $\eta=+\infty$ if $\mu=0$).

    Moreover, the KL divergence between $B(x)$ and $B(y)$, of parameters $\eta$, $\eta'$ is given by
    $\kl(x,y) = \KL_{B}(\eta,\eta') = x \log(x/y) + (1-x) \log((1-x)/(1-y))$.

    \item
    \textbf{Gaussian distributions} of a fixed variance are a one-dimensional exponential family,
    and in this thesis we do not consider Gaussian with an unknown variance.
    For a variance of $\sigma^2$, the family uses
    % $T(x) = [x ; x^2]^T$ and
    $h(x) = 1/\sqrt{2\pi\sigma^2} \exp(-x^2/(2\sigma^2))$,
    and for a Gaussian distribution with mean $\mu\in\R$, denoted $\cN(\mu,\sigma^2)$,
    the parameter is $\eta = \mu/\sigma^2$, giving $A(\eta) = \mu^2/(2\sigma^2)$
    (see Chapter~8 of \cite{JordanCourseStatBerkeley}).
    % % For a generic variance,
    % This shows that the Gaussian distributions
    % % are a two-dimensional exponential families,
    % with a fixed variance $\sigma^2$ form indeed a one-dimensional exponential family.

    Moreover, the KL divergence between $\cN(x,\sigma^2)$ and $\cN(x,\sigma^2)$, of parameters $\eta$, $\eta'$ is given by
    $d(x,y) = \KL_{\cN}(\eta,\eta') = (x-y)^2 / (2\sigma^2)$.
\end{itemize}


In the rest of this thesis, \emph{we only consider bounded rewards in the mathematical developments}, and we mostly focus on Bernoulli distributions, because they are usually the most relevant choice for the application of choice.
First, we restrict for simplicity to Bernoulli distributions for the simulations shown in Chapter~\ref{chapter:3}, even if our library does implement all the distributions commonly found in the literature (including unbounded one-dimensional exponential families and even Markov models).
% Then in Chapter~\ref{chapter:4}, each static IoT device in the noisy environment is assumed to be following a Bernoulli emission pattern in a fixed channel, FIXME
%
For example when we study multi-player bandits in Chapter~\ref{chapter:5}, for the model without sensing, we explain that restricting to the Bernoulli case is interesting and it is actually the hardest one, as continuous distributions with a null mass on $r(t)=0$ yield a much simpler problem.
%
Finally, in Chapter~\ref{chapter:6} we analyze our proposed algorithm in the setting on bounded distributions, without restricting to Bernoulli distributions, but we use the fact that bounded distributions on $[0,1]$ are sub-Bernoulli, and we use the Bernoulli Kullback-Leibler divergence in our analysis.


\paragraph{Other kind of hypothesis: sub-Gaussian and sub-Bernoulli distributions.}
%
A lot of research works considers rewards distributions that are not Gaussian but sub-Gaussian, of a known variance, for instance $1/4$.
For instance, bounded distributions on $[0,1]$ are known to be $1/4$ sub-Gaussian, and this fact is used for instance in \cite{Maillard2018GLR}.
It means that their moment generating function is dominated by that of a Gaussian distribution with the same mean and a variance $1/4$.
In Chapter~\ref{chapter:6}, we instead consider sub-Bernoulli distributions, formally introduced in Definition~\ref{def:6:subBernoulliDistributions}.
%
Such hypothesis is also proposed for other families of distributions, for instance \cite{KimTewari2019} analyses their Follow-the-Perturbed-Leader algorithm for perturbations following any sub-Weibull distribution (sub-Weibull distributions generalize both sub-Gaussian and sub-Exponential distributions).



\paragraph{About Markov models.}
%
% Markov models: we do not present it in this thesis.
Finally, we note that Markov models, while being implemented in SMPyBandits, are not used in this thesis.
They were introduced in the 1980s, by Whittle in \cite{Whittle1988} and Anantharam and others in \cite{Anantharam87b}.
A Markov MAB model maps an arm to a Markov chain \cite{Norris98}, instead of a distribution, and thus they are no longer stochastic nor stationary.
Such Markov models come in two flavors: rested or restless.
For $K$ arms, each Markov chain has a finite number of states $s$, each corresponding to a (constant) reward that the player obtains if she selects this arm while its Markov chain is in state $s$.
Rested Markov models means that only the state of the selected arm's Markov chain can change, following its Markov transition matrix.
Restless models remove this hypothesis, making them harder to track and solve.
%
Such models were less studied than stationary or adversarial models, but some interesting works focussed on Markov models in the last 10 years.
For instance, \cite{Melian15} proposed a cognitive radio model mixing MAB and Hidden Markov Models (HMM), solved by a mixed policy called UCB-HMM.


% ----------------------------------------------------------------------------
\section{Different applications of stochastic MAB}
\label{sec:2:applicationsofStochasticMAB}

The blooming success of the research on multi-armed bandits is easily explained by the different spectrum of applications of MAB models to real-world discrete-time decision making problems.
This research field has been very active since the years 2010s, but it started as early as 1933 with \cite{Thompson33}, and was active since the 1980s and the seminal works by Lai and Robbins \cite{LaiRobbins85} and by Anantharam and others \cite{Anantharam87a}.
%
MAB have been applied successfully to various decision making problems, like the following:

Historically, MAB models were first applied to \textbf{clinical trials}, where an arm represents a treatment, and the distribution associated with such treatment can be a Bernoulli distribution: a reward of $0$ means the drug did not heal the disease, and a reward of $1$ indicates a success. The mean of an arm, in this application, represents the mean success rate of a treatment, and the goal of a doctor in a clinical trial is to identify the best treatment, \ie, the arm with highest mean, in a number of trials as short as possible.
%
% After introducing formally the notations used in all this thesis, we review below possible applications of multi-armed bandits.

% Other popular applications include the following.
%
% \begin{itemize}
%     \item
    \textbf{A/B testing}, for instance for websites, is a popular application of the best arm identification problem,
    where the task is purely exploratory and the player is asked to identify the best of two options (or more), in a finite number of steps \cite{audibert2010best}.
    The problem can either consider a fixed budget and no freedom on the ending of the game (\ie, an known horizon $T$), or a fixed confidence and a certain freedom on the budget (\ie, the identified arm must be the true best arm with probability at least $1-\delta$) \cite{Garivier16BAI}.
    The theoretical complexity of this use of bandit for A/B testing was first studied in \cite{Kaufmann14},
    and later on a more practical point-of-view was proposed in \cite{Jamieson17ABTest}.

    % \item
    MAB can also be applied to a broader setting of \textbf{online content recommandation},
    with more than two options.
    The seminal work of \cite{Li10} studies the application of contextual bandit to news article recommendation, as it is used in practice on platforms such as Microsoft's Bing news website,
    or in applications like Netflix.
    In such models, the arms correspond to items to recommend (\eg, articles or movies), and the contexts contain features about each user of the system.
    An interesting work is \cite{Louedec16}, who studies slowly-varying non-stationary models applied to recommender systems.

    % \item
    Using bandit algorithms for \textbf{improved machine learning} models or algorithms has been an active research domain for the last ten years or so.
    As presented below in Section~\ref{sec:2:chooseYourPreferredBanditAlgorithm}, a certain ``leader'' bandit algorithm can be used to select on the run the best bandit algorithm from a pool of ``followers'' algorithms.
    Other possible use cases include hyper-parameter optimization, or feature selection.
    Hyper-parameters include real-valued parameters, like $\alpha\in[0,\infty)$ for the \UCB{} or $\varepsilon$ for the $\varepsilon$-greedy bandit algorithms, a step size multiplier in a gradient method, or the width of a radial based function (RBF) kernel method.
    Discrete-valued parameters are also common, like a choice in a fixed set of kernel functions or the depth of neural networks,
    and higher dimensional or more complex hyper-parameters include the entire architecture of a neural network.

    % % FIXME this is copied pasted from \cite{bouneffouf2019survey}
    % It is well known that performance of machine learning algorithms depends critically on identifying a good set of hyper-parameters.
    % While recent approaches use Bayesian optimization to adaptively select optimal hyper-parameter configurations, they rather focus on speeding up random search through adaptive resource allocation and early-stopping.
    % A different approach was initiated in \cite{LiJamieson2018}, by formulating hyper-parameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem, where predefined resources, such as iterations, data samples, or features are allocated to randomly sampled configurations. This work introduced a novel algorithm, Hyperband, for this framework and analyzed its theoretical properties, providing several desirable guarantees.
    % Furthermore, Hyperband was compared with popular Bayesian optimization methods on a suite of hyper-parameter optimization problems; it was observed that Hyperband can provide more than an order-of-magnitude speedup over its competitors on a variety of deep-learning and kernel-based learning problems.
% \end{itemize}

% - clinical trial
% - A/B testing of websites
% - online content recommandation
% - wireless communication, lots of possibilities, one is frequency/channel selection in the OSA paradigm, detailed after
% - hyper-parameters tuning of deep neural networks etc

The different applications detailed above are still active research directions,
and a curious reader can find other interesting applications of MAB models and algorithms in \cite{bouneffouf2019survey}.


\paragraph{Applications for Opportunistic Spectrum Access.}
% Detail the previous work from our team SCEE on bandits + OSA : Wassim, Navik

The focus of this work is on cognitive radio and IoT networks, where arms can represent wireless orthogonal channels, but more generally any resource characterizing the communication between a wireless device and a gateway (\eg, spreading factor for LoRa \cite{KerkoucheAlami18}, power allocation for NOMA etc). In cognitive radio using centralized supervision, for instance if the gateway can decide the allocation of devices to resources, MAB can also be used to let the gateway explore different allocations and learn by itself a good allocation, see for instance this article that consider 5G-like networks with small cells \cite{Maghsudi16}.

Previous works of our SCEE team showed that MAB can be used to model the problem of spectrum access for a secondary user accessing a licensed spectrum.
In this model, arms represent a finite set of orthogonal channels, \ie, different frequency bands in a licensed spectrum.
In the model with sensing, the rewards $r_k(t)$ represents the feedback obtained by the CR-equipped device after sensing the channel $k$ at time $t$.
A reward of $r_k(t) = 1$ indicates that no Primary User was sensed, while a reward of $r_k(t)=0$ indicates that the channel $k$ is busy at time $t$ and no uplink message can be sent.
%
This model was first introduced by Wassim Jouini during his PhD thesis,
in \cite{Jouini09} and later studied in both \cite{Jouini10,Jouini12}.
Proof-of-concepts using real-world radio hardware was first propsed in \cite{MoyWSR2014,RobertSDR2014}.
In a second PhD thesis supervised by Christophe Moy, Navikkumar Modi studied the impact
of using MAB algorithms to optimize channel selection
on the battery life of a wireless device.
On the one hand, running a MAB algorithm such as UCB-like algorithms was proved to be useful and can bring significant improvement in terms of successful transmission rates, directly increasing the battery life of the device.
On the other hand, classical MAB algorithms tend to switch arms a lot of times, especially in the beginning of the learning process, and this induces a lot of dynamical hardware reconfiguration for the wireless device, as selecting a different channel requires a change in the radio hardware used by the device.
Each hardware reconfiguration costs energy for the device, and quickly switching algorithms will lead to a reduction of the battery life.
The tradeoff between the two aspects is studied empirically in
\cite{modiDemo2016}.
%
Another interesting works is \cite{Modi17QoS}.

\TODOL{FIXME Finir ce blabla rapidement ? Ou alors ce sera déjà fait en intro/chapitre1 ??}


% ----------------------------------------------------------------------------
\section{Definition, decomposition and lower-bounds on the regret}
% : definition, decomposition, and lower bounds
\label{sec:2:lowerUpperBoundsRegret}

As explained above, the main objective of a player facing a bandit game is to maximize its (expected) cumulated reward.
An efficient algorithm should obtain a mean reward converging to the maximum reward.
We introduce in this section the notion of \emph{regret}, and the equivalence for a player between maximizing its sum of rewards and minimizing its regret.
In Machine Learning, we usually prefer to aim at minimizing certain quantities, such as the error rate in supervised learning, or the distance to the optimum in an optimization problem,
but the main interest of studying the regret is to be able to obtain higher-order information about the convergence of the mean reward to the max reward,
or in other words, to quantify the speed of convergence of a MAB algorithm.


\subsection{Measuring performance with the (mean) regret}

Let us first introduce some notations.
From now on, we consider a stochastic and stationary MAB problem, with $K$ arms of distributions $(\nu_k)_k$, that is, for any time $t$, $r_k(t)\sim \nu_k$.
We denote $\mu_k$ the mean of the distribution of arm $k$ (it will be referred to as the mean of arm $k$ in the rest of the manuscript), that is $\mu_k = \E[r_k(t)] \in \R$.
%
% \paragraph{Defining the regret.}
%
To define the regret, we first need to distinguish between \emph{optimal} and \emph{sub-optimal} arms.

\begin{defn}[Optimal and sub-optimal arms]\label{def:2:optimalSubOptimalArms}
    Consider a bandit problem of $K$ arms with distributions characterized by their means $\bm{\mu}=\mu_1,\dots,\mu_k$.
    Denote $\mu^* = \max_k \mu_k$ the largest mean.\\
    The best arm can be non unique, and any arm $k$ having $\mu_k = \mu^*$ is said to be \emph{optimal},
    while arms satisfying $\mu_k < \mu^*$ are called \emph{sub-optimal}.
\end{defn}

If the goal of the player is to maximize $\E\left[\sum_{t=1}^T r_{A(t)}(t)\right]$,
the optimal strategy for this bandit problem is to always pull an optimal arm, but of course it is unrealistic as the player does not know the true means and cannot know which arm is optimal.
%
Comparing the difference between the performance of a fixed baseline and that of the player is a common approach in machine learning research,
and here we can compare with the oracle strategy that obtains an (expected) reward of $\mu^*$ at each time step.
%
For a fixed horizon $T$, if $k^*$ denotes the index of any optimal arm,
let us introduce the (mean) regret $R_T^{\cA}$ of an algorithm $\cA$ as
\[ R_T^{\cA} = \E\left[ \sum_{t=1}^T r_{k^*}(t) - r_{A(t)}(t) \right]. \]
%
We can rewrite this expression to obtain the following definition of the regret:
$R_T^{\cA}$
$= \E\left[ \sum_{t=1}^T r_{k^*}(t) \right] - \E\left[ \sum_{t=1}^T r_{A(t)}(t) \right]$
$= T \E\left[ r_{k^*}(1) \right] - \sum_{t=1}^T \E\left[ r_{A(t)}(t) \right]$
$= T \mu^* - \sum_{t=1}^T \E\left[ r_{A(t)}(t) \right]$.
% This is the definition we give below.


\begin{defn}[Regret]\label{def:2:regret}
    For an algorithm $\cA$, a bandit problem of $K$ arms characterized by their means $\bm{\mu} = \mu_1,\dots,\mu_K$ and if $\mu^* = \max_k \mu_k$, then the (mean) regret at horizon $T$ is defined as
    \begin{equation}
        R_T^{\cA} = T \mu^* - \sum_{t=1}^T \E\left[ Y(t) \right] = T \mu^* - \sum_{t=1}^T \E\left[ r_{A(t)}(t) \right].
    \end{equation}
\end{defn}


\paragraph{A useful decomposition of the regret.}
%
Remember that $N_k(t) = \sum_{s=1}^t \mathbbm{1}(A(s) = k)$ denotes the number of times arm $k$ was selected between times $1$ and $t$,
%
and that the rewards $r_k(t)$ are all \iid{} of mean $\mu_k$.
The \emph{gap} between any arm $k\in[K]$ and an optimal arm is defined as $\Delta_k = \mu^* - \mu_k$
(an arm $k$ is sub-optimal if $\Delta_k > 0$),
and thus we can write the following decomposition on the regret,

\begin{lemma}[Regret decomposition]\label{lem:2:RegretDecomposition}
    The (mean) regret $R_T^{\cA}$ can be decomposed as a sum of the number of selections of sub-optimal arms $k$,
    weighted by their gap, like this:
    \begin{equation}\label{eq:2:RegretDecomposition}
        R_T^{\cA} = \sum_{k=1}^K \Delta_k \E[ N_k(T) ] = \sum_{k=1,\dots,K : \Delta_k > 0} \Delta_k \E[ N_k(T) ].
    \end{equation}
\end{lemma}
%
\begin{proof}\label{proof:2:RegretDecomposition}
    One can use the chain rule of expectation, and because the expectation is taken on the randomness of the (\iid) rewards $(r_k(t))_t$ and on the decisions of the player $(A(t))_t$ (which are measurable wrt to the past observations $\cO_t$),
    one can rewrite the expected cumulated rewards like this,
    $\E \left[ \sum_{t=1}^T r_{A(t)}(t) \right]$
    $= \E \left[ \sum_{k=1}^K \sum_{t=1}^T r_k(t) \mathbbm{1}(A(t) = k) \right]$
    $= \sum_{k=1}^K \E [ r_k(1) \sum_{t=1}^T \mathbbm{1}(A(t) = k) ]$
    % \underbrace{XXX}_{= N_k(T)}
    $= \sum_{k=1}^K \mu_k \E \left[ N_k(T) \right].$
    %
    Thus $R_T^{\cA} = \sum_{k=1}^K \Delta_k \E[ N_k(T) ]$.
    The sum can then be simplified to only count sub-optimal arms, \ie, arms $k$ such that $\Delta_k > 0$.
\end{proof}


\paragraph{Consequences.}
%
Such decomposition of the regret is useful for at least two reasons.

-- On the one hand, from a numerical simulation point-of-view, when we run a finite number of repetitions of the same stochastic experiment, if one wants to compute and visualize the regret, she can either use the definition with the rewards, or the decomposition and simply sum the gaps $\Delta_k$ with the number of sub-optimal draws.
Mathematically, both quantities are equal in expectation, but with only a finite number of trajectories and observations, the first estimate is more noisy than the second one, since the randomness on the rewards is (partially) removed in the decomposition \eqref{eq:2:RegretDecomposition}.
In our library SMPyBandits, we implement both estimators, and all values of regret used in this thesis are based on the one using the decomposition of Lemma~\ref{lem:2:RegretDecomposition}, because it gives smoother curves and faster convergence to the limit behavior.

-- On the other hand, this decomposition is necessary as theoretical analyses of the regret of MAB algorithms are usually based on controlling the suboptimal draws $N_k(T)$ and not directly the regret $R_T^{\cA}$.
For example, we extend this decomposition to the multi-player case in Chapter~\ref{chapter:3}, in Lemma~\ref{lem:5:DecompositionRegret}, and as it also uses the draws $N_k(T)$, it is the first quantity we prove to be bounded by $\bigO{\log(T)}$ when we prove the regret upper-bound of our proposal \MCTopM-\klUCB.


\subsection{Regret lower bounds}

We include in this section two well-known results about what bandit algorithms \emph{cannot} do.
First, a \emph{problem-dependent lower-bound} states that any algorithm suffers a regret at least $\Omega(\log T)$ on any problem (from \cite{LaiRobbins85}).
Then, a \emph{worst-case result} states that any algorithm can perform as badly as $\Omega(\sqrt{K T})$ on a certain problem instance, designed specifically to make it perform badly (a result from \cite{Auer02NonStochastic}),
%
In a certain family of problems, \eg, $K$ Bernoulli distributed arms, the difficulty of a problem is characterized by a measure that depends only on the arms means. This measure of difficulty of a problem is hidden in the $\Omega$ notation of the lower-bound.

In all this section, we restrict to stationary stochastic problems with $K\geq2$ arms.
We do not give proofs of the following theorems, as they can be found in the historical papers, and simpler proofs are given in recent references, such as \cite{Bubeck12}, \cite{LattimoreBanditAlgorithmsBook}, or Chapter~2 in \cite{Slivkins2019} for instance.
%
Let $\cI$ denote the set of all problem instances, with $K \geq 2$ arms. We assume the rewards lie in $[0,1]$ (but no additional hypothesis).
To specify the dependency on an instance $I$, we denote the regret of $\cA$ on instance $I$ and horizon $T$ by $R_T^{\cA}(I)$.


\paragraph{Problem-dependent lower-bound in $\log T$}

The following two theorems were proven in \cite{LaiRobbins85}.
These lower-bounds are of highest interest to design efficient algorithms,
and a significant part of the research literature on MAB algorithms has focussed on finding algorithms that matches the Lai and Robbins' lower-bound asymptotically.
This means that an upper-bound is proven on the regret of algorithm $\cA$, that asymptotically match the lower-bound, with the same constant in the big-$\cO$ notation (in which case we say that the algorithm is \emph{optimal}), or with a larger constant (the algorithm is said to be \emph{order-optimal} in such case).

Moreover, this lower-bound can be directly used to design efficient algorithms, as thanks to the regret decomposition given in Lemma~\ref{lem:2:RegretDecomposition} above, the expression in \eqref{eq:2:forSecondLogTLowerBound2} essentially says that any efficient algorithm
should sample each sub-optimal arm $k$ about $\log(T)/\KL(\mu_k,\mu^*)$ times.
Tracking this quantity is used for instance in the OSSB algorithm proposed in \cite{Combes17}.
% FIXME explain why this is interesting for the rest of this manuscript!

\begin{theorem}\label{thm:2:firstLogTLowerBound}
    No algorithm $\cA$ can achieve a (mean) regret $R_T^{\cA}(I) = \smallO{c_I \log(T)}$ for all problem instances $I \in \cI$,
    where this notation means that the ``constant'' $c_I$ can depend on the problem instance $I$ (\eg, on the means and on $K$) but not on the time horizon $T$.
\end{theorem}

We consider uniformly efficient\footnote{This notion is then extended for ``strongly uniformly efficiency'' in the multi-player case with Definition~\ref{def:5:DecentralizedUniformEfficiency}, where we also include a notion of (expected) fairness.} algorithms, to rule out algorithms achieving low regret on some problem instances while achieving linear regret on other instances.
In particular, it is necessary to rule out algorithms that always pick the same arm, as on some problem instances such fixed-arm algorithms can achieve zero regret.
This class contains for instance the \UCB{} algorithm, as its regret is proven to be logarithmic on any problem instance.

\begin{defn}\label{def:2:uniformlyEfficientAlgorithm}
    An algorithm $\cA$ is \emph{uniformly efficient} if its (mean) regret satisfies
    \begin{equation}
        R_T^{\cA}(I) = \smallO{C_{I,\alpha} T^{\alpha}}
    \end{equation}
    for any value $\alpha>0$ and any problem instance $I\in \cI$,
    where this notation means that the ``constant'' $C_{I,\alpha}$ can depend on the problem instance $I$ and on $\alpha$, but \emph{not} on the time horizon $T$.
\end{defn}

Now we can state the second logarithmic lower-bound, for algorithms in this family:

\begin{theorem}\label{thm:2:secondLogTLowerBound}
    Let $\cA$ denote a uniformly efficient algorithm,
    and $I$ an arbitrary problem instance.
    For this instance,
    there exists a constant $C_I$ depending only on the problem $I$,
    and a time $T_0$ such that
    \begin{equation}
        \forall T \geq T_0, \;\;\; R_T^{\cA}(I) \geq C_I \log(T).
    \end{equation}
\end{theorem}

This third theorem specifies possible values of the constants $C_I$ for the two previous results.

\begin{theorem}\label{thm:2:forSecondLogTLowerBound}
    Let $\cA$ denote a uniformly efficient algorithm,
    and $I$ an arbitrary problem instance.
    \begin{itemize}
        \item
        The bound from Theorem~\ref{thm:2:secondLogTLowerBound} holds with,
        \begin{equation}\label{eq:2:forSecondLogTLowerBound}
            C_I = \mu^* (1 - \mu^*) \sum_{k: \Delta_k > 0} \frac{1}{\Delta_k}.
        \end{equation}
        \item
        For any $\varepsilon>0$, the bound from Theorem~\ref{thm:2:secondLogTLowerBound} holds with,
        \begin{equation}\label{eq:2:forSecondLogTLowerBound2}
            C_I = \sum_{k: \Delta_k > 0} \frac{\Delta_k}{\KL(\mu_k, \mu^*)} - \varepsilon.
        \end{equation}
    \end{itemize}
\end{theorem}


Many algorithms have been proved to achieve logarithmic regret in the stochastic case,
and in particular it is the case of the algorithms used in this thesis, \UCB{} from \cite{Auer02}, Thompson sampling from \cite{Thompson33} and analyzed in \cite{AgrawalGoyal11,Kaufmann12Thompson}, and \klUCB{} from \cite{Garivier11KL,KLUCBJournal}.
%
Such bounds are valid in different settings, and in particular \UCB{} is order-optimal for bounded rewards or one-dimensional exponential families,
while Thompson sampling is optimal for bounded rewards, and \klUCB{} have been proven to be optimal for both cases.


\paragraph{Worst-case lower-bound in $\sqrt{T}$}

For a fixed horizon $T$, it is interesting to note that one can find instances $I$ that are so ``hard'' that a logarithmic regret (lower or upper) bound that uses a constant $C_I$ no longer bring any information.
Indeed, we can naively bound the regret by $R_T^{\cA} \leq (\max_k \Delta_k) T$, and thus if $(\max_k \Delta_k)$ can be taken so small that $C_I \log(T) \gg (\max_k \Delta_k) T$, and thus a regret upper bound like $R_T^{\cA} \leq C_I \log(T)$ is useless.

For an example, consider any large horizon $T>10$, and $K=2$ Bernoulli arms.
Chose any small $\varepsilon$ such that $\varepsilon \ll \sqrt{4 \log(T) / T}$, then chose let $\mu_1 = 1/2$ and $\mu_2 = 1/2 - \varepsilon$.
The constant $C_I$ from \eqref{eq:2:forSecondLogTLowerBound} is $C_I = 1 / (4 \varepsilon)$ and $\Delta=\varepsilon$ and it becomes so large that
the logarithmic upper-bound is useless in such case, as
$\varepsilon \ll \sqrt{\frac{4 \log(T)}{T}} \Longleftrightarrow \frac{1}{4\varepsilon} \log(T) \gg \Delta T$.

For this reason, the research literature has also looked with interest another family of bounds on regret, that are not problem-dependent but worst-case, also called minimax \cite{Audibert2009minimax,audibert2010minimax}.
Hence it is interesting to quote another lower-bound on the regret, that can bring useful information in such settings.
The following theorem was proven in \cite{Auer02NonStochastic}.

\begin{theorem}\label{thm:2:worstCaseLowerBound}
    Fix the number of arms $K$, and the algorithm $\cA$.
    Then for any horizon $T$, there exists a problem instance $I_T$ on which the algorithm suffers a regret $R_T^{\cA}(I_T)$ that verifies
    \begin{equation}
        R_T^{\cA}(I_T) \geq \Omega(\sqrt{K T}).
    \end{equation}
\end{theorem}

% \TODOL{FIXME explain why this is interesting for the rest of this manuscript!}

Similarly to what is considered for the first lower-bound,
a natural question is to know if there is an algorithm that achieve a regret upper-bound of the form $R_T^{\cA}(I) \leq \bigO{\sqrt{K T}}$ for any instance, independently on the problem difficulty.
It is the case for the Exp3 algorithm from \cite{Auer02}, for example.
Since a few years, some algorithms were shown to achieve both a problem-dependent logarithmic and a minimax upper-bounds,
like MOSS in \cite{Audibert2009minimax} or recently \KLUCBpp{} in \cite{Menard17},
and such results are usually referred to as ``best of both worlds''.


% % do not detail too much, don't explain the tools behind the result

% \begin{itemize}
%     \item
%     - [Lai and Robbins] lower-bound in $\Omega(\log(T))$ \cite{LaiRobbins85}
%     \item
%     - Worst case lower-bound in $\Omega(\sqrt{T})$ \cite{Auer02,Auer02NonStochastic,Bubeck12}
%     \item
%     - Adversarial lower-bound in $\Omega(\sqrt{T})$ (also useful for piece-wise stationary models) \cite{Auer02NonStochastic}
% \end{itemize}


\subsection{Other measures of performances}

In this thesis, we only study analytically the mean regret $R^{\cA}$ (in Chapters \ref{chapter:5} and \ref{chapter:6}), but we quickly mention other measures of performances used in our work.

% - Quantile regret or just histogram of regrets
First, when doing numerical experiments about bandits, if one studies the regret as an empirical mean based on a large number of random repetitions (\eg, $N=1000$ repetitions), it is important to not only show the mean value but also the variance of the values taken by the regret on each repetition, to verify that all algorithms perform consistently.
Indeed, by only visualizing the mean of $1000$ values, it is possible that we miss some ``bad runs'': if $1$ run out of the $1000$ gives linear regret (\ie, $R_T^{\cA} \propto T$) and the $999$ other give logarithmic regret, then the mean will appear logarithmic.
This is the case of the \Selfish{} algorithm that is defined and explored in Chapter~\ref{chapter:5}.
By visualizing the entire \emph{distribution} (as an histogram), or the variance of the values of $R_T^{\cA}$, and if the number $N$ is reasonably large, we can verify that the regret appears logarithmic for all runs.

% DONE on s'en fout !
% % - Best Arm Identification?
% Other measures of performance that has been studied in the literature include
% the best arm identification (BAI) rate
% and the best arm selection (BAS) rate.
% The BAI rate counts the frequency at which an algorithm $\cA$ correctly identified the best arm(s) at the horizon $T$,
% while the BAS rate counts the total number of time steps during which $\cA$ selected (one of) the best arm(s).
% Both quantities are computed and stored in all numerical simulations using SMPyBandits, but visualizations for both have not been included in this thesis.

% - Worst case regret (max $R_T^{r}$ for $r$ index of Monte-Carlo simulation?)
The \emph{switching cost} $SC^{\cA}(T)$ counts how many times the player's decision has changed from one time step to the next one, \ie, $SC^{\cA}(T) = \sum_{t=1}^{T-1} \indic(A(t) \neq A(t+1))$.
It has recently gained interest in the literature, for instance in \cite{Koren17}.
% FIXME give the main result proved in this paper?
In single-player models,
it is easy to show that achieving logarithmic regret directly implies a logarithmic upper-bound on the switching cost, and conversely the lower-bound from \cite{LaiRobbins85} also gives an asymptotic logarithmic lower-bound.
But even if $SC^{\cA}(T) = \Theta(\log T)$ for an efficient algorithm, it can be interesting to numerically evaluate this quantity, as a large value might indicate an algorithm that is alternating too much between the optimal arm and other arms.
%
% FIXME we give notations but I don't use them in Chapter5 
It is more interesting to studying $SC_M^{\cA}(T)$, the sum of the switching costs of the $M$ players in a multi-player bandit game, as in this case, it is possible that the players follow an efficient centralized or decentralized algorithm (which gives a logarithmic centralized regret) while still suffering from a linear switching cost.
It is very satisfying to obtain a logarithmic switching cost for our algorithm \MCTopM-\klUCB{} in Chapter~\ref{chapter:5}, even if minimizing the switching cost was not one of our objective, and the bound we obtain on $SC_M^{\cA}(T)$ is just a consequence of our proof of the control we give on the regret $R^{\cA}(T)$.
% - Switching cost? \cite{modiDemo2016} \cite{Koren17}
On a more experimental note, it was showed in \cite{modiDemo2016} that the previous state-of-the-art policy for multi-player bandits, \rhoRand, could be tuned to run in batches\footnote{~The batch bandit setting means that all players use the same decisions for instance for $50$ consecutive times, and update their decisions only once every $50$ time steps. For more details, see \cite{modiDemo2016} for experiments on the multi-player case, or \cite{perchet2016,gao2019batched,kolnogorov2019multi} for theoretical developments on the single-player case.}, in order to reduce by a certain multiplicative factor its switching cost while only adding an additive factor on its regret.


% - Fairness ?
Finally, another interesting measure of performance is the \emph{fairness} between the $M$ players in a multi-player bandit models, as we explain in Chapter~\ref{chapter:5}.


% ----------------------------------------------------------------------------
\section{Review of stochastic MAB algorithms}
\label{sec:2:famousMABalgorithms}

This Section starts by discussing two naive strategies which fail dramatically,
%
and then we present two simple strategies which performs efficiently, only if they are tuned using a prior knowledge of the problem difficulty, but are thus unusable for practical applications.
%
We focus then on index policies, with a first efficient policy, \UCB, that uses upper confidence bounds (UCB) on the means estimates, and is known to be order-optimal for bounded rewards.
Two other well-known and optimal algorithms are exposed: \klUCB{} extends the idea of \UCB{} but use smaller confidence intervals and thus it typically obtains better theoretical results, and Thompson sampling which replaces the principle of ``optimism under uncertainty'' by a Bayesian point-of-view.
%
Finally, we conclude by briefly exposing other families of algorithms.


\paragraph{Implementation.}
%
We describe our library SMPyBandits in more details in Chapter~\ref{chapter:3}, but all the algorithms described in this chapter are implemented in SMPyBandits, in the \texttt{Policies} module, alongside with many more algorithms (there are about 65 for single-player stochastic problems).
A complete list of the implemented policies can be found on the following web page on the documentation,
\href{https://smpybandits.github.io/docs/Policies.html}{\texttt{SMPyBandits.GitHub.io/docs/Policies.html}}.


% ---------------------------------------------------
\subsection{Naive or simple strategies}
\label{sub:2:naiveSimpleStrategies}


\paragraph{Naive strategies: pure exploitation or pure exploration}

Let us first describe two naive strategies, that both fail dramatically.
We recall the notations introduced above in Section~\ref{par:2:interactiveDemoDiscoverMAB}, the sums of rewards are $X_k(t) = \sum_{s=1}^t r_k(s) \mathbbm{1}(A(s) = k)$, and the numbers of samples are $N_k(t) = \sum_{s=1}^t \mathbbm{1}(A(s) = k)$.
%
The estimated means, or empirical averages, are $\widehat{\mu_k}(t) = X_k(t) / N_k(t)$ (when $N_k(t)>0$).

% Why Follow-the-Leader (EmpiricalMeans https://smpybandits.github.io/docs/Policies.EmpiricalMeans.html) don't work, example.
-- The ``\emph{Follow-the-Leader}'' strategy consists in first playing once each arm, then always playing $A(t)\in\argmax \widehat{\mu_k}(t)$.
The player only exploits the collected information, and this strategy can fail dramatically, \ie, obtain linear regret in some problems.
Indeed consider $K=2$ Bernoulli arms of means $\mu_1=1/2$ and $\mu_2=\varepsilon$ where $\varepsilon < 1/2$, then with probability $\varepsilon/2$ the player observes a reward of $0$ for arm $1$ then $1$ for arm $2$ on the first rounds, and so she will play arm $2$ for the $T-2$ remaining rounds, giving a linear (mean) regret $R_T \geq \frac{\varepsilon}{2}\left(\frac{1}{2} - \varepsilon\right) (T-1)$.

-- The uniform strategy always plays the $K$ arms uniformly at random.
The player only explores without using the collected information, and this strategy fails dramatically for any non trivial problem (\ie, linear regret).
Indeed it obtains a linear (mean) regret $R_T = \frac{1}{K} \sum_{k=1}^K \Delta_k T$
which gives $R_T \propto T$ for any problem with at least one sub-optimal arms (\ie, all problems expect where $\mu_1=\mu_2=\dots=\mu_K$).


% ---------------------------------------------------
\paragraph{Simple efficient strategies: $\varepsilon$-greedy and Explore-then-Exploit}

As illustrated by the two previous examples, an efficient strategy needs to solve the trade-off between exploration and exploitation.
The two following solutions both consist in splitting the $T$ time steps into $T_0$ steps of exploration and $T-T_0$ steps of exploitation.
%  either in an alternative way or in a fixed way (``explore then exploit'').

% https://smpybandits.github.io/docs/Policies.EpsilonGreedy.html
-- The $\varepsilon$-greedy strategy consists in alternating exploration and exploitation at a certain ratio \cite{SuttonBarto2018,Bubeck12,LattimoreBanditAlgorithmsBook}.
Fix $0<\varepsilon<1$, then at each round, with probability $\varepsilon$ the player selects an arm uniformly at random (exploration) and with probability $1-\varepsilon$ the arm with highest empirical mean is selected (exploitation).
If $\varepsilon$ is constant, then the (mean) regret is still growing linearly as it is lower bounded by $\varepsilon \frac{1}{K} \sum_{k=1}^K \Delta_k T$.
In this first case, in average $T_0 = \varepsilon T$ steps are spent on exploration.
%
But if we consider $\varepsilon_t$ decreasing with time $t$, for instance $\varepsilon_t = \varepsilon_0 \frac{1}{t}$ with $\varepsilon_0 = 6 K / d^2$, for a constant $0 < d < \min_{k: \Delta_k > 0} \Delta_k$,
then it was shown in \cite{Auer02} that the regret is of the order of $K \log(T) / d + \smallO{T}$, which leads to an order-optimal regret of $\bigO{\log(T)}$.
In this second case, about $T_0 \leq \varepsilon_0 \log(T)$ steps are spent on exploration.


% https://smpybandits.github.io/docs/Policies.ExploreThenCommit.html
-- The ``explore-then-exploit'' strategy, as studied for instance in \cite{GarivierETC2016}, consists in first exploring uniformly the $K$ arms for $T_0$ time steps, then only exploiting the best arm.
We can lower bound the regret of this strategy by
$R_T \geq K (\min_{k: \Delta_k > 0} \Delta_k) T_0/K + p (T - T_0)$,
if $p$ denotes the probability that the chosen arm is not optimal (\ie, the arm with highest mean after $T_0/K$ samples of each arms).
And we can show that $p$ only depends on the number of collected samples $T_0$ and the gaps $\Delta_k$,
so if $T_0$ is fixed independently of $T$ and the problem difficulty, the regret is again growing linearly, \ie, $R_T = \Omega(T)$.

\begin{proof}
    We prove it for the case of two arms.
    If the distributions are Bernoulli (or sub-Gaussian), we can use Hoeffding's inequality from \cite{hoeffding1963probability} to control this probability $p$.
    %
    Indeed if $Z_1,\dots,Z_n$ are \iid{} samples from a Bernoulli distribution of parameter $\mu$, of mean $\widehat{Z_n}$ we have
    \begin{equation}
        \forall x < \mu, \bP(\widehat{Z_n} < x) \leq \exp(-2 n (x-\mu)^2),
        \;\;\;
        \forall x > \mu, \bP(\widehat{Z_n} > x) \leq \exp(-2 n (x-\mu)^2).
    \end{equation}
    %
    If $\mu_1 = \mu_2 + \Delta$, then
    $p = \bP(\widehat{\mu_1} < \widehat{\mu_2}) \leq \bP(\widehat{\mu_1} < \mu_1 - \Delta/2) + \bP(\widehat{\mu_2} > \mu_2 + \Delta/2)$, and both terms can be bounded by using Hoeffding's inequality with $n=T_0/2$ samples,
    to obtain $p \leq 2 \exp(-T_0 \Delta^2 / 4)$.
    %
    For $K$ arms, the same proofs applies, and we can obtain
    $p \leq 2 K \exp(-T_0 d^2 / 4)$, if $d$ is a lower-bound on the positive gaps.
    %
    In all cases, it shows that $p$ is a constant wrt $T$.
\end{proof}

Using the same analysis, if a lower-bound $d$ on the positive gaps is known beforehand, one can find a tuning of $T_0$ that gives a logarithmic regret.
%
Like for the lower-bound, we have $R_T \leq (\max_{k: \Delta_k > 0} \Delta_k) T_0 + p (T - T_0)$, thus Hoeffding's inequality gives $R_T \leq K T_0 + K \exp(-T_0 d^2/4) T$. Optimizing on $T_0$ gives $T_0 = 4/d^2 \log(d^2 T / 4)$
and proves that the ``explore-then-exploit'' strategy can also obtain an order-optimal regret, $R_T \leq K 4/d^2 (1 + \log(d^2 T / 4)) = \bigO{\log(T)}$, if it is tuned with a fixed time $T_0$ using prior knowledge on the problem (\ie, $d$) and the horizon $T$.
%
This strategy can also achieve $R_T = \bigO{T^{2/3} (K \log(T))^{1/3}}$ without prior knowledge on the problem, as shown in Section~1 of \cite{Slivkins2019}.

An extension of this strategy is to consider not a fixed time $T_0$ but a random time $\tau$ at which exploration stops.
This time $\tau$ must be a \emph{stopping time} in the sense that it is a measurable random variable, dependent of the past observations, as introduced by \cite{Wald45}. The strategy is then referred to as ``explore-then-commit'' (ETC), and the idea is to use a statistical test at every time step $t$, and stop as soon as enough samples were collected to effectively identify the best arm with a certain confidence level $\delta$.
Choosing $\delta \propto 1/T$ and using a lower-bound $d$ on the gaps typically lead to an order-optimal algorithm as shown in \cite{GarivierETC2016}.


For both cases, the strategy obtains sub-optimal regret if it is tuned independently of the problem at hand, but they can be tuned to be efficient (\ie, with logarithmic regret) if a lower-bound on the gaps $\Delta_k$ is known.
%  \ie, they require a prior knowledge on the problem difficulty.
As such, this weakness make them unapplicable on an unknown problem, and so these first strategies are less interesting from a practical point-of-view.


% ----------------------------------------------------------------------------
\subsection{Index policies, \UCB, \klUCB{} and others}

A large family of algorithms are index policies, that compute an index $U_k(t)$ on each arm $k$ at time $t$.
The indexes should represent the expected quality of each arm, thus index policies play the arm that maximizes their index, \ie, $A(t) = \argmax_k U_k(t)$.
If more than one index maximizes $(U_k(t))_k$, the arm is chosen among the set, usually in a uniform random manner: $A(t) \sim \cU(\argmax_k U_k(t))$.
%
The Algorithm~\ref{algo:2:indexPolicy} below details a generic index policy, that include well known and efficient algorithms such as \UCB, \klUCB{} and many others.

% \begin{small} % XXX remove if needed
\begin{figure}[h!]
	\centering
    \begin{framed}
	\begin{algorithm}[H]
		% \begin{small} % XXX remove if needed
		\For(){$t = 1, \dots, T$}{
            \uIf{$t \leq K$}{
                Play arm $A(t) = t$\;
            }
			\Else(){
                For each arm, compute the index $U_k(t)$\;
                Play arm $A(t) = \arg\max_{1\leq k \leq K} U_k(t)$\;
            }
            Observe a reward $r_{A(t)}(t)$\;
            Update both $X_k(t)$ and $N_k(t)$\;
		}
		\caption{A generic index policy, using indexes $U_k(t)$ (\eg, \UCB, \klUCB{} etc).}
		\label{algo:2:indexPolicy}
		% \end{small} % XXX remove if needed
	\end{algorithm}
	\end{framed}
\end{figure}
% \end{small} % XXX remove if needed


\paragraph{The \UCB{} index policy: using Hoeffding's inequality to build confidence intervals.}

Let us consider another approach for adaptive exploration, known as ``optimism under uncertainty'': assume each arm is as good as it can possibly be given the observations so far, and choose the best arm based on these optimistic estimate.
%
This intuition leads to the $\UCB_1$ algorithm, initially introduced in \cite{Auer02}.
For a parameter $\alpha$, the \UCB{} indexes are computed as follows, as a sum of
the \emph{average reward} $\widehat{\mu_k}(t)=\frac{X_k(t)}{N_k(t)}$
and a \emph{confidence radius} $\xi_k(t) = \sqrt{\alpha \frac{\log\left(t\right)}{N_k(t)}\right)}$.
%
\begin{align}\label{eq:UCB_index}
    U_k^{\UCB}(t) = \underbrace{\frac{X_k(t)}{N_k(t)}}_{\widehat{\mu_k}(t)} + \underbrace{\sqrt{\alpha \frac{\log\left(t\right)}{N_k(t)}\right)}}_{\xi_k(t)}.
\end{align}

This selection rule makes sense for the following reason:
an arm $k$ is chosen at step $t$ because it has a large $U_k^{\UCB}(t) = \UCB_k(t)$, which can happen for two reasons:
$i)$ because the average reward $\widehat{\mu_k}(t)$ is large, in which case this arm is likely to have a high reward,
$ii)$ and/or because the confidence radius $\xi_k(t)$ is large, in which case this arm has not been explored much.
Either reason makes this arm worth choosing.
%
Furthermore, these two terms $\widehat{\mu_k}(t)$ and $\xi_k(t)$ in the \UCB{}  respectively represent exploitation and exploration, and summing them up is a natural way to trade off the two.
The parameter $\alpha$ in $\xi_k(t)$ also controls this trade-off, and the theoretical analysis suggests to restrict to $\alpha\geq1/2$, and to chose $\alpha=1/2$ for optimal uniform performance.

To analyze the algorithm $\UCB_1$, we denote again $k^*$ an optimal arm, and $A(t)$ is the arm chosen by the algorithm in round $t$.
According to the algorithm, $\UCB_k(t) \geq \UCB_{k^*}(t)$.
%
Under the clean event,μ(at) +rt(at)≥ ̄μt(at)andUCBt(a∗)≥μ(a∗). Therefore:
XXX
It follows that
XXX

More details on the proofs can be found for instance in Chapter~2 of \cite{Bubeck12}, or Section~1.3 of \cite{Slivkins2019}.
% \TODOL{Expliquer la fin}
The $\UCB_1$ algorithm achieves order-optimal problem-dependent bounds with a $\cO(K\log(T)/\Delta^2)$ regret.

It has the advantage of not relying on any prior knowledge of the problem difficulty, of being anytime, and computationally simple, for both memory and time aspects.
The $\UCB_1$ algorithm requires a constant storage capacity for each arm (storing $X_k(t)$ and $N_k(t)$ require two integers or a \texttt{float} and an integer, \ie, $\bigO{1}$), giving a storage capacity of $\bigO{K}$, independently of the horizon $T$.
It also needs to perform some mathematical operations, but a square-root and a logarithm are cheap, thus it costs a $\bigO{1}$ constant time for each arm and time step, thus a time $\bigO{K T}$ for the $T$ rounds.
%
We use it as the base building block for other policies in Chapter~\ref{chapter:4}, for these reasons.
Moreover, the $\UCB_1$ algorithm has the advantage of being easy to explain and understand, and its decisions can be interpreted clearly: by reading the values of $\widehat{\mu_k}(t)$ and $\UCB_k(t)$, anyone can see why an arm was chosen at anytime.
This is why it is used in our demonstration presented in Section~\ref{sec:4:gnuradio}.


\paragraph{The \klUCB{} index policy: using optimal confidence intervals.}

\TODOL{Expliquer klUCB}
\TODOL{Expliquer que les indices klUCB = des UCB sur les moyennes + principle of optimism in face of uncertainty}

Optimal upper confidence bounds based on Kullback-Leibler divergence and klUCB indexes

If rewards lie in $[a,b]$, then the indexes are computed as follows:
\begin{align}\label{eq:klUCB_index}
    U_k^{\klUCB}(t) = \sup\limits_{q \in [a, b]} \left\{ q : \kl\left(\frac{X_k(t)}{N_k(t)}, q\right) \leq \frac{c \log(t)}{N_k(t)} \right\}.
\end{align}

\TODOL{Short note on how to implement such policy, using Newton's method ?}


\paragraph{Variants on \UCB}

Many variants of the \UCB{} algorithms have been studied in the multi-armed bandits literature.
For instance, \UCB-H replaces the $\log(t)$ by a $\log(T)$ (as well as \klUCB-H), and it actually corresponds to the first algorithm analyzed in \cite{Auer02}.
Using an estimator for the variance of the arms' samples gives \UCB-V, which was for instance proven to be efficient against Gaussian distributions with unknown variance.
Other variants include \UCB$^+$, $\UCB^{\dagger}$ from \cite{Lattimore2018refining}, {\UCB}oost, and many more.
%
All these variants are usually comparable to the original \UCB{} algorithm, in terms of time and memory complexity, and they achieve improved regret upper bounds in the same setting or some extensions of the initial setting.


\paragraph{Other policies inspired by \UCB}
%
The ``Successive Elimination'' policy also uses confidence bounds, and use both Upper and Lower Confidence Bounds (LCB).
After playing each active arms once, it eliminates arm $a$ if it is statistically identified as sub-optimal, \ie, if there is another arm $a' \neq a$ such that $\UCB_a(t) < \mathrm{LCB}_{a'}(t)$.
It also achieves a $\bigO{\log(T)}$ problem-dependent regret upper bound, and a $\bigO{\sqrt{K T \log(T)}}$ problem-independent bound.


% ----------------------------------------------------------------------------
\subsection{Bayesian policies}

% Idea behind Thompson sampling and Bayesian philosophy.

% The Beta posterior and others (https://smpybandits.github.io/docs/Policies.Posterior.html) and Thompson Sampling.

Thompson sampling is the oldest known MAB algorithms, dating back from the first paper by Thompson in 1933 \cite{Thompson33}.
It became popular since its asymptotical and finite-time analyses in \cite{AgrawalGoyal11,Kaufmann12Thompson}.
%
Thompson sampling uses a Bayesian point of view:
instead of building confidence intervals on the means of the arms like \UCB{} does,
it starts with a prior on the $K$ distributions (\eg, a flat uniform prior), and then after every observation $(k, r(t))$ it updates the posterior distribution of the arm $k$ with a new data $r(t)$.
If $\pi_k(t)$ denotes this posterior distribution, Thompson sampling makes its decision at time $t$ by first sampling a random sample $s_k \sim \pi_k(t)$, and then playing according to the ``optimism under uncertainty'' philosophy if these samples represent the arms (expected) quality: $A(t) \in \argmax_k s_k$.

For problems with Bernoulli distributions on the $K$ arms,
the best choice of prior is a Beta distribution, that starts as an initially uniform prior (\ie, $\mathrm{Beta}(1,1)$),
and maintain a distribution $\mathrm{Beta}(X_k(t)+1,N_k(t)-X_k(t)+1)$,
as $X_k(t)$ counts the number of successes observed on arm $k$, and $N_k(t)-X_k(t)$ counts the number of failures.
%
We detail the Thompson sampling algorithm in this case, below in Algorithm~\ref{algo:2:ThompsonSampling}, as it corresponds to the case used in Chapter~\ref{chapter:4} in Section~\ref{sub:41:numericalResults}.
%
In this setting as well as for Gaussian distributions,
Thompson sampling was proven to achieve asymptotical optimal problem-dependent bounds \cite{Kaufmann12Thompson,AgrawalGoyal11}.


% \begin{small} % XXX remove if needed
\begin{figure}[h!]
	\centering
    \begin{framed}
	\begin{algorithm}[H]
        % \begin{small} % XXX remove if needed
		\For(){$t = 1, \dots, T$}{
            For each arm, sample $s_k(t) \sim \pi_k(t-1)$ from the posterior of arm $k$, $\pi_k(t-1)=\mathrm{Beta}(X_k(t-1)+1,N_k(t-1)-X_k(t-1)+1)$\;
            Play arm $A(t) = \arg\max_{1\leq k \leq K} U_k(t)$\;
            Observe a reward $r_{A(t)}(t)$\;
            Update both $X_k(t)$ and $N_k(t)$\;
		}
		\caption{Thompson Sampling for Beta prior and posteriors, on Bernoulli rewards.}
		\label{algo:2:ThompsonSampling}
		% \end{small} % XXX remove if needed
	\end{algorithm}
	\end{framed}
\end{figure}
% \end{small} % XXX remove if needed


Bayes-UCB from \cite{Kaufmann12BUCB} also uses the Bayesian point of view, updated like for Thompson sampling, but used in a different way.
Instead of sampling from the posteriors and playing the arms with maximum sample, Bayes-UCB computes the $1-1/t$ quantile of each arm, at time $t$, and plays the arm with the largest quantile.
It was also proved to achieve asymptotical optimal problem-dependent bounds \cite{Kaufmann12BUCB}.
%
Bayes-UCB is comparable to Thompson sampling in terms of memory complexity, but it can be slower in terms of time complexity as computing a $1-1/t$ quantile is generally more costly than sampling from a distribution.
% a Beta prior and posterior, initially uniform (\ie, $\mathrm{Beta}(1,1)$) (\texttt{BayesUCB}).
% But in particular for Bernoulli distributed arms and for Beta posteriors and priors, Bayes-UCB is only about twice as slow as Thompson sampling.

For curiosity, we mention a third Bayesian algorithm, AdBandits introduced in \cite{Truzzi13}, mainly because it has not gained any popularity despite its good empirical performance.
%  using $\alpha=1$ (\texttt{AdBandits}).
AdBandits follows the same assumptions as Thompson sampling and Bayes-UCB, and for instance for Bernoulli problems it uses a Beta posterior on each arm.
It uses a random mixture between the two algorithms, and with a certain probability it uses the Thompson sampling decision rule, otherwise it uses the Bayes-UCB decision rule.
It was proposed in this article \cite{Truzzi13} and illustrated on empirical simulations, but no theoretical analysis was given.
We found that in practice it can achieve very good performance, as illustrated in Section~\ref{sec:3:reviewSPAlgorithms}.
%  and it did not gain popularity since then.
% In practice, AdBandits is usually (slighly) more efficient than both Thompson sampling and Bayes-UCB in terms of regret, it is comparable in terms of computation times, but costs more memory.

Finally, we would like to mention that a recent work \cite{KimTewari2019} presented an approach that closes the gap between the principle of optimism under uncertainty and the Bayesian point-of-view.
%  underlying the approaches based on UCB, might not be necessary at all.
% With perturbation: maybe we don't need optimism in face of uncertainty, we just need uniform sampling!
The authors of \cite{KimTewari2019} showed that their Randomized Confidence Bound (RCB) algorithm can achieve the same regret upper bounds as \UCB{} or Thompson sampling, with uniform perturbations.
Their RCB algorithm ressembles \UCB, and builds the same confidence intervals $\mathrm{LCB}_k(t), \UCB_k(t)$ on arm $k$ at time $t$. But instead of playing the arm with maximum UCB, a random sample is taken on each interval, and the played arm is the one maximizing this sample.
% https://smpybandits.github.io/docs/Policies.RandomizedIndexPolicy.html


% ----------------------------------------------------------------------------
\subsection{Other policies}

We can briefly mention other families of policies.

% It's a good opportunity to show that I worked on my own on some non-standard policies.

% - One of the first paper I read in 2016 was defining AdBandits, https://smpybandits.github.io/docs/Policies.AdBandits.html, a mixture between Thompson sampling and BayesUCB (very efficient in practice)
% - I especially love the BESA algorithm by [? and ? and Maillard].

BESA (Best Empirical Sampled Average) was introduced in \cite{Baransi2014}, in order to fix a simple observation:
for instance in \UCB, if arm $1$ has $100$ samples and arm $2$ has $50$ samples, it should make more sense to only use $50$ samples of arm $1$ to compute and compare any statistics for each arms (\eg, their empirical mean).
The BESA algorithm for $K=2$ arms is then very simple: at each time step, if arms $1$ and $2$ have respectively $n_1$ and $n_2$ samples, and if $n_1>n_2$ (for instance), first it sub-samples $n_2$ observations from $n_1$, then it computes the mean of these observations, and play the arm maximizing this mean (now that there is the same number of observations for both arms, it makes more sense to compare these quantities).
It was analyzed partially, for the two-armed case it was proved to be asymptotically optimal for one-dimensional exponential families, and the authors illustrated that it can be very efficient empirically.
Analyzing it in the generic case was found to be difficult, and it is still an open problem.
% We implemented it using a binary tournament implement in a naive but efficient way (not using recursive functions, \ie, using the \texttt{BESA} class with \texttt{''non\_recursive''=True}).
% By using such binary tournament, the extension $K>2$ arms is costly in terms of both time and memory, as illustrated below, and suffers from an exponential blow-up when $K$ increases.


% ----------------------------------------------------------------------------
\paragraph{Policies for adversarial bandits.}

Another important family of bandits algorithms are the algorithms proposed for the adversarial setting \cite{Auer02NonStochastic}.
The first of them is the Exp3 policy, for ``\emph{Exp}onential weights for \emph{Exp}loitation and \emph{Exp}loiration'',
which is an example of the well-known \emph{multiplicative weights update algorithm}\footnote{~For more details see
on Wikipedia \href{https://en.wikipedia.org/wiki/Multiplicative_weight_update_method}{\texttt{en.wikipedia.org/wiki/Multiplicative\_weight\_update\_method}},
or this blog post by Jeremy Kun \href{https://jeremykun.com/2017/02/27/the-reasonable-effectiveness-of-the-multiplicative-weights-update-algorithm/}{\texttt{JeremyKun.com/2017/02/27/the-reasonable-effectiveness-of-the-}}\\
\href{https://jeremykun.com/2017/02/27/the-reasonable-effectiveness-of-the-multiplicative-weights-update-algorithm/}{\texttt{multiplicative-weights-update-algorithm}},
which comes with an interactive online demonstration to help understanding the behavior of the algorithm, hosted at \href{https://j2kun.github.io/mwua/index.html}{\texttt{j2kun.GitHub.io/mwua/}} (accessed the 24th of May 2019). More details can be found in the survey \cite{Arora2012multiplicative}.},
a family of algorithms developed since the 1950s, and applied to a wide range of different domains.
% https://smpybandits.github.io/docs/Policies.Exp3.html
%
The Exp3 policy maintains weights $w_k(t)$ on the $K$ arms, and at time $t$ it samples an arm from the distribution which is a mixture of $\bm{w}(t)=[w_1(t),\dots,w_K(t)]$ and the uniform distribution:
$\Pr(A(t)=k) = (1-\gamma) w_k(t) + \gamma \frac{1}{K}$.
The weights are initially uniform, \ie, $w_k(0)=\frac{1}{K}$, and then at each time after observing a reward $r_k(t)$ from arm $k$, Exp3 updates the weight of the chosen arm multiplicatively, using $w_k(t+1) = w_k(t) \times \exp(\widehat{r_k}(t) / (\gamma N_k(t)))$. The weights are then renormalized, the have a sum $\sum_k w_k(t+1) = 1$.
This notation $\widehat{r_k}(t)$ means an unbiased estimator of the reward $r_k(t)$, that is, $\widehat{r_k}(t) = \Pr(A(t)=k)$.
%
Like for the $\varepsilon$-greedy policy, the parameter $\gamma$ can be constant, or can be a decreasing sequence $(\gamma_t)_t$.
%
It was proved in \cite{Auer02NonStochastic} that Exp3 with constant temperature parameter $\gamma$ achieves $R_T = \bigO{\sqrt{K T \log(T)}}$ problem-independent regret,
while using a decreasing sequence, such as $\gamma_t = \sqrt{\log(K) / K} \log(t) / t$ gives an order-optimal problem-independent regret upper-bound of $\bigO{\sqrt{K T}}$.
For a short proof of this results as well as more details on Exp3, see \cite{Bubeck12}.
%
% https://smpybandits.github.io/docs/Policies.Exp3PlusPlus.html
Finally, it is worth mentioning the Exp3$^{++}$ algorithm, as a recent variant of Exp3, proposed by \cite{Seldin17},
that uses an adaptive tuning of its parameters.
It is anytime, and was proven to obtain good ``best of both world'' performances, meaning that it achieves $\cO(K\log(T)/\Delta^2)$ regret for problem-dependent bounds, and $\cO(\sqrt{KT})$ for problem-independent bounds.



% ----------------------------------------------------------------------------
\paragraph{Hybrid policies}

As explained before, in the last few years, the MAB research community has been interested in finding algorithms which achieves both a problem-dependent logarithmic and a minimax upper-bounds.
The first example is MOSS from \cite{Audibert2009minimax}, or recently \KLUCBpp{} in \cite{Menard17}.
They are both index-based, using the following indexes:
\begin{align}\label{eq:2:indexes_for_MOSS_klUCBpp}
    U^{\mathrm{MOSS}}_k(t) &= \frac{X_k(t)}{N_k(t)} + \sqrt{\max\left(0, \frac{\log\left(\frac{t}{K N_k(t)}\right)}{N_k(t)}\right)}. \\
    U^{\KLUCBpp}_k(t) &= \sup\limits_{q \in [a, b]} \left\{ q : \kl\left(\frac{X_k(t)}{N_k(t)}, q\right) \leq \frac{g(N_k(t), T, K)}{N_k(t)} \right\}.
\end{align}
%
If we denote $g(t, T, K) = \log^+(y (1 + \log^+(y)^2))$, and $y = \frac{T}{K t}$, where $\log^+(x) = \max(0, \log(x))$.

Both algorithms are index policies, that are not anytime.
For any $K$, $T$, and problem instances $I$ in a certain family $\cI$ (\eg, with Bernoulli distributions),
they enjoy regret bounds like the following, for two constants $c_{\cA}$ and $c'_{\cA}$ that depend on the algorithm, and a constant $C_I$ that depend on the problem instance only: $R_T^{\cA} \leq \min( c_{\cA} \sqrt{K T}, c'_{\cA} C_I \log(T))$.

More recently, the \klUCB-switch from \cite{Garivier18} is the first algorithm to be proved to
enjoy simultaneously a distribution-free regret bound of optimal order $\sqrt{KT}$, and a distribution-dependent regret bound of optimal order as well, that is, matching the $C_I \ln(T)$ lower bound by \cite{LaiRobbins85}.
It is an index policy that uses a modified version of the indexes of the \klUCB$^+$ algorithm, for arms that are not sampled much, and a modified version of the indexes of the MOSS$^+$ algorithm, for the other arms.
To detail this, first let $f(T, K) = \lfloor (T/K)^{\gamma}\rfloor$ for $\gamma=1/5$,
then \klUCB-switch use the index defined as follows:
%
\begin{align}\label{eq:2:indexes_for_klUCBswitch}
    U^{\mathrm{MOSS}+}_k(t) &= \frac{X_k(t)}{N_k(t)} + \sqrt{\max\left(0, \frac{\log\left(\frac{T}{K N_k(t)}\right)}{N_k(t)}\right)}. \\
    U^{\KLUCB+}_k(t) &= \sup\limits_{q \in [a, b]} \left\{ q : \kl\left(\frac{X_k(t)}{N_k(t)}, q\right) \leq \frac{\log(T / (K N_k(t)))}{N_k(t)} \right\} \\
    U^{\klUCB\text{-}\mathrm{switch}}_k(t) &= \begin{cases}
        U^{\KLUCB+}_k(t) & \text{if } N_k(t) \leq f(T, K), \\
        U^{\mathrm{MOSS}+}_k(t) & \text{if } N_k(t) > f(T, K).
    \end{cases}.
\end{align}

% Recent work.
% https://smpybandits.github.io/docs/Policies.MOSS.html
% https://smpybandits.github.io/docs/Policies.IMED.html
% https://smpybandits.github.io/docs/Policies.klUCBPlusPlus.html
% https://smpybandits.github.io/docs/Policies.klUCBswitch.html

% Talk about these?
% https://smpybandits.github.io/docs/Policies.OCUCB.html
% https://smpybandits.github.io/docs/Policies.OSSB.html


FIXME?
\TODOL{Je dois conclure un peu cette section...}

% \newpage

% ----------------------------------------------------------------------------
\section{Different approaches on algorithm selection}
\label{sec:2:chooseYourPreferredBanditAlgorithm}

For any real-world applications of MAB algorithms,
several solutions have been explored based on various models, and for any model, typically there are many algorithms available, as we have seen above.
%
Thus, when a practitioner is facing a problem where MAB algorithms could be used, it is not an easy task to decide which algorithm to use.
It is hard to predict which solution could be the best for real-world conditions at every instants,
and even by assuming a stationary environment, when one is facing a certain problem but has limited information about it, it is hard to know beforehand which algorithm can be the best solution.

In this Section, we first present two naive approaches for selecting an algorithm when facing a new problem, and then we detail the online approach that uses a ``leader'' MAB algorithm running on top of a pool of ``followers'' algorithms, and we present our contribution that is a new ``leader'' algorithm based on \ExpQ.
This section is based on our article \cite{Besson2018WCNC}.


% ----------------------------------------------------------------------
\subsection{Motivation for online algorithm selection}\label{sub:25:introduction}

Many different learning algorithms have been proposed by the machine learning community,
and most of them depend on several parameters, for instance $\alpha$ for \UCB, the prior for Thompson sampling or BayesUCB,
the $\kl$ function for \klUCB{} etc.
Every time a new MAB algorithm $\Alg$ is introduced, it is compared and benchmarked on some bandit instance, parameterized by $\boldsymbol{\mu} = (\mu_1,\dots,\mu_K)$, usually by focusing on its expected regret $R_T^{\Alg}$.
%
For a known and specific instance, simulations help to select the best algorithm in a pool of algorithms.
But when one wants to tackle an \emph{unknown} real-world problem, one expects to be efficient against \emph{any} problem, of any size and complexity in a certain family:
ideally one would like to use an algorithm that can be applied identically against any problem of such family.


\paragraph{Naive approaches.}
%
On the one hand, a practitioner can decide to pick one algorithm, maybe because it seems efficient on other problems, or maybe because it is simple enough to be used in its application. It might be unrealistic to implement complicated algorithms on limited hardware such as embedded chips in a very low-cost IoT end-device, and for instance a practitioner could chose to only consider the \UCB{} algorithm (or other low-cost algorithms).
%
On the other hand, if prior knowledge on the application at hand is available, one could implement some benchmarks, and compare a set of algorithms on different problems. If a leader appears clearly, it is then possible to choose it for the application.


\paragraph{Illustrative example.}
%
For instance, if you know that the considered problem can either have $K$ arms with very close means, or one optimal arm far away from the other, two versions of a \UCB{} algorithms will perform quite differently in the two problems:
using a large $\alpha$, \ie, favoring exploration, will give low regret in the first case,
while using a low $\alpha$, \ie, favoring exploitation, will give low regret in the second case.
One approach can be to use an intermediate value, as $\alpha=1/2$ suggested by theory, but another approach could be to consider an aggregated vote of different versions of \UCB, each running with a different value of $\alpha$ (\eg, in a logarithmic grid), and let another learning algorithm decide which value of $\alpha$ is the best \emph{for the problem at hand}.

\paragraph{The online approach.}
%
Another possibility is to consider an online approach, which is interested in the case where the computation power or memory storage of the application is not a limitation factor, but where one cannot run benchmarks before deploying the application.
We consider a fixed set of algorithms, and we use another learning algorithm on top of this set, to learn \emph{on the fly} which one should be trusted more, and eventually, used on its own.

The aggregation approach is especially interesting if we know that the problem the application will face is one of a few known kinds of problems.
In such cases, if there are $N$ different sorts of problems, and if the practitioner has prior knowledge on it, one can use the naive approach to select an algorithm $\cA_i$ which should perform well on problem $i$, for $i\in[N]$,
and use the aggregation of $\cA_1,\dots,\cA_N$ when facing the unknown problem.

% To choose the best algorithm, three approaches can be followed:
% \begin{itemize}
%     \item
% \end{itemize}
% either extensive benchmarks are done beforehand -- if this is possible -- to select the algorithm and its optimal parameters, or an adaptive algorithm is used to learn \emph{on the fly} its parameters.

% We present a simple adaptive solution, that aggregates several learning algorithms in parallel and adaptively chooses which one to trust the most.

% \paragraph{Outline}
% This section is organized as follows.
% First, we explain in Section~\ref{sub:25:aggregation} how to combine such algorithms for aggregation.
% Then we present our proposed algorithm, called \Aggr, in Section~\ref{sub:25:Aggr},
% Finally, we present numerical experiments in Section~\ref{sub:25:numExp},
% on Bernoulli and non-Bernoulli MAB problems,
% comparing the regret of several algorithms against different aggregation algorithms.
% Theoretical guarantees are shortly discussed in Section~\ref{sub:25:theory}, and Section~\ref{sub:25:conclusion} concludes.


% \subsection{Naive approaches}

% As mentioned, there are two naive approaches

% use UCB or Thompson sampling, and forget about the rest!}
% First solution: be naive, only use UCB everywhere


% \subsection{Use prior knowledge about the future application to select beforehand the chosen algorithm}

% Second solution, if one has some prior knowledge about the domain or setting for which the learning algorithms will be deployed, she can run synthetic or real-world simulations, compare many algorithms before deployment, and select the best algorithm!


\subsection{Online algorithm selection with expert aggregation}

\input{2-Chapters/2-Chapter/IEEE_WCNC_2018.git/IEEE_WCNC__2018__Paper__Lilian_Besson__07-17.tex}

% A last solution is online algorithm selection, inspired from expert aggregation.
% Include here the discussion about expert aggregation and my \textbf{Aggregator} algorithm, see https://hal.inria.fr/hal-01705292


\newpage  % FIXME

% ----------------------------------------------------------------------------
\section{Conclusion}
\label{sec:2:conclusion}

In this chapter, we presented the multi-armed bandit model, focussing on a finite number of arms, and real-valued rewards.
Our main focus is on one-dimensional exponential families of distributions, and on stochastic and stationary problems.
By showcasing an interactive demonstration made available online,
% XXX ?
% (\href{https://perso.crans.org/besson/phd/MAB\_interactive\_demo/}{\texttt{perso.crans.org/besson/phd/MAB\_interactive\_demo/}}),
we presented the notations used in all this thesis.

The first contribution of this manuscript concludes this chapter, in Section~\ref{sec:2:chooseYourPreferredBanditAlgorithm}.
% and corresponds to our article \cite{Besson2018WCNC}.
We tackle the question of how to select a particular bandit algorithm when a practitioner is facing a particular (unknown) bandit problem.
Instead of always choosing a fixed algorithm, or running costly benchmarks before real-world deployment of the chosen algorithm, we advise to select a few candidate algorithms, where at least one is expected to be very efficient for the given problem, and use online algorithm selection to automatically and dynamically decide the best candidate.
We proposed an extension of the Exp4 algorithm for this problem, \Aggr, and illustrate its performance on some bandit problems.
%
The numerical simulations used our Python open-source library, SMPyBandits, that is presented in details in the next Chapter~\ref{chapter:3}.
The next chapter also includes simulations of the most important MAB algorithms that we presented above, on Bernoulli distributed problems of various sizes and durations.

We presented the simplest MAB model studied in this chapter, by focussing on one agent playing a stationary game.
Both hypotheses are removed or weaken in the rest of this manuscript,
% first in Chapter~\ref{chapter:4} where we consider many XXX
first by considering players facing a stationary problem in a decentralized way in Chapter~\ref{chapter:5},
and then by considering a single player facing a non-stationary or a piece-wise stationary problem in Chapter~\ref{chapter:6}.
%
For both directions, we present in the two final chapters natural extensions of the base model, and we detail our contributions that obtained state-of-the-art results for the two problems
of stationary multi-player and piece-wise stationary bandits.

We take another approach in Chapter~\ref{chapter:4}, where the MAB model is generalized to study decentralized learning of a large set of independent players, all having different activation times.
This extension is significantly harder than the two previously evoked ones, and we were unfortunately unable to obtain any strong theoretical results under these loose hypotheses.
This model is however more generic and as such it was found suitable for applications to Internet of Things (IoT) networks, where arms model orthogonal wireless channels, players model communicating devices (\ie, IoT end-devices) and rewards model successes or failures of a wireless packet sent by a device.


\paragraph{Possible future works.}
%
We focussed in this thesis on finite-arms and one-dimensional bandit problems,
and thus two possible directions of future works could be to extend our works
to MAB models with either multidimensional rewards, like contextual bandits, or infinite arms, like Lipschitz bandits.


% % ----------------------------------------------------------------------------
% \section{Appendix}
% \label{sec:2:appendix}

% \TODOL{Write this ?}