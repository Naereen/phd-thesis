%!TEX root = ../PhD_thesis__Lilian_Besson

% First chapter begins here
\chapter{The Single Player and Stationary Multi-Armed Bandits Model}
\label{chapter:2}
\minitoc

\newpage
\TODOL{L'abstract ici est trop long, mais je l'ai écrit avant de commencer le chapitre, pour avoir une idée de ce que je devais faire ! Je couperai à la fin !}

\paragraph{Abstract.}
%
In this chapter, we present the common base of all the models studied in this thesis:
the multi-armed bandit (MAB) model,
restricting to the single-player and stationary stochastic case.
We focus on decision-making models with a finite number of resources, called arms, and on stochastic models, where an arm is associated with a one-dimensional distribution.
In all this thesis, we only consider one-dimensional exponential families, and in particular Bernoulli distributions.
%
We review quickly the possible applications of bandits, that range from recommender systems and information retrieval to healthcare and finance. In the rest of this thesis, we focus on cognitive radio, an important field of research where bandits has already been applied with success in the last ten years.

We then explain the main measure of performance used in this thesis, defined as the regret and written $R_T^{\cA}$ for algorithm $\cA$ and horizon $T$. The regret compares the performance of the decision-making player and the oracle who knows in advance the best arm and always selects it.
Other measures of performance, such as best-arm identification, are not used later on in this document, but they are also shortly discussed.
%
We review the main theoretical results from famous previous works, that give a lower-bound on the regret of any algorithm, essentially showing that we cannot find an algorithm achieving a sub-logarithmic regret, in this stochastic MAB model ($R_T^{\cA} = \Omega(\log(T))$).

We give an overview of the main families of algorithms designed to tackle the stochastic MAB model, and we mainly focus on the \UCB, \klUCB{} and Thompson sampling algorithms, as they are used in the rest of the manuscript.
When presenting our library SMPyBandits in next Chapter~\ref{chapter:3}, designed to perform numerical simulations of MAB problem, we use it to compare the empirical performances of the most famous and most efficient algorithms, in terms of regret and by evaluating them on different problems.
%
In the point-of-view of numerical simulations or real-world applications of bandits on embedded systems, one must also take care of two other measures of performance: time and memory complexities, which can vary greatly between different families of algorithms.
We also empirically compare different algorithms for these two measures in Section~\ref{sec:3:timeAndMemoryCosts}.
%
We then give regret upper-bounds of the two main algorithms used in this thesis, \UCB{} and \klUCB, to highlight the difference between their finite-time regret upper-bounds. They both attain a logarithmic regret, proving their order optimality (\ie, their regret upper-bound asymptotically matches the lower-bound up-to a constant factor). Moreover, \klUCB{} is known to be optimal for a wide range of arms distributions.
% A sketch of the proof of both results is given in Appendix.

Finally, we explore the problem of algorithm selection: as there are many different algorithms, all having different qualities and properties, how one can decide which algorithm to use for a particular application?
We present one of the first contribution made during my PhD \cite{Besson2018WCNC}, consisting of an efficient algorithm for aggregating algorithms.
We illustrate its performance empirically, but no satisfactory theoretical result is given, as the bound we had hoped for have been proved unachievable since then.


\paragraph{Publication.}
%
Section~\ref{sec:2:chooseYourPreferredBanditAlgorithm} in this chapter is based on the article \cite{Besson2018WCNC}.


\paragraph{Main references for the curious reader.}
%
For more details and a more formal introduction to multi-armed bandits, we suggest to the interested reader to work on a very recent text-book by Slivkins \cite{Slivkins2019}.
Another excellent but reasonably short survey is the book by Bubeck and Cesa-Bianchi \cite{Bubeck12}, while the more recent book by Lattimore and Szepesv{\'a}ri \cite{LattimoreBanditAlgorithmsBook} is the most complete resource about bandit algorithms.
Finally, we recommend \cite{bouneffouf2019survey} for a short but good survey on applications of MAB.

The notations used in this thesis are the usual notations in all the MAB literature, and we follow closely the formalism of \cite{Kaufmann12PhD}, as well as \cite{Slivkins2019,LattimoreBanditAlgorithmsBook,Bubeck12}.
The Nomenclature includes a summary of the notations, see Page~\pageref{chapter:nomenclature}.


\newpage
% Write miniTOC just after the title
\graphicspath{{2-Chapters/2-Chapter/Images/}}


% ----------------------------------------------------------------------------
\section{Stationary MAB models}
\label{sec:2:notations}


Multi-Armed Bandits (MAB) models were introduced by Thompson as early as in 1933 \cite{Thompson33}, and later studied from the 1950s by Robbins \cite{Robbins52} and others.
This family of models was first proposed for clinical trials, and later applied to a wide range of different problems.
Their name refers to one-armed bandits found in casinos, as showed in Figure~\ref{fig:2:example_of_a_5_arm_bandit_problem} below.
% The seminal paper by Lai and Robbins \cite{LaiRobbins85} was the first to introduce the measure of performance widely used in the literature, the notion of \emph{regret}, and to analyze a lower-bound on the performance of any algorithm against a fixed problem.
% Since then, many efficient algorithms were proposed, XXX


A MAB is a model of a decision-making game, where a \emph{player} has to sequentially select one action in a (usually finite) set of actions, and only receives a (random) feedback about the selected action, also called a \emph{reward}.
We consider $K\in\N, K\geq2$ arms, and discrete times $t\in\N^*$.
When the player decides at time $t$ to play (or pull) the arm $A(t)\in\{1,\dots,K\}=[K]$, she receives a reward $r_{A(t)}(t)\in\R$, and so on for a finite number of steps $t$, from $t=1$ until $t=T$ for an horizon $T$.
%
A commonly studied goal for the player is to maximize its sum of received rewards, $\sum_{t=1}^T r_{A(t)}(t)$
(or its mean in stochastic and stationary models).


The difficulty of this decision-making game comes from balancing the trade-off between \emph{exploration}, because the player has to observe as much as possible all the arms to get more knowledge about the distributions of their rewards, and \emph{exploitation}, because she also has to select as much as possible the best arm.
The MAB model is a famous example of a reinforcement learning model, where the decision-making process has to adapt to an unknown environment using noisy observations and a discrete time, alternating between decisions and observations.
We illustrate this cycle in Figure~\ref{fig:2:ReinforcementLearningCycleMABmodel} below.
For more details on reinforcement learning, in particular about more generic models, we refer to the famous book \cite{SuttonBarto2018}.


\tikzstyle{block} = [align=center, draw, fill=gray!25, rectangle, minimum height=3em, minimum width=6em]
\begin{figure}[h!]
    \centering
    \resizebox{0.50\textwidth}{!}{
        \begin{tikzpicture}[auto, node distance=5cm, >=latex]
            %
            % We start by placing the blocks
            \node [block] (player) at (0,0) {Player};
            % We draw an edge between the player and system block to
            \node [block] (environment) at (4,0) {MAB problem};
            % Once the nodes are placed, connecting them is easy.
            \draw [->] (player) to[bend left=90] node[pos=0.5] {Chooses a discrete action $k=A(t) \in\{1,\dots,K\}$} (environment);
            \draw [->] (environment) to[bend left=90] node[pos=0.5] {Observes a real-valued reward $r_k(t) \in \R$} (player);
            %
        \end{tikzpicture}
    }
\caption{Reinforcement learning cycle in a MAB model, for time steps $t=1,\dots,T$.}
\label{fig:2:ReinforcementLearningCycleMABmodel}
\end{figure}


This quantity $\sum_{t=1}^T r_{A(t)}(t)$ is indeed random, and depends on two aspects.
First, the rewards depend on the randomness of the environment, \ie, the unknown and unpredictable values $r_k(t)$.
Then, this quantity also depends on the player's decision-making policy, \ie, the choices $A(t)$, that are based on (all) the past observations, \ie, $A(1)$, $r_{A(1)}(1)$, $\dots$, $A(t-1)$, $r_{A(t-1)}(t-1)$, and possibly on an external source of randomness.
%
Without loss of generality, it can be given by a sequence $U_0,U_1,\dots$ of \iid{} random values, following the uniform distribution on $[0,1]$, and that has to be independent from the rewards $r_k(t)$.
External randomness is used in most MAB algorithms, for instance to break ties uniformly in an index policy.


% ----------------------------------------------------------------------------
\subsection{Finite arms and stochastic rewards}


In all this thesis, we focus on the model with finite arms, and binary or real-valued stochastic rewards, that is $A(t)\in\{1,\dots,K\}$ for a fixed and known number of arms $K\in\N,K\geq2$, and $r_{A(t)}(t)\in\R$.
%
Restricting to models with a finite number of arms
rules out an entire domain of the recent research on MAB, in particular we do not consider linear nor contextual bandits (for more details, see Parts V and VI of \cite{LattimoreBanditAlgorithmsBook}).
%
We also restrict to \emph{stochastic} rewards, that is we associate a real-valued distribution to each arm, denoted $\nu_k$ for arm $k\in\{1,\dots,\}=[K]$.
Rewards are stationary, meaning that $(r_k(t))_{t\in\N^*}$ is independent and identically distributed (\iid), $r_k(t) \sim \nu_k$ for any $t\geq1$.


\paragraph{An interactive demo to discover the MAB problem (for the novice).}
%
If you are discovering the concept of bandit model here, I would like to recommend you to go online and play a few times with an interactive demonstration.
On this demo, you will be facing a MAB problem with $K=5$ arms, and you have $T=100$ decisions to make.
The demo is hosted on my website, at \href{https://perso.crans.org/besson/phd/MAB\_interactive\_demo/}{\texttt{perso.crans.org/besson/phd/MAB\_interactive\_demo/}}, and illustrated below.

% \begin{figure}[h!]  % [htbp]
%     \centering
%     \includegraphics[width=0.75\linewidth]{2-Chapters/2-Chapter/Images/example_of_a_5_arm_bandit_problem__step0.png}
%     \caption{See }
%     \label{fig:2:example_of_a_5_arm_bandit_problem__step0}
% \end{figure}

\begin{figure}[h!]  % [htbp]
    \centering
    \includegraphics[width=0.85\linewidth]{2-Chapters/2-Chapter/Images/example_of_a_5_arm_bandit_problem.png}
    \caption{Screenshot of the demonstration available online on my website, for a current step of $t=24$.}
    \label{fig:2:example_of_a_5_arm_bandit_problem}
\end{figure}


The webpage looks like Figure~\ref{fig:2:example_of_a_5_arm_bandit_problem} above.
The arms follow Bernoulli distributions, \ie, $\nu_k = \mathcal{B}(\mu_k)$, of unknown means $\mu_k\in[0,1]$, and your goal in this interactive demonstration is to obtain the highest possible cumulated reward in $100$ steps, $\sum_{t=1}^{100} r_{A(t)(t)$.
Your decisions have to be made sequentially: at time $t$, you pick one of the arms, $A(t) \in\{1,2,3,4,5\}$, then the demo shows the random reward obtained from this (virtual) casino machine (in \textcolor{gold}{yellow}), \ie, the binary value $r_{A(t)}(t)\in\{0,1}$, that is sampled \iid{} from $\nu_k$.
%
The UI of the demo also shows the current value of $t$ (``total plays'') and $\sum_{s=1}^t r_{A(s)}(s)$ the ``total reward''.
For each arm, we show the sum of rewards obtained from that arm, \ie, $X_k(t) = \sum_{s=1}^t r_k(s) \mathbbm{1}(A(s) = k)$, in the ``Rewards'' line, and the number of pulls of that arm, \ie, $N_k(t) = \sum_{s=1}^t \mathbbm{1}(A(s) = k)$ in the ``Pulls'' line.
%
The demo also shows the estimated probability of each arm, that is $\widehat{\mu_k}(t) = X_k(t) / N_k(t)$ (when $N_k(t)>0$), in the ``Estimated probs'' line.

In the first Figure~\ref{fig:2:example_of_a_5_arm_bandit_problem}, the current state of the game is shown at time $t=24$.
% Currently at time $t=24$ (out of $T=100$ total time steps),
At this step, the player has collected a sum of rewards of $14$, by observing $X(t) = [6,2,2,2,2]$ rewards of value $1$ in the $K=5$ different arms. Arms were sampled $N(t) = [8,4,4,4,4]$ times, meaning that the value $0$ was seen respectively $[2,2,2,2,2]$ times, and currently arm $1$ appears to be the best one. The true means of the arms are $\bm{\mu}=[0.6, 0.2, 0.55, 0.7, 0.5]$, and (much) more samples are needed before the player can accurately identify arm $4$ as the best arm.
%
In the second Figure~\ref{fig:2:example_of_a_5_arm_bandit_problem__step100} below, we display the result of an example of run, when the player was following the \UCB{} algorithm from \cite{Auer02}.
After $T=100$ steps, the player obtained a cumulated reward of $56$, by playing mostly arms $4,3,5,1,0$ (in decreasing order of number of plays). The empirical means $\widehat{\mu_k}(T)$ correctly identify the best arm (arm $4$), but do not correctly rank the arms as arms $1$ and $3$ obtained means of $\widehat{\mu_1}(T) = 0.5 < \widehat{\mu_3}(T)=0.6$ but the true means are $\mu_3 = 0.55 < \mu_1 = 0.6$.

\begin{figure}[h!]  % [htbp]
    \centering
    \includegraphics[width=0.75\linewidth]{2-Chapters/2-Chapter/Images/example_of_a_5_arm_bandit_problem__step100.png}
    \caption{Screenshot of the demonstration, after the end of the game, after $t=T=100$ steps.  See \href{https://perso.crans.org/besson/phd/MAB\_interactive\_demo/}{\texttt{perso.crans.org/besson/phd/MAB\_interactive\_demo/}}}
    \label{fig:2:example_of_a_5_arm_bandit_problem__step100}
\end{figure}


\paragraph{A bandit policy is a measurable function.}

A bandit algorithm $\cA$, also referred to as a strategy or a policy, and sometimes denoted by $\pi$ or $\rho$, selects an arm $A(t)$ at time $t$ by using the past observations and the past external randomness.
More formally, an algorithm is thus a sequence of measurable functions $(\cA_t)_{t\geq1}$,
where $\cA_t$ maps the past observations $\cO_t = (U_0, Y_1, U_1, \dots, Y_{t-1}, U_{t-1})$
to an arm $\cA_t(\cO_t) = A(t) \in[K]$,
if we denote $Y_s = r_{A(s)}(s)$ she $s$-th reward.
The initial information is reduced to $\cO_1 = (U_0)$, and the first decision is $A(1) = \cA_1(\cO_1)$. Usually, most algorithms starts by selecting $A(1)=1,\dots,A(K)=K$ (or a permutation of the $K$ arms) in the $K$ first steps.
%
An algorithm is said to be deterministic if it does not depend on the external randomness $U_0,U_1,\dots$, but in this thesis we only use non-deterministic algorithms.

The most common objective for the player is to maximize its expected rewards $\E[\sum_{t=1}^T Y_t] = \sum_{t=1}^T \E[r_{A(t)}(t)]$.
The expectancy is taken on the rewards $(r_k(t))_{k,t}$, and the extern random variables $(U_s)_s$.
%
Other common objectives include best-arm identification (BAI) \cite{audibert2010best}.


% ----------------------------------------------------------------------------
\subsection{Different hypotheses on rewards distributions}

In the example above, we consider Bernoulli distributions, but other distributions have been studied in the literature.
From now on until the last chapter of this thesis, we only consider stochastic rewards. The piece-wise stationary model is studied in Chapter~\ref{chapter:6}.
%
% Everything is implemented and documented at
% https://smpybandits.github.io/docs/Arms.html
We also focus only on real-valued rewards, meaning that $r_k(t)\in\R$ for all $k$ and $t$.

An important hypothesis is whether rewards are bounded or not,
and whether the player knows if they are bounded or not before starting the bandit game.
Moreover, if rewards are known to be bounded, let say in an interval $[a,b]$, another important hypothesis is whether the player knows the values of $a$ and $b$.
%
Intuitively, the bandit game is easier if the player knows the support of the distributions, and we restrict to this case in all the thesis.
Most of the algorithms proposed in the literature follow this hypothesis as well.
%
The mostly used
infinitely supported distributions are Gaussian, exponential and Poisson,
while in the literature, the Bernoulli distribution is the most common case of finitely supported distributions.
Continuous distributions with finite support also included truncated versions of infinitely supported distributions, in particular truncated Gaussian are used for numerical experiments in lots of research articles.

% Just bounded, in $[a,b]$ but without loss of generality in $[0,1]$ etc.
\paragraph{The normalization trick.}
%
If the player knows that reward are bounded in an interval $[a,b]$, and if she knows $a$ and $b$ ($a<b$), then with no loss of generality we can restrict to the interval $[0,1]$, as if $r\in[a,b]$, the player can instead consider the normalized reward $r' = $ that lies in $[0,1]$.
Note that this ``normalization trick'' is implemented for any policy in SMPyBandits, with the \texttt{lower} and \texttt{amplitude} optional arguments, respectively representing $a$ the lower-bound on rewards and $b-a$ the length of the interval of possible values of rewards.

% Bernoulli, Gaussian, sub-Bernoulli, sub-Gaussian, Exponential, sub-Exponential...

\paragraph{On one-dimensional exponential family.}
%
% On one-dimensional exponential family...

In all this thesis we restrict to distributions that have a certain form, and focus on real-valued distributions lying in an exponential families.
One-dimensional exponential families include Bernoulli and Poisson distributions, as well as Gaussian distributions with a fixed variance.
Our main interest is Bernoulli distributions, but we prefer to present here the notations on a generic exponential family.
We follow the notations from the course on statistical learning (Stat 260) taught in 2010 by Michael Jordan at the University of Berkeley \cite{JordanCourseStatBerkeley}, that are also the notations used for instance in Emilie Kaufmann's PhD in 2014 \cite{Kaufmann12PhD}.

Given a measure $\lambda$, that is usually the natural Lebesgue measure on $\R^d$, an exponential family of probability distributions is defined as the distributions whose density (relative to $\lambda$) have the following form:
\begin{equation}\label{eq:2:exponentialFamily}
    \Pr(x | \lambda) = h(x) \exp \let( \eta^T T(x) - A(\eta) \right),
\end{equation}
for a parameter vector $\eta$ (the canonical parameter), and given functions $T$ (the sufficient statistic) and $h$.
$A(\eta)$ is the cumulant function, and it is entirely determined by the $\eta$, $h$, $T$,
$A(\eta) = \log \left( \int h(x) \exp(\eta^T T(x)) \lambda(\mathrm{d} x) \right)$.
%
The natural parameter space is the set of values of $\eta$ such that this integral $A(\eta)$ is finite,
and usually the literature focusses on regular and minimal exponential families (when the natural parameter space is an non-empty open set in $\R$).

-- An important result on exponential families is that a distribution in such family is entirely characterized by its parameter $\eta$.
That is why such one-dimensional distributions are entirely characterized by their mean, as $\mu = \E_{\eta}[T(X)]$.

-- A second important result is a simplified form for the Kullback-Leibler divergence \cite{KullbackLeibler51}, for two distributions lying in the same exponential family.
The KL divergence between two distributions $d_1=\Pr(x|\eta_1)$ and $d_2=\Pr(x|\eta_2)$ is defined as
\begin{equation}\label{eq:2:Kullback-LeiblerDivergenceExpFamily1}
    \KL\left(\Pr(x|\eta_1), \Pr(x|\eta_2)\right) = \int \Pr(x|\eta_1) \log(\frac{\Pr(x|\eta_1)}{\Pr(x|\eta_1)}) \lambda(\mathrm{d}x) = \E_{d_1}[ \log(\frac{\Pr(x|\eta_1)}{\Pr(x|\eta_1)}) ].
\end{equation}
%
So this KL divergence can be written $\KL( \eta_1, \eta_2 ) = \KL(\Pr(x|\eta_1), \Pr(x|\eta_2))$, and it can be simplified to an expression using only the two parameters $\eta_1$ and $\eta_2$,
and $\mu_1 = \E_{\eta_1}[T(X)]$ the mean of distribution $d_1$:
%
\begin{equation}\label{eq:2:Kullback-LeiblerDivergenceExpFamily2}
    \KL\left( \eta_1, \eta_2 \right) = \E_{d_1}[ (\eta_1 - \eta_2)^T T(X) ] - A(\eta_1) + A(\eta_2) = (\eta_1 - \eta_2)^T \mu_1 - A(\eta_1) + A(\eta_2),
\end{equation}


Without diving more in the details of exponential families,
it is interesting to illustrate this definition and the notations with two important examples:
%
\begin{itemize}
    \item
    On the first hand, Bernoulli distributions can be seen as an exponential family with
    $T(x) = x$ and $h(x) = 1$,
    and for a Bernoulli distribution of mean $\mu\in[0,1]$,
    the parameter is $\eta = \mu / (1 - \mu)$, giving $A(\eta) = \log(1 + \mathrm{e}^{\eta})$
    (with the limit behavior $\eta=+\infty$ if $\mu=0$).

    \item
    On the other hand, Gaussian distributions use
    $T(x) = [x ; x^2]^T$ and $h(x) = 1/\sqrt(2\pi)$,
    and for $\cN(\mu,\sigma^2)$ a Gaussian distribution with mean $\mu\in\R$ and variance $\sigma^2$,
    $\eta = [\mu/\sigma^2 ; -1/\sigma^2]^T$, giving $A(\eta) = -\eta_1^2/4\eta_2 -1/2 \log(-2\eta_2)$ (see Chapter~8 of \cite{JordanCourseStatBerkeley}).
    For a generic variance, this form shows that the Gaussian distributions are a two-dimensional exponential families,
    and fixing the variance $\sigma$ gives indeed a one-dimensional exponential family.
\end{itemize}


In the rest of this thesis, \emph{we only consider bounded rewards in the mathematical developments}, and we mostly focus on Bernoulli distributions.
First, we restrict for simplicity to Bernoulli distributions for the simulations shown in Chapter~\ref{chapter:3}, even if our library does implement all the distributions commonly found in the literature (including unbounded one-dimensional exponential families and even Markov models).
% Then in Chapter~\ref{chapter:4}, each static IoT device in the noisy environment is assumed to be following a Bernoulli emission pattern in a fixed channel, FIXME
%
For example when we study multi-player bandits in Chapter~\ref{chapter:5}, for the model without sensing, we explain that restricting to the Bernoulli case is interesting and it is actually the hardest one, as continuous distributions with a null mass on $r(t)=0$ yield a much simpler problem.
%
Finally, in Chapter~\ref{chapter:6} we analyze our proposed algorithm in the setting on bounded distributions, without restricting to Bernoulli distributions, but we use the fact that bounded distributions on $[0,1]$ are sub-Bernoulli, and we use Bernoulli Kullback-Leibler divergences in our analysis.


\paragraph{Other kind of hypothesis: sub-Gaussian and sub-Bernoulli distributions.}
%
A lot of research works considers rewards distributions that are not Gaussian but sub-Gaussian, of a known variance, for instance $1/4$.
For instance, bounded distributions on $[0,1]$ are known to be $1/4$ sub-Gaussian, and this fact is used for instance in \cite{Maillard2018GLR}.
It means that their moment generating function is dominated by that of a Gaussian distribution with the same mean and a variance $1/4$.
In Chapter~\ref{chapter:6}, we instead consider sub-Bernoulli distributions, formally introduced in Definition~\ref{def:6:subBernoulliDistributions}.
%
Such hypothesis is also proposed for other families of distributions, for instance \cite{KimTewari2019} analyses their Follow-the-Perturbed-Leader algorithm for perturbations following any sub-Weibull distribution (sub-Weibull distributions generalize both sub-Gaussian and sub-Exponential distributions).



\paragraph{About Markov models.}
%
% Markov models: we do not present it in this thesis.
Finally, we note that Markov models, while being implemented in SMPyBandits, are not used in this thesis.
They were introduced in the 1980s, by Whittle in \cite{Whittle1988} and Anantharam and others in \cite{Anantharam87b}.
A Markov MAB model maps an arm to a Markov chain \cite{Norris98}, instead of a distribution, and such models come in two flavors: rested or restless.
For $K$ arms, each Markov chain has a finite number of state $s$, each corresponding to a (constant) reward that the player obtains if she selects this arm while its Markov chain is in state $s$.
Rested Markov models means that only the state of the selected arm's Markov chain can change, following its Markov transition matrix.
Restless models remove this hypothesis, making them harder to track and solve.
%
Such models were less studied than stationary or adversarial models, but some interesting works focussed on Markov models in the last 10 years.
For instance, \cite{Melian15} proposed a cognitive radio model mixing MAB and Hidden Markov Models (HMM), solved by a mixed policy called UCB-HMM.

\TODOL{On garde ça sur les modèles de bandits Markovien, par curiosité ?}


% ----------------------------------------------------------------------------
\section{Applications of stationary MAB}
\label{sec:2:applicationsofStationaryMAB}

The blooming success of the research on multi-armed bandits is easily explained by the different spectrum of applications of MAB models to real-world discrete-time decision making problems.
This research field has been very active since the years 2010s, but it started as early as 1933 with \cite{Thompson33}, and was active since the 1980s and the seminal works by Lai and Robbins \cite{LaiRobbins85} and by Anantharam and others \cite{Anantharam87a,Anantharam87b}.
%
MAB have been applied successfully to various decision making problems, including clinical trials and cognitive radio that we mentioned above.

Historically, MAB models were first applied to clinical trials, where an arm represents a treatment, and the distribution associated with such treatment can be a Bernoulli distribution: a reward of $0$ means the drug did not heal the disease, and a reward of $1$ indicates a success. The mean of an arm, in this application, represents the mean success rate of a treatment, and the goal of a doctor in a clinical trial is to identify the best treatment, \ie, the arm with highest mean, in a number of trials as short as possible.
%
The focus of this work is on cognitive radio and IoT networks, where arms can represent wireless orthogonal channels, but more generally any resource characterizing the communication between a wireless device and a gateway (\eg, spreading factor for LoRa \cite{KerkoucheAlami18}, power allocation for NOMA etc ). In cognitive radio using centralized supervision, for instance if the gateway can decide the allocation of devices to resources, MAB can also be used to let the gateway explore different allocations and learn by itself a good allocation, see for instance this article that consider 5G-like networks with small cells \cite{Maghsudi16}.
%
% After introducing formally the notations used in all this thesis, we review below possible applications of multi-armed bandits.


Other popular applications include the following:
%
\begin{itemize}
    \item
    A/B testing of websites is a popular application of the best arm identification problem,
    where the task is purely exploratory and the player is asked to identify the best of two options (or more), in a finite number of steps \cite{audibert2010best}.
    The problem can either consider a fixed budget and no freedom on the ending of the game (\ie, an known horizon $T$), or a fixed confidence (\ie, the identified arm must be the true best arm with probability at least $1-\delta$) and a certain freedom on the budget \cite{Garivier16BAI}.
    The theoretical complexity of this use of bandit for A/B testing was first studied in \cite{Kaufmann14},
    and later on a more practical point-of-view was proposed in \cite{Jamieson17ABTest}.

    \item
    MAB can also be applied to a broader setting of online content recommandation,
    with more than two options.
    The seminal work of \cite{Li10} studies the application of contextual bandit to news article recommendation, as it is used in practice on platforms such as Microsoft's Bing news website,
    or in applications like Netflix.
    In such models, the arms correspond to items to recommend (\eg, articles or movies), and the contexts contain features about each user of the system.
    An interesting work is \cite{Louedec16}, who studies slowly-varying non-stationary models applied to recommender systems.

    \item
    Using bandit algorithms to improve machine learning models or algorithms has been an active research domain for the last ten years or so.
    As presented below in Section~\ref{sec:2:chooseYourPreferredBanditAlgorithm}, a certain ``master'' bandit algorithm can be used to select on the run the best bandit algorithm from a pool of ``slave'' algorithms.
    Other possible use cases include hyper-parameter optimization, or feature selection.
    Hyper-parameters include real-valued parameters, like $\alpha\in[0,\infty)$ for the \UCB{} or $\varepsilon$ for the $\varepsilon$-greedy bandit algorithms, a step size multiplier in a gradient method, or the width of a radial based function (RBF) kernel method.
    Discrete-valued parameters are also common, like a choice in a fixed set of kernel functions or the depth of neural networks,
    and higher dimensional or more complex hyper-parameters include the entire architecture of a neural network.

    % FIXME this is copied pasted from \cite{bouneffouf2019survey}
    It is well known that performance of machine learning algorithms depends critically on identifying a good set of hyper-parameters.
    While recent approaches use Bayesian optimization to adaptively select optimal hyper-parameter configurations, they rather focus on speeding up random search through adaptive resource allocation and early-stopping.
    A different approach was initiated in \cite{LiJamieson2018}, by formulating hyper-parameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem, where predefined resources, such as iterations, data samples, or features are allocated to randomly sampled configurations. This work introduced a novel algorithm, Hyperband, for this framework and analyzed its theoretical properties, providing several desirable guarantees.
    Furthermore, Hyperband was compared with popular Bayesian optimization methods on a suite of hyper-parameter optimization problems; it was observed that Hyperband can provide more than an order-of-magnitude speedup over its competitors on a variety of deep-learning and kernel-based learning problems.

\end{itemize}

% - clinical trial
% - A/B testing of websites
% - online content recommandation
% - wireless communication, lots of possibilities, one is frequency/channel selection in the OSA paradigm, detailed after
% - hyper-parameters tuning of deep neural networks etc

The three applications detailed above are still active research directions,
and a curious reader will find other interesting applications of MAB models and algorithms in \cite{bouneffouf2019survey}.


\paragraph{Applications for Opportunistic Spectrum Access.}
% Detail the previous work from our team SCEE on bandits + OSA : Wassim, Navik
%
Previous works of our SCEE team showed that MAB can be used to model the problem of spectrum access for a secondary user accessing a licensed spectrum.
In this model, arms represent a finite set of orthogonal channels, \ie, different frequency bands in a licensed spectrum.
In the model with sensing, the rewards $r_k(t)$ represents the feedback obtained by the CR-equipped device after sensing the channel $k$ at time $t$.
A reward of $r_k(t) = 1$ indicates that no Primary User was sensed, while a reward of $r_k(t)=0$ indicates that the channel $k$ is busy at time $t$ and no uplink message can be sent.
%
This model was first introduced by Wassim Jouini during his PhD thesis,
in \cite{Jouini09} and later studied in both \cite{Jouini10,Jouini12}.
Proof-of-concepts using real-world radio hardware was first propsed in \cite{MoyWSR2014,RobertSDR2014}.
In a second PhD thesis supervised by Christophe Moy, Navikkumar Modi studied the impact
of using MAB algorithms to optimize channel selection
on the battery life of a wireless device.
On the one hand, running a MAB algorithm such as UCB-like algorithms was proved to be useful and can bring significant improvement in terms of successful transmission rates, directly increasing the battery life of the device.
On the other hand, classical MAB algorithms tend to switch arms a lot of times, especially in the beginning of the learning process, and this induces a lot of dynamical hardware reconfiguration for the wireless device, as selecting a different channel requires a change in the radio hardware used by the device.
Each hardware reconfiguration costs energy for the device, and quickly switching algorithms will lead to a reduction of the battery life.
The tradeoff between the two aspects is studied empirically in
\cite{modiDemo2016}.
%
Another interesting works is \cite{Modi17QoS}.

\TODOL{FIXME Finir ce blabla rapidement ? Ou alors ce sera déjà fait en intro/chapitre1 ??}


% ----------------------------------------------------------------------------
\section{Regret: definition, decomposition, and lower bounds}
\label{sec:2:lowerUpperBoundsRegret}

As explained above, the main objective of a player facing a bandit game is to maximize its (expected) cumulated reward.
In Machine Learning, we usually prefer to aim at minimizing certain quantity, such as error rate in supervised learning, or the distance to the optimum in an optimization problem.
We introduce in this section the notion of \emph{regret}, and the equivalence for a player between maximizing its sum of rewards and minimizing its regret.


\subsection{Measuring performance with the (mean) regret}

Let us first introduce some notations.
We consider from now on a stochastic and stationary MAB problem, with $K$ arms of distributions $(\nu_k)_k$, that is, for any time $t$, $r_k(t)\sim \nu_k$.
We denote $\mu_k$ the mean of the distribution of arm $k$ (it will be referred to as the mean of arm $k$ in the rest of the manuscript), that is $\mu_k = \E[r_k(t)]$.
%
% \paragraph{Defining the regret.}
%
To define the regret, we first need to distinguish between \emph{optimal} and \emph{sub-optimal} arms.

\begin{defn}[Optimal and sub-optimal arms]\label{def:2:optimalSubOptimalArms}
    Now consider $\mu^* = \max_k \mu_k$ the mean of the best arm.
    The best arm can be non unique, and any arm $k$ having $\mu_k = \mu^*$ is said to be \emph{optimal},
    while arms satisfying $\mu_k < \mu^*$ are called \emph{sub-optimal}.
\end{defn}

If the goal is to maximize $\E[\sum_{t=1}^T r_{A(t)}(t)]$,
the optimal strategy for this bandit problem is to always pull an optimal arm, but of course it is unrealistic as the player does not know the true means and cannot know which arm is optimal.
%
Comparing the difference between the performance a fixed baseline and that of the player is a common approach in machine learning research,
and here we can compare with the oracle strategy that obtains an expected reward of $\mu^*$ at each time step.
%
For a fixed horizon $T$, if $k^*$ denotes the index of any optimal arm,
let us introduce the (mean) regret $R_T^{\cA}$ of an algorithm $\cA$ as
\[ R_T^{\cA} = \E\left[ \sum_{t=1}^T r_{k^*}(t) - r_{A(t)}(t) \right]. \]
%
We can rewrite this to obtain the following definition of the regret.
\begin{align}
    R_T^{\cA}
    &= \E\left[ \sum_{t=1}^T r_{k^*}(t) \right] - \E\left[ \sum_{t=1}^T r_{A(t)}(t) \right] \\
    &= T \E\left[ r_{k^*}(1) \right] - \sum_{t=1}^T \E\left[ r_{A(t)}(t) \right] \\
    &= T \mu^* - \sum_{t=1}^T \E\left[ r_{A(t)}(t) \right].
\end{align}


\begin{defn}[Regret]\label{def:2:regret}
    For an algorithm $\cA$, a bandit problem of $K$ arms characterized with their means $\bm{\mu} = \mu_1,\dots,\mu_K$ and if $\mu^* = \max_k \mu_k$, then the (mean) regret at horizon $T$ is defined as
    \begin{equation}
        R_T^{\cA} = T \mu^* - \sum_{t=1}^T \E\left[ Y(t) \right = T \mu^* - \sum_{t=1}^T \E\left[ r_{A(t)}(t) \right].
    \end{equation}
\end{defn}

\TODOL{Est-ce qu'il faut autre chose ?}


\paragraph{A useful decomposition of the regret.}
%
As introduced above, let $N_k(t) = \sum_{s=1}^t \mathbbm{1}(A(s) = k)$ be the number of times arm $k$ was selected until between times $1$ and $t$.
Remember that the rewards $r_k(t)$ are all \iid{} of mean $\mu_k$.
Thus one can use the chain rule of expectation, and because the expectation is taken on the randomness of the rewards and of the decisions of the player,
one can rewrite the expected cumulated rewards like this:
%
\begin{align}
    \E \left[ \sum_{t=1}^T r_{A(t)}(t) \right]
    &= \E \left[ \sum_{k=1}^K \sum_{t=1}^T r_k(t) \mathbbm{1}(A(t) = k) \right] \nonumber\\
    &= \sum_{k=1}^K \E [ r_k(1) \underbrace{\sum_{t=1}^T \mathbbm{1}(A(t) = k)}_{= N_k(T)} ] \nonumber\\
    &= \sum_{k=1}^K \mu_k \E \left[ N_k(T) \right].
\end{align}

If we introduce the \emph{gap} $\Delta_k = \mu^* - \mu_k$ between any arm $k\in[K]$ and an optimal arm,
then an arm $k$ is sub-optimal if $\Delta_k > 0$, and we can write the following decomposition on the regret,

\begin{lemma}[Regret decomposition]\label{lem:2:RegretDecomposition}
    The (mean) regret $R_T^{\cA}$ can be written like
    \begin{equation}
        R_T^{\cA} = \sum_{k=1}^K \Delta_k \E[ N_k(T) ].
    \end{equation}
\end{lemma}
%
\begin{proof}\label{proof:2:RegretDecomposition}
    Thanks to the tower rule, we had
    $\E \left[ \sum_{t=1}^T r_{A(t)}(t) \right] = \sum_{k=1}^K \mu_k \E \left[ N_k(T) \right]$, thus
    \[ R_T^{\cA} = \sum_{k=1}^K \Delta_k \E[ N_k(T) ]. \]
    % FIXME more details?
\end{proof}

\TODOL{Est-ce qu'il faut autre chose ?}


\subsection{Regret lower bounds: do not detail too much, don't explain the tools behind the results}

\begin{itemize}
    \item
    - [Lai and Robbins] lower-bound in $\Omega(\log(T))$
    \item
    - Worst case lower-bound in $\Omega(\sqrt{T})$
    \item
    - Adversarial lower-bound in $\Omega(\sqrt{T})$ (also useful for piece-wise stationary models)
\end{itemize}


\subsection{Regret upper bounds, tools and different kinds of bounds}


\begin{itemize}
    \item
    - different kinds of bounds, problem-dependent with $\bigO{C \log(T)}$ with $C$ a measure of the problem complexity, or problem-independent with $\bigO{\sqrt{KT}}$ ?
    \item
    - tools to show regret upper-bounds, concentration inequalities?
    FIXME non j'ai pas la place
    \item
    - short proof of UCB or klUCB? just to start to show the tools
    FIXME non j'ai pas la place
\end{itemize}


\subsection{Other measures of performances}

- Best Arm Identification?
- Quantile regret or just histogram of regrets
- Worst case regret (max $R_T^{r}$ for $r$ index of Monte-Carlo simulation?)
- Fairness ?
- Switching cost? \cite{modiDemo2016} \cite{Koren17}


% ----------------------------------------------------------------------------
\section{Review of MAB algorithms}
\label{sec:2:famousMABalgorithms}

Everything is implemented and documented at
https://smpybandits.github.io/docs/Policies.html


\subsubsection{Why do we need smart algorithms?}
Why Follow-the-Leader (EmpiricalMeans https://smpybandits.github.io/docs/Policies.EmpiricalMeans.html) don't work, example.

\subsection{Simple but efficient algorithms when well tuned: $\varepsilon$-greedy and Explore-then-Commit}

See
https://smpybandits.github.io/docs/Policies.EpsilonGreedy.html
https://smpybandits.github.io/docs/Policies.ExploreThenCommit.html


% ----------------------------------------------------------------------------
\subsection{Index policies: UCB and others}

\subsubsection{UCB}
Optimism in face of uncertainty, for UCB etc.


\subsubsection{klUCB}
Optimal upper confidence bounds based on Kullback-Leibler divergence and klUCB indexes

\subsubsection{Other policies inspired by UCB}

- UCB-H and klUCB-H
- UCB-V
- UCB-dagger
- UCBoost
- UCB+ and klUCB+


% ----------------------------------------------------------------------------
\subsection{Bayesian policies: Thompson Sampling and others}

Idea behind Thompson sampling and Bayesian philosophy.

The Beta posterior and others (https://smpybandits.github.io/docs/Policies.Posterior.html) and Thompson Sampling.


% ----------------------------------------------------------------------------
\subsection{Other policies: BESA and others}

It's a good opportunity to show off and say that I worked on my own on some non-standard policies.

- One of the first paper I read in 2016 was defining AdBandits, https://smpybandits.github.io/docs/Policies.AdBandits.html, a mixture between Thompson sampling and BayesUCB (very efficient in practice)
- I especially love the BESA algorithm by [? and ? and Maillard].


% ----------------------------------------------------------------------------
\subsection{Policies for adversarial bandits: Exp3 and others}

Exp3, Exp3++ etc
https://smpybandits.github.io/docs/Policies.Exp3.html
https://smpybandits.github.io/docs/Policies.Exp3PlusPlus.html

Softmax: is it the same, but written differently?
https://smpybandits.github.io/docs/Policies.Softmax.html


% ----------------------------------------------------------------------------
\subsection{Hybrid policies: MOSS, klUCB++ and klUCB-Switch and others}

Recent work.
https://smpybandits.github.io/docs/Policies.MOSS.html
https://smpybandits.github.io/docs/Policies.IMED.html
https://smpybandits.github.io/docs/Policies.klUCBPlusPlus.html
https://smpybandits.github.io/docs/Policies.klUCBSwitch.html

Talk about these?
https://smpybandits.github.io/docs/Policies.OCUCB.html
https://smpybandits.github.io/docs/Policies.OSSB.html

With perturbation: maybe we don't need optimism in face of uncertainty, we just need uniform sampling!
Randomized Confidence Bound (RCB) with uniform perturbations performs as well as UCB!
https://smpybandits.github.io/docs/Policies.RandomizedIndexPolicy.html


\subsection{Summary of classification of the most famous algorithms}

A huge table in one page in portrait mode, giving:
- name
- reference, year
- family, classification
- time/memory complexity ?
- regret bound for stochastic regret

of about 20/30 algorithms?

\newpage

% ----------------------------------------------------------------------------
\section{Different approaches on algorithm selection}
\label{sec:2:chooseYourPreferredBanditAlgorithm}

\subsection{Naive approach: use UCB or Thompson sampling, and forget about the rest!}
First solution: be naive, only use UCB everywhere

\subsection{Use prior knowledge about the future application to select beforehand the chosen algorithm}
Second solution, if one has some prior knowledge about the domain or setting for which the learning algorithms will be deployed, she can run synthetic or real-world simulations, compare many algorithms before deployment, and select the best algorithm!

\subsection{Online algorithm selection with expert aggregation}

\input{2-Chapters/2-Chapter/IEEE_WCNC_2018.git/IEEE_WCNC__2018__Paper__Lilian_Besson__07-17.tex}

% A last solution is online algorithm selection, inspired from expert aggregation.
% Include here the discussion about expert aggregation and my \textbf{Aggregator} algorithm, see https://hal.inria.fr/hal-01705292


\newpage  % FIXME

% ----------------------------------------------------------------------------
\section{Conclusion}
\label{sec:2:conclusion}

In this chapter, we presented the multi-armed bandit model, focussing on a finite number of arms, and real-valued rewards.
Our main focus is on one-dimensional exponential families of distributions, and on stochastic and stationary problems.
By showcasing an interactive demonstration made available online,
% XXX ?
% (\href{https://perso.crans.org/besson/phd/MAB\_interactive\_demo/}{\texttt{perso.crans.org/besson/phd/MAB\_interactive\_demo/}}),
we presented the notations used in all this thesis.

The first contribution of this manuscript \cite{Besson2018WCNC} concluded this chapter, in Section~\ref{sec:2:chooseYourPreferredBanditAlgorithm}. We tackle the question of how to select a particular bandit algorithm when a practitioner is facing a particular (unknown) bandit problem.
Instead of always choosing a fixed algorithm, or running costly benchmarks before real-world deployment of the chosen algorithm, we advise to select a few candidate algorithms, where at least one is expected to be very efficient for the given problem, and use online algorithm selection to automatically and dynamically decide the best candidate.
We proposed an extension of the Exp4 algorithm for this problem, \Aggr, and illustrate its performance on some bandit problems.
%
The numerical simulations used our Python open-source library, SMPyBandits, that is presented in details in the next Chapter~\ref{chapter:3}.
The next chapter also includes simulations of the most important MAB algorithms that we presented above, on Bernoulli distributed problems of various sizes and durations.

We presented the simplest MAB model studied in this chapter, by focussing on one agent playing a stationary game.
Both hypotheses are removed or weaken in the rest of this manuscript,
% first in Chapter~\ref{chapter:4} where we consider many XXX
first by considering players facing a stationary problem in a decentralized way in Chapter~\ref{chapter:5},
and then by considering a single player facing a non-stationary or a piece-wise stationary problem in Chapter~\ref{chapter:6}.
%
For both directions, we present in the two final chapters natural extensions of the base model, and we detail our contributions that obtained state-of-the-art results for the two problems
of stationary multi-player and piece-wise stationary bandits.

We take another approach in Chapter~\ref{chapter:4}, where the MAB model is generalized to study decentralized learning of a large set of independent players, all having different activation times.
This extension is significantly harder than the two previously evoked ones, and we were unfortunately unable to obtain any strong theoretical results under these loose hypotheses.
This model is however more generic and as such it was found suitable for applications to Internet of Things (IoT) networks, where arms model orthogonal wireless channels, players model communicating devices (\ie, IoT end-devices) and rewards model successes or failures of a wireless packet sent by a device.


\paragraph{Possible future works.}
%
We focussed in this thesis on finite-arms and one-dimensional bandit problems,
and thus two possible directions of future works could be to extend our works
to MAB models with either multidimensional rewards, like contextual bandits, or infinite arms, like Lipschitz bandits.


% % ----------------------------------------------------------------------------
% \section{Appendix}
% \label{sec:2:appendix}

% \TODOL{Write this ?}

% \subsection{Efficient numerical computation of Kullback-Leibler divergences and \klUCB{} indexes}

% \TODOL{On s'en fout en fait de ça, non ? Ma thèse n'a pas étudiée klUCB, et les implémentations en C ne sont pas de moi mais viennent de pymaBandits... Donc inutile de parler de ça !}

% I want to detail the maths behind an implementation of klUCB, see https://smpybandits.github.io/docs/Policies.kullback.html and all what is detailed there.

% How to write a fast version of klBern and klucbBern (the two most used functions), using C or Numba or Cython, maybe I can include the speed up simulations here? First piece of code, before really presenting SMPyBandits ?

% My work for this part is also published on https://github.com/Naereen/Kullback-Leibler-divergences-and-kl-UCB-indexes
% (in Python) and https://github.com/Naereen/KullbackLeibler.jl (in Julia).

