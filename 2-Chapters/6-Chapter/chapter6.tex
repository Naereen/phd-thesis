
%!TEX root = ../PhD_thesis__Lilian_Besson

% \chapter{Non-Stationary MAB Models and Possible Applications for IoT Networks}
% \chapter{The Piece-wise Stationary MAB Model and the GLR-klUCB Algorithm}
\chapter{Piece-Wise Stationary Multi-Armed Bandits}
\label{chapter:6}

\graphicspath{{2-Chapters/6-Chapter/Images/}}

\paragraph{Abstract.}

In this last chapter, we study a generalization of the stationary multi-armed bandit model,
to account for possible non-stationarity of the rewards.
% % that is, for problems that could be tackled with MAB algorithms.
% Instead of dealing with changes happening possibly at every time step, like in adversarial models, we rather consider a finite small number $\Upsilon_T$ of \emph{change-points}, and we consider problems that are stationary on any interval between two consecutive change-points.
%
We review existing works on piece-wise stationary MAB models.
% for which the two main families of proposed solutions are either passively or actively adaptive. In practice, it was already observed that actively adaptive strategies usually greatly outperform the passively adaptive ones.
% The more efficient strategies from the current state-of-the-art are based on a combination of an efficient algorithm for the stationary problem with an efficient online change detection algorithm, like CUSUM, PHT or the Generalized Likelihood Ratio Test (GLRT), and so we pursue on this direction.
%
We study the Generalized Likelihood Ratio Test (GLRT) for
% Using finite-time results of a recent paper studying the GLRT for sub-Gaussian distributions, we then extend the results to the GLRT for any 
bounded or sub-Bernoulli distributions.
Our test enjoys finite-time guarantees
% its \emph{false alarm probability}, meaning that in a stationary model, the test should not detect any change,
% and its \emph{detection delay}, meaning that in a model with a change-point, the test should detect within a reasonable delay.
%
of its \emph{false alarm probability} and \emph{detection delay}, and can be combined with an efficient bandit policy (\klUCB), to propose an efficient algorithm for piece-wise stationary problems, \GLRklUCB.
% Our policy takes ideas from the two most recent and most efficient policies, \CUSUMUCB{} and \MUCB, that enjoys good regret bounds of the order of $R_T = \bigO{\sqrt{\Upsilon_T T \log(T)}}$, if $T$ is the horizon and $\Upsilon_T$ the number of change-points (known before hand).
% % \MUCB{} restarts the underlying \UCB{} bandit policy on all arms when a change is detected, while \CUSUMUCB{} restarts the observations of only the arm on which a change is detected.
%
% We give the first unified analysis of a piece-wise stationary bandit algorithm for the two variants, and our analysis
We analyze its regret, and show that it achieves state-of-the-art non-asymptotic regret bounds.
% and yield regret bounds with explicit constants.
% Moreover, our regret bounds are the first ones to be of the order $\bigO{\sqrt{\Upsilon_T T \log(T)}}$, when the algorithm runs with no prior knowledge of the problem other than the horizon $T$ and the number of change-points $\Upsilon_T$ (both \MUCB{} and \CUSUMUCB{} require to know before hand a certain measure $\Delta^{\text{change}}$ of the difficulty of the problem).
%
Finally, we compare our approach against other state-of-the-art solutions on extensive numerical experiments.
% , on different piece-wise stationary bandit problems.
% % , of increasing difficulty.
% %
% We also include additional details about our policy \GLRklUCB, including a sensitivity analysis of its two parameters, a numerical evaluation of its time and memory costs, as well as details on an easy but efficient way to speed up the B-GLRT test while not loosing too much in terms of regrets.

\minitoc
% ----------------------------------------------------------------------

\newpage


% This chapter is basically a raw include from my paper ``The Generalized Likelihood Ratio Test meets klUCB: an Improved Algorithm for Piece-Wise Non-Stationary Bandits'', see https://hal.inria.fr/hal-02006471

\input{2-Chapters/6-Chapter/nonstatbandits.git/NonStatB.tex}


% % ----------------------------------------------------------------------------
% \section{Conclusion}
% \label{sec:6:conclusion}

% In this chapter, we saw...

% Future works include...


\newpage
% ----------------------------------------------------------------------------
\section{Appendix}
\label{sec:6:appendix}

We start by including figures as complementary illustrations to the numerical results presented in Section~\ref{sec:6:NumericalExperiments} above.
We then give some additional useful results, along with their proofs, and then we give complementary numerical experiments to justify some choices made in the presentation of our proposal \GLRklUCB.

\input{2-Chapters/6-Chapter/nonstatbandits.git/NonStatB_appendix.tex}
