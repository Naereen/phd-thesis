
\TODOL{FIXME next paragraph is the short version of related works review I have to merge the long and short together and keep as much details as I can!}

The piece-wise stationary bandit model was first studied by \cite{Kocsis06,YuMannor09,Garivier11UCBDiscount}. It is also known as \emph{switching} (\cite{MellorShapiro13}) or \emph{abruptly changing stationary} (\cite{WeiSrivastava18}) environment. %We do not consider the \emph{slowly-varying} MAB models (\cite{Louedec16,WeiSrivastava18}), even if it is interesting for some applications.
% We review previous works that studied the piece-wise stationary bandit model.
To our knowledge, all the previous approaches combine a standard bandit algorithm, like \UCB{}, Thompson Sampling or EXP3, with a strategy to account for changes in the arms distributions. This strategy often consists in \emph{forgetting old rewards}, to efficiently focus on the most recent ones, more likely to be similar to future rewards. We make the distinction between \emph{passively} and \emph{actively} adaptive strategies.


The first proposed mechanisms to forget the past consist in either discounting rewards (at each round, when getting a new reward on an arm, past rewards are multiplied by $\gamma^n$ if that arm was not seen since $n>0$ times, for a discount factor $\gamma\in(0,1)$), or using a sliding window (only the rewards gathered in the $\tau$ last observations of an arm are taken into account, for a window size $\tau$).
Those strategies are passively adaptive as the discount factor or the window size are fixed, and can be tuned as a function of $T$ and $\Upsilon_T$ to achieve a certain regret bound.
Discounted UCB (D-UCB) was proposed by \cite{Kocsis06} and analyzed by \cite{Garivier11UCBDiscount}, who prove a $\cO(\sqrt{\Upsilon_T T }\log(T))$ regret bound, if $\gamma = 1 - \sqrt{\Upsilon_T/T}/4$.
The same authors proposed the Sliding-Window UCB (SW-UCB) and prove a $\cO(\sqrt{\Upsilon_T T \log(T)})$ regret bound, if $\tau = 2 \sqrt{T\log(T)/\Upsilon_T}$.


% \TODOE{Il faut faire attention pour DTS, tu attaques leur papier! Qu'est-ce que nos expériences montrent exactement qui est en contradiction avec ce qu'ils disent?}
% \TODOE{Je vire EXP3.S: c'est une version d'EXP3 qui a des garanties en terme de ``nombre de breakpoint'' dans un cadre adversarial, pas stochastique. Il n'est pas basé sur des restart et un discount}
% \TODOE{Vérifie que je raconte pas de conneries pour RExp3, Odalric et Subho l'avaient mis dans la catégorie ``passive'' mais je comprends pas trop comment marche l'algo}


%
The first \emph{actively adaptive} strategy is Windowed-Mean Shift (\cite{YuMannor09}), which combines any bandit policy with a change point detector which performs \emph{adaptive restarts} of the bandit algorithm. However, this approach is not applicable to our setting as it takes into account side observations. Another line of research on actively adaptive algorithms uses a Bayesian point of view.
A Bayesian Change-Point Detection (CPD) algorithm is combined with Thompson Sampling by \cite{MellorShapiro13}, and more recently in the Memory Bandit algorithm of \cite{Alami17}. Both algorithms do not have theoretical guarantees and their implementation is very costly, hence we do not include them in our experiments. Our closest competitors rather use frequentist CPD algorithms (see, e.g. \cite{Basseville93}) combined with a bandit algorithm.
% The idea behind Memory Bandit is to use an expert aggregation algorithm, like EXP4 from \cite{Auer02}, modified to efficiently aggregate an growing number of experts from techniques presented in \cite{Mourtada17}.
% At each time step, a new expert is introduced, and experts correspond to different Thompson sampling algorithms, each using a different history of pulls and rewards. Intuitively, after a change-point the newest experts will soon become the most efficient, as they are learning by using rewards drawn from the new distribution(s).
%
The first algorithm of this flavor, Adapt-EVE algorithm (\cite{Hartland06}) uses a Page-Hinkley test and the \UCB{} policy, but no theoretical guarantee are given. EXP3.R (\cite{Allesiardo15,Allesiardo17}) combines a CPD with EXP3, and the history of all arms are reset as soon as a a sub-optimal arm is detecting to become optimal and it achieve a $\cO(\Upsilon_T \sqrt{T \log(T)})$ regret. This is weaker than the $\cO(\sqrt{\Upsilon_T T \log(T)})$ regret achieved by two recent algorithms, \CUSUMUCB{} (\cite{LiuLeeShroff17}) and Monitored UCB (\MUCB, \cite{CaoZhenKvetonXie18}).