% https://perso.crans.org/besson/articles/BMBM__IEEE_WCNC_2019.pdf

\graphicspath{{2-Chapters/4-Chapter/IEEE_WCNC__2019__Paper__BMBM.git/}}

In this Section, we extend the model presented before in order to take into account the possibility for retransmission of messages after a collision.
As before, we propose and evaluate different learning strategies based on MAB algorithms.

The presented strategies allow IoT devices to improve their access to the network and their autonomy, while taking into account the impact of encountered radio collisions.
For that end, several heuristics employing \UCB{} algorithms are examined, to explore the contextual information provided by the number of retransmissions.
Our results show that approaches based on \UCB{} obtain a significant improvement in terms of successful transmission probabilities, compared to a naive approach which does not learn.
Furthermore, it also reveals that a pure \UCB{} channel access is as efficient as more sophisticated learning strategies.

% \TODOL{This Section is based on the publication ``Upper-Confidence Bound for Channel Selection in LPWA Networks with Retransmissions'', see \texttt{https://perso.crans.org/besson/articles/BMBM\_\_IEEE\_WCNC\_2019.pdf}}

\TODOL{TODO:

- Be sure to not repeat any motivations / maths / explanations given before
}


% ----------------------------------------------------------------------
\subsection{Retransmissions in an ALOHA like protocol}
\label{sub:43:introduction}
% ----------------------------------------------------------------------

In the context of Cognitive Radio \cite{Mitola99,Haykin05},
Multi-Arm Bandit (MAB) algorithms \cite{Auer02,Auer02,Bubeck12} have been recently proposed as a potential solution for channel access in LPWA networks \cite{Bonnefoi18,Azari18,Bonnefoi17}.
We presented in Section~\ref{sec:4:firstModel} the impact of non-stationarity on the network performance using MAB algorithms is studied.
Low-cost algorithms following two well-known approaches, such as the Upper-Confidence Bound (\UCB{}) \cite{Auer02,Auer02}, and the Thompson Sampling (TS) algorithms \cite{Thompson33} have reported encouraging results.
Other recent directions include theoretical analysis, like what we present in the next Chapter~\ref{chapter:5},
and realistic empirical simulations \cite{kumar2016two,kumar2017channel} or the previous Section~\ref{sec:4:gnuradio},
of the application of MAB algorithms for slotted wireless protocols in a decentralized manner,
or for instance applications to multi-hoping networks \cite{Mitton16,Toldov16}.
None of the above mentioned works discusses in detail the impact of retransmissions on the performance of MAB learning algorithms as we do now.

The aim of this Section is to assess the performance of MAB algorithms for channel selection in LPWA networks, while taking into account the impact of retransmissions on the network performance.
For this reason, several decision making strategies are applied after a first retransmission (\ie, when a collision occurs).
Proposed approach employs contextual information provided by the number of retransmissions, and implemented at each device, so that no coordination among them is needed.
Moreover, our \UCB{}-based heuristics show low complexity making them suitable for being embedded in LPWA devices.

The contributions of this Section are summarized as follows:
\begin{itemize}
	\item
	Firstly, we provide a close form approximation of the radio collision probability after a first retransmission.
	By doing this, we highlight the need to develop a learning approach for channel selection upon collision.

	\item
	Secondly, several heuristics are proposed to cope with retransmissions.

	\item Lastly, we conduct simulations in order to compare the performance of the proposed heuristics with a naive uniform random approach, and a \UCB{} strategy (\ie, without any learning for the retransmissions).
\end{itemize}


\paragraph{Outline}

The rest of the Section is organized as follows.
First the system model is introduced in Section~\ref{sub:43:model},
% A formal description of the MAB learning algorithms is given in Section~\ref{sub:43:motivations},
and our motivations are exposed in Section~\ref{sub:43:MABalgo}.
The proposed \UCB-based heuristics are presented in Section~\ref{sub:43:heuristics}, while the corresponding numerical results are shown in Section~\ref{sub:43:numExp}.
Finally, some conclusions are drawn in Section~\ref{sub:43:conclusion}.


% ----------------------------------------------------------------------
\subsection{Extending our model with retransmissions}
\label{sub:43:model}
% ----------------------------------------------------------------------

% ----------------------------------------------------------------------
\paragraph{LPWA network}

Like before in this chapter, we consider an LPWA network composed of a gateway and a large number of end-devices that regularly send short data packets, where $K$ channels ($K>1$) are available for the transmission of their packets.
%
We assume that this network is constituted by two types of devices:
on the one hand, we have \emph{static} devices that operate in one channel\footnote{~Note that, for unlicensed bands, this definition also encompasses any device following a different standard or trying to establish communication with gateways of other networks.} in order to communicate with the gateway.
%
On the other hand, there are  IoT devices, that possess the additional advantage of being able to select any of the $K$ available channels to perform their transmissions.

Regardless the type of devices, each of them follows a slotted ALOHA protocol \cite{Roberts75}, and has a probability $p>0$ to transmit a packet in a time slot.
We make the hypothesis that the transmission is successful if the channel is available, otherwise upon radio collision, these devices will attempt to retransmit their packet up-to $M$ times, with $M \in\mathbb{N}$.
Note that, every retransmission is carried out after a random back-off time, uniformly distributed in $\llbracket 0, m-1 \rrbracket$, where $m \in\mathbb{N}, m>0$ is the length of the back-off interval.


% ----------------------------------------------------------------------
\paragraph{Model of our IoT devices}

The aforementioned contention process can be described by a Markov chain model \cite{Norris98} similar to the one presented in \cite{Yang12}, as it is depicted in Fig.~\ref{fig:43:Markov_model}.
A device containing a packet for transmission goes from an idle state to a transmission state, while considering retransmissions due to different collision probabilities, \ie, $\{p_{c}, p_{c1}, \dots, p_{M-2} \}$, at each $M$ back-off stage.
At each time slot, a transition from an idle state to a transmission state (denoted as \texttt{Trans.}) occurs if a packet transmission is required, while waiting states (denoted as \texttt{Wait}), correspond to a $m$ back-off interval.

A device aims to select a channel with the highest probability of a successful transmission, for which it resorts to a reinforcement learning approach. It is formulated as MAB problem, where each channel (also called arms) is viewed as a gambling machine (bandit), and each bandit has a \emph{reward}. Then, at every trial, a device chooses a channel that maximizes the sum of the collected rewards. These \emph{rewards} are the \emph{acknowledgment} (\emph{Ack}) signals received after transmitting packets to the gateway. In this way, a transmission is considered successful when an acknowledgment is received, and a learning approach is employed to select the best channel.

We address the problem of channel selection taking into account the described Markov model for the retransmissions of end-devices.
% It motivates our present work for which we consider the retransmissions in the analysis of MAB algorithms.
It motivates our present work for which we consider the number of retransmissions, carried out by each device to address several MAB algorithms.

\begin{figure}[htp!]  % [htbp]
	\centering
	% \includegraphics[width=0.65\linewidth]{Markov_model.eps}
	\includegraphics[width=0.70\linewidth]{Markov_model-eps-converted-to.pdf}
	\caption{The considered Markov model for the behavior of all the devices in the network.}
	\label{fig:43:Markov_model}
\end{figure}
%

% ----------------------------------------------------------------------
\subsection{Motivations for the proposed approach}
\label{sub:43:motivations}
% ----------------------------------------------------------------------

When a device experiments a collision, it goes in a back-off state to retransmit the same packet on a channel.
If all devices remain in the same channel for retransmissions, it could result in a sequence of successive collisions with the same packets' devices that previously collided.
%
Thus, it seems interesting to consider in the decision making policy the possibility for a device to retransmit in a different channel.
One of our motivations to develop new MAB algorithms for our problem is this option of using a different communication channels between the first transmission and the next retransmissions.

By considering this possibility, the device will have to learn more, thus, we expect that the learning time to be longer, but it could be possible that the final performance gain (\ie, in terms of network performance) increases too.
The next Section~\ref{sub:43:numExp} presents analysis to check this performance gain, for various heuristics based on the \UCB{} algorithm.
%
Here after, we start by presenting a mathematical derivation that backups this idea.
To do so, we study the collision probabilities considering the Markov process depicted in Fig.~\ref{fig:43:Markov_model}, and foresee the impact of addressing bandit strategies, as well as setting guidelines for the design of heuristic approaches.


\paragraph{Probability of collision at a second transmission slot}

As it is well known, having a collision during an access time can be overcome by a retransmission procedure (this can take several retransmission attempts).
What interest us here, is to obtain a mathematical approximation of the collision probability at the second transmission slot $p_{c1}$, as a function of the first collision probability $p_{c}$.

We consider two hypotheses $\cH_{1}$ and $\cH_{2}$ defined as,
\begin{itemize}
	\item $\cH_{1}$:
    The probability $p_{c1}$ is composed by the sum of two probabilities: i)
    the probability of colliding consecutively twice, \ie, the devices that collide at a given time slot and collide again when retransmitting their packets,
    and ii) the probability of collision related with devices that attempt to transmit in this channel for first time.
    In addition, we suppose that the number of devices that attempt to retransmit is small in comparison to the total number of devices sharing the same channel.

	\item $\cH_{2}$:
	The total number of the back-off stages at time $t$ is constant, and it is assumed to be large enough to consider that no device will ever be in the last failure state (this case is the one on the right side in Figure~\ref{fig:43:Markov_model}), after $M$ successive failed retransmissions.
\end{itemize}

Considering one device and a channel,
we denote $x_t^i$ the probability that it is transmitting a packet for the $i+1$ time in a given time slot $t$ (with $i\in \llbracket 0, M-1 \rrbracket$),
and let $x_t = \sum_{i=0}^{M-1}x_t^i$ be the probability that it transmits a packet.
We consider $N$ active devices following the same policy.

We assume to be in the steady state \cite{Norris98}, in our Markov chain model depicted in Figure~\ref{fig:43:Markov_model}, and thus the probabilities no longer depend on the slot number $t$ (\ie, $\forall t, x_t=x$).
Therefore, the probability that this device has a collision at the first transmission is $p_c$, and has the following expression
\begin{equation}\label{eq:43:1}
	p_c = 1-\left(1-x_t\right)^{N-1} \iff x_t = 1-\left(1-p_c\right)^{\frac{1}{N-1}}.
\end{equation}

Moreover, from \eqref{eq:43:1} we define the probability $p_{cp}(n)$ that involves the collision of $n$ packets sent by each IoT device (for any $1\leq n \leq N-1$), during the first transmission slot, and is defined by the following equation
\begin{equation*}\label{eq:43:2}
	p_{cp}(n) = {N-1 \choose n} \; x^n \left(1-x\right)^{N-1-n}.
\end{equation*}

As explained above, if an IoT device is experiencing a collision at the first transmission, it proceeds for the retransmission of its packet after a random back-off interval.
We denote $p_{ca}$ the probability to have a collision with a packet involved in the previous collision.
Under assumption $\cH_{1}$, the probability that the same device's packet is experiencing again a collision at the second time slot is
\begin{equation}\label{eq:43:decomppc1}
	p_{c1} = p_{ca}+\left(1-p_{ca} \right)p_c.
\end{equation}

If the device had a collision, we consider $p_{bp}(n)$ the probability that it had a collision with \emph{exactly} $n$ packets (for any $1\leq n \leq N-1$), and that \emph{at least one} of the $n$ devices involved in the previous collision chose the same back-off interval.
Hence, under hypothesis $\cH_{2}$, we can relate $p_{ca}$ with $p_{bp}(n)$, and the different probabilities that a device experienced a collision during the first slot and has the same back-off interval for its retransmission is,
%
\begin{equation}\label{eq:43:sumpca}
	p_{ca} = \sum_{n=1}^{N-1} p_{bp}(n).
\end{equation}

Therefore, the expression of $p_{ca}$ is
\begin{equation}\label{eq:43:sumpc2}
	\frac{1}{p_c} \sum_{n=1}^{N-1}{N-1 \choose n} x^n \left(1-x\right)^{N-1-n}\left[1-\left( 1-\frac{1}{m}\right)^n \right].
\end{equation}

Once again under $\cH_{1}$, assuming that the number of devices involved in the first collision is small compared to $N-1$, the first terms of the sum in \eqref{eq:43:sumpc2} are predominant. Moreover, for these terms, $n$ is small compared to $N-1$, and so $N-1-n$ can be approximated to $N-1$. Thus it gives,
\begin{align}\label{eq:43:sumpca2}
	p_{ca} = & 1- \frac{1}{p_c}\sum_{n=1}^{N-1}{N-1 \choose n} x^n \left(1-x\right)^{N-1-n}\left( 1-\frac{1}{m}\right)^n,\nonumber\\
	\simeq & 1- \frac{\left(1-x\right)^{N-1}}{p_c}\sum_{n=1}^{N-1}{N-1 \choose k} x^n \left( 1-\frac{1}{m}\right)^n.
\end{align}

We use the binomial theorem to compute the sum in \eqref{eq:43:sumpca2}, and so the expression of $p_{ca}$ becomes
\begin{align}\label{eq:43:pca}
	p_{ca} \simeq \frac{1}{p_c} - \left(\frac{1}{p_c}-1\right)\left[ 1+\left(1-\left(1-p_c\right)^{\frac{1}{N-1}}\right)\left(1-\frac{1}{m}\right)\right]^{N-1}.
\end{align}

Finally, our approximation of $p_{c1}$ can be obtained by inserting \eqref{eq:43:pca} in \eqref{eq:43:decomppc1}.



% --- --- --- --- --- --- --- --- --- ---
\paragraph{Behavior analysis of $p_{c}$ and $p_{c1}$}\label{sub:43:numericalValidationPC1PC}

In order to assess the proposed approximation, we suppose a unique channel where all the devices follow the same contention Markov process.
We simulate an ALOHA protocol with a maximum number of retransmissions $M=10$, a maximum back-off interval $m=10$, and a transmission probability $p=10^{-3}$.
%
In Fig.~\ref{fig:43:Approximation_m10}, we show the collision probabilities for different number of devices $N$ (from $N=50$ up-to $N=400$), for both $p_{c}$ and $p_{c1}$.
%
From this simulations, we can verify that our approximation is very precise for lower values $p_{c1} \leq 30 \%$ (\ie, red and orange curves are quite close).
Moreover, a significant gap between $p_{c1}$ and $p_c$,
of up-to $10\%$, can be observed,
which suggests us to resort to MAB algorithms for the channel selection for both the first transmission and next retransmissions.

\begin{figure}[htp!]  % [htbp]
	\centering
	% \includegraphics[width=0.65\linewidth]{Approximation_m10.eps}
	\includegraphics[width=0.65\linewidth]{Approximation_m10-eps-converted-to.pdf}
	\caption[Proposed approximation for the probability of collision at the second transmission]{Proposed approximation for the probability of collision at the second transmission. It is more precise for smaller values of $N$.}
	\label{fig:43:Approximation_m10}
\end{figure}


\paragraph{Learning is useful for non-congested networks}

It is worth to highlight that, if we write \eqref{eq:43:decomppc1} as $p_{c1} = p_c + p_{ca} \left(1-p_c\right)$,
then it is obvious that $p_{c1}$ is always larger than $p_c$ (as $p_{ca} \left(1-p_c\right) > 0$).
But for large values of $p_c$, $p_{ca}\left(1-p_c\right) \simeq 0$ so the gap gets small,
and for small values of $p_c$ the gap is significant.
Moreover, we can verify (\eg, numerically or by differentiating)
that the gap decreases when $p_c$ increases (for fixed $N$ and $m$).
This backups mathematically the observation we made from Fig.~\ref{fig:43:Approximation_m10}:
the smaller the $p_c$, the larger is the gap between $p_c$ and $p_{c1}$.

We interpret this fact in two different situations.
\begin{itemize}
	\item
	On the one hand, in a congested network, when devices suffer from a large probability of collision on their first transmission (\ie, $p_c$ is not so small), then $p_{c1}\simeq p_c$ and so devices cannot really hope to reduce their collision probabilities even if the use a different channel for retransmission.
	\item
	On the other hand, if $p_c$ is small enough, \ie, in a network not yet too congested, then our derivation shows that $p_{c1} \gg p_c$, meaning that the possible gain of retransmitting in a different channel that the one used for the first transmission can be large, in terms of collision probability (\eg, up-to $10\%$ in this experimental setting).
	In other words, when learning can be useful (small $p_c$), learning to retransmit in a different channel can have a large impact on the global collision rate,
	thus justifying our approach.
\end{itemize}


\TODOL{Remove this part or compress}

%-----------------------------------------------------------------
\subsection{The \UCB{} algorithm as a building block for different heuristics}
\label{sub:43:MABalgo}
% ----------------------------------------------------------------------

Before presenting our proposed heuristics, we remind here the \UCB{} bandit algorithm \cite{Auer02}.
We include it again to facilitate the understanding of the heuristics, which use \UCB{} as a building block.


% % --- --- --- --- --- --- --- --- --- ---
\paragraph{The \UCB{} algorithm}\label{sub:43:algoUCB}

More formally, for one device, let $N_k(t)$ be the number of times the channel $k$ (for $k\in \llbracket 1, K \rrbracket$) was selected up-to time $t-1$, for $t\geq 0$
for any $t\in\mathbb{N}$,
\begin{equation}\label{eq:43:Nkt}
	N_k(t) = \sum_{\tau=0}^{t-1} \mathbbm{1}(C(\tau) = k),
\end{equation}
The empirical mean estimator $\widehat{\mu_k}(t)$ of channel $k$ is defined as the mean reward obtained up-to time $t-1$,
\begin{equation}\label{eq:43:mukt}
	\widehat{\mu_k}(t) = \frac{1}{N_k(t)} \sum_{\tau=0}^{t-1} r_k(\tau) \mathbbm{1}(C(\tau) = k).
\end{equation}
where $r_{k}(t)$ is the reward obtained after transmission in channel $k$ at time $t$ ($1$ for a successful transmission, and $0$ otherwise)
%
A \emph{confidence} term $B_k(t)$ is given by \cite{Auer02},
\begin{equation}\label{eq:43:Bkt}
	B_k(t) = \sqrt{\alpha \log(t) / N_k(t)},
\end{equation}
where $\alpha$ refers to an exploration coefficient,
% \footnote{~In fact, the larger this coefficient is, the longer the exploration, while the \UCB{} algorithm is proven to be order optimal for $\alpha>0.5$ \cite{Bubeck12}, and has reported a good performance for lower values of $\alpha>0$.},
that we chose equal to $1/2$, as suggested in \cite{Audibert07} and as done in previous works \cite{Bonnefoi18,Bonnefoi17}.
Then, an upper confidence bound in each channel $k$ is defined as
\begin{equation}\label{eq:43:ucb}
	U_k(t) = \widehat{\mu_k}(t) + B_k(t).
\end{equation}
Finally, the transmission channel at time step $t$
is the one maximizing this \UCB{} index $U_k(t)$,
as it is the one expected to be the best one at the current time step $t$,
\begin{equation}\label{eq:43:maxucb}
	C(t) = \arg\max_{1\leq k \leq K} U_k(t).
\end{equation}

The \UCB{} algorithm is implemented independently by each device, and we present it in Algorithm~\ref{algo:43:UCB}.
Note that a device using this first approach is only able to select a channel for the first and all the corresponding retransmissions of a packet.

% \begin{small} % XXX remove if needed
\begin{figure}[h!]
	\centering
	\begin{algorithm}[H]
		% \begin{small} % XXX remove if needed
		\For(){$t = 0, \dots, T$}{
			Compute for each channel $ U_k(t) = \widehat{\mu_k}(t) + B_k(t)$, following Eqs.~\eqref{eq:43:Nkt},~\eqref{eq:43:mukt}, and~\eqref{eq:43:Bkt}\;
			Transmit in channel $C(t) = \arg\max_{1\leq k \leq K} U_k(t)$\;
			Reward $r_{C(t)}(t) = 1$, if \emph{Ack} is received, else $0$\;
		}
		\caption{The \UCB{} algorithm for channel selection (base building block).}
		\label{algo:43:UCB}
		% \end{small} % XXX remove if needed
	\end{algorithm}
\end{figure}
% \end{small} % XXX remove if needed


% ----------------------------------------------------------------------
\subsection{Heuristics to (try to) learn how to retransmit efficiently}
\label{sub:43:heuristics}
% ----------------------------------------------------------------------

A device that implements the UCB algorithm is led to focus is transmissions and retransmissions in the channel which has been identified as the best.
As explained before, focusing in one channel increases the collision probability in retransmissions.
In this Section, we describe the proposed heuristics for the channel selection in a retransmission. It is carried out taking
into account that a device can incorporate a different channel selection strategy while being in a back-off state.
Hence, a natural question is to evaluate whether using this additional contextual information can improve the performance of a learning policy.

For that end, all of our heuristics comprise two stages:
the first stage is a \UCB{} algorithm employed for the first attempt to transmit,
and the second stage is another algorithm used for channel selections for the next retransmissions.
%
We present below four heuristics for this second stage (short names in ``quotes'' correspond to the legend on Figures~\ref{fig:43:mainExperiment1}, \ref{fig:43:mainExperiment2}).


% --- --- --- --- --- --- --- --- --- ---
\paragraph{Uniform random retransmission (``Random'')}\label{sub:43:UCBthenRandom}

In this first proposal, the device uses a random channel selection, following a uniform distribution (in $\llbracket 1, K \rrbracket$).
It is described below in Algorithm~\ref{algo:43:UCBthenRandom}.

\vspace*{-3pt}
\begin{figure}[h!]
	\centering
	\begin{algorithm}[H]
	% \begin{small} % XXX remove if needed
	\For(){$t = 0, \dots, T$}{
			\uIf{First packet transmission}{
				Use first-stage \UCB. %as in Algorithm~\ref{algo:43:UCB}.
			}
			\Else(\tcp*[f]{Random retransmission}){
				Transmit in channel $C(t) \sim \mathcal{U}(1,\ldots,K)$\;
			}
		}
		\caption{Heuristic: uniform random retransmission.}    % A naive
		\label{algo:43:UCBthenRandom}
	% \end{small} % XXX remove if needed
\end{algorithm}
\end{figure}


% --- --- --- --- --- --- --- --- --- ---
\paragraph{\UCB{} for retransmission (``Only \UCB{}'')}\label{sub:43:TwoUCB}

Instead of applying a random channel selection,
another heuristic is to use a second \UCB{} algorithm in the second stage.
In other words, we expect that this algorithm is able to learn the best channel to retransmit a packet.
It is described in Algorithm~\ref{algo:43:TwoUCB}, and it is still a practical approach, since the storage requirements and time complexity remains linear w.r.t. the number of channels $K$ (\ie, of order $\mathcal{O}(K)$).

Note that, we use the subscript $({}^r)$ to denote the variables
$\widehat{\mu^r_k}(t)$, $B^r_k(t)$ and $U^r_k(t)$,
related to the \UCB{} algorithm employed for the retransmission.

\vspace*{-3pt}
% \begin{small} % XXX remove if needed
\begin{figure}[h!]
	\centering
	\begin{algorithm}[H]
		% \begin{small} % XXX remove if needed
		\For(){$t = 0, \dots, T$}{
			\uIf{First packet transmission}{
				Use first-stage \UCB. %as in Algorithm~\ref{algo:43:UCB}.
			}
			\Else(\tcp*[f]{Packet retransmission}){
				Compute for each channel $U^r_k(t) = \widehat{\mu^r_k}(t) + B^r_k(t)$ following Eqs.~\eqref{eq:43:Nkt},~\eqref{eq:43:mukt}, and~\eqref{eq:43:Bkt}\;
				Transmit in channel $C^r(t) = \arg\max_{1\leq k \leq K} U^r_k(t)$\;
				Reward $r^r_{C^r(t)}(t) = 1$, if \emph{Ack} is received, else $0$\;
			}
		}
		\caption{Heuristic: \UCB{} for retransmission.}    % A naive
		\label{algo:43:TwoUCB}
		% \end{small} % XXX remove if needed
		\end{algorithm}
\end{figure}
% \end{small} % XXX remove if needed

% --- --- --- --- --- --- --- --- --- ---
\paragraph{$K$ different {\UCB}s for retransmission (``$K$ \UCB{}'')}\label{sub:43:UCBthenKp1}

Another heuristic is to not use the same algorithm no matter where the collision occurred, but to use $K$ different \UCB{} algorithms.
Meaning that after a failed first transmission in channel $j$, the device relies on the $k$-th algorithm to decide its retransmission.
The corresponding algorithm is depicted in Algorithm~\ref{algo:43:UCBthenKp1}.
Each of these algorithms are denoted using the subscript $({}^{j})$, for $j\in\llbracket 1, K \rrbracket$.

Although, this approach increases the complexity and store requirements (of order $\mathcal{O}(K^2)$).
For our LPWA networks of interest, such as LoRaWAN, the cost of its implementation is still affordable, since a small number of channels is used.
For instance, for $K=4$ channels,
the memory to store $K+1=5$ algorithms is of the order of the requirements to storing one.

Note that for other networks this heuristic could not be practical, for instance Sigfox consist in a large number of very narrow-band channels.
The storage requirements and time complexity is now quadratic in $K$, and as such we no longer consider this heuristic to be a practical proposal in some LPWA networks, as for instance Sigfox networks consist in a large number of very narrow-band channels. But for LoRaWAN networks with $K=4$, storing $K+1=5$ algorithms does not cost much more than storing $2$.

\vspace*{-3pt}
% \begin{small} % XXX remove if needed
	\begin{figure}[h!]
		\centering
		\begin{algorithm}[H]
			% \begin{small} % XXX remove if needed
			% \KwData{$\forall k,j\in[\![1;K]\!]$, $N_k^j(t)$, $\widehat{\mu_k}^j(t)$, $B_k^j(t)$ and $U_k^j(t)$}
			\For(\tcp*[f]{At every time step})
			{$t = 0, \dots, T$}{
				\uIf{First packet transmission}{
					Use first-stage \UCB. %as in Algorithm~\ref{algo:43:UCB}.
				}
				\Else(\tcp*[f]{Packet retransmission}){ % Retr. after trying channel $j$
					j $\leftarrow$ last channel selected by first-stage \UCB\;
					Compute for each channel $U_k^j(t) = \widehat{\mu_k}^j(t) + B_k^j(t)$ following Eqs.~\eqref{eq:43:Nkt},~\eqref{eq:43:mukt}, and~\eqref{eq:43:Bkt}\;
					Transmit in channel $C^j(t) = \arg\max_{1\leq k \leq K} U^j_k(t)$\;
					Reward $r^j_{C^j(t)}(t) = 1$ if \emph{Ack} is received, else $0$\;
				}
			}
			\caption{Heuristic: $K$ different {\UCB}s for retransmission.}
			\label{algo:43:UCBthenKp1}
			% \end{small} % XXX remove if needed
		\end{algorithm}
	\end{figure}
% \end{small} % XXX remove if needed

% --- --- --- --- --- --- --- --- --- ---
\paragraph{Delayed \UCB{} for retransmission (``Delayed \UCB{}'')}\label{sub:43:UCBwithDelay}

This last heuristic is a composite of
the random retransmission (Algorithm~\ref{algo:43:UCBthenRandom})
and the \UCB{} retransmission (Algorithm~\ref{algo:43:TwoUCB}) approaches.
Instead of starting the second stage \UCB{} directly from the first retransmission, we introduce a fixed delay $\Delta\in\mathbb{N}$, $\Delta \geq 1$,
and start to rely on the second stage \UCB{} after $\Delta$ transmissions.
The selection for the first steps is handled with the random retransmission.

The idea behind this delay is to allow the first stage \UCB{} to start learning the best channel, before starting the second stage \UCB{} (see details in Algorithm~\ref{algo:43:UCBwithDelay}).
The number of transmissions to wait before applying the second algorithm is denoted by $\Delta$, it has to be fixed before-hand\footnote{~Choosing the value of $\Delta$ could be done by extensive benchmarks but such approach goes against the reinforcement learning idea: an heuristic should work against any problem, without the need to be able to simulate the problem in advance in order to find a good value of some internal parameter. As such, we only consider a delay of $\Delta=100$.}.

Note that, we use the subscript $({}^d)$ to denote the variables
related to the delayed second-stage \UCB{} algorithm.

\vspace*{-3pt}
% \begin{small} % XXX remove if needed
	\begin{figure}[h!]
		\centering
		\begin{algorithm}[H]
			% \begin{small} % XXX remove if needed
			\For(\tcp*[f]{At every time step})
			{$t = 0, \dots, T$}{
				\uIf{First packet transmission}{
			 		Use first-stage \UCB. %as in Algorithm~\ref{algo:43:UCB}.
				}
				\uElseIf(\tcp*[f]{Random retransmission}){$t \leq \Delta$}{
					Transmit randomly in a channel $C(t) \sim \mathcal{U}(1,\ldots,K)$.
				}
				\Else(\tcp*[f]{Delayed \UCB{}}){
					Compute for each channel $ U^d_k(t) = \widehat{\mu^d_k}(t) + B^d_k(t)$ following Eqs.~\eqref{eq:43:Nkt},~\eqref{eq:43:mukt}, and~\eqref{eq:43:Bkt}\;
					Transmit in channel $C^d(t) = \arg\max_{1\leq k \leq K} U^d_k(t)$\;
					Reward $r^d_{C^d(t)}(t) = 1$ if \emph{Ack} is received, else $0$\;
				}
			}
			\caption{Heuristic: Delayed \UCB{} for retransmission.}
			\label{algo:43:UCBwithDelay}
			% \end{small} % XXX remove if needed
		\end{algorithm}
	\end{figure}
% \end{small} % XXX remove if needed


% ----------------------------------------------------------------------
\subsection{Simulations to compare our heuristics}
\label{sub:43:numExp}
% ----------------------------------------------------------------------

We simulate our network considering $N$ devices following the contention Markov process described in Section \ref{sub:43:model}, and a LoRa standard with $K=4$ channels.
Each device is set to transmit with a fixed probability $p=10^{-3}$, \ie, a packet about every $20$ minutes for time slots of $1\;\mathrm{s}$.
%
For the evaluation of the proposed heuristics, a total number of $T=10^{6}$ time slots is considered, and the results are averaged over $10^{3}$ independent random simulations.
%In this way, it allows us to evaluate the learning rate, as well as the convergence of each learning approach.

In a first scenario, we consider a total number of $N=1000$ IoT devices, with a non-uniform repartition of static devices given by $10\%,30\%,30\%,30\%$ for the four channels.
In other words, the channels are occupied $10\%$, $30\%$, $30\%$, and $30\%$ of time, and the contention Markov process considered is given by $M = 5$, and $m=5$.
In Fig.~\ref{fig:43:mainExperiment1}, we show the successful transmission rate versus the number of slots, for all the proposed heuristics.

A first result is that all the heuristics clearly outperform the non-learning approach that simply use random channel selection for both transmissions and retransmissions (\ie, the \textbf{no \UCB{}} curve).
The improvement of the heuristics over the non-learning approach is evident, and for every heuristic that use a kind of learning mechanism it can observed a successful transmission rate that increases rapidly (or equivalently an PLR decreasing).
Moreover, all of these approaches show a fast convergence making them suitable for the targeted application.
It is also worths mentioning that the employment of the same \UCB{} algorithm for retransmissions denoted here as ``Only \UCB{}'' achieves a better performance, while a ``Random" retransmission features a slight degradation. This result can be explained as follows: the loss of performance related to the separation of information for several algorithms is greater than the gain obtained by considering the first transmissions and retransmissions separately.

\begin{figure}[h!]  % [htbp]
	\centering
	\includegraphics[width=0.75\linewidth]{ResultsUCB.eps}
	\caption[First comparison between the exposed heuristics for the retransmission: Only \UCB, Random, \UCB, $K$ \UCB, and Delayed \UCB]{
		Comparison between the exposed heuristics for the retransmission: \textcolor{blue}{Only \UCB}, \textcolor{cyan}{Random}, \textcolor{purple}{\UCB}, \textcolor{green}{$K$ \UCB}, and \textcolor{red}{Delayed \UCB}.
		The usage of the same learning policy for transmissions and retransmission is named \textcolor{blue}{Only \UCB{}},
		whereas the usage of a random channel selection, for both transmission and retransmission, is labeled as \textbf{no \UCB{}}.
		First scenario: learning helps but learning to retransmit smartly is not needed, as we observe that the \textcolor{cyan}{random retransmission} heuristic achieves similar performance than the others.
	}
	\label{fig:43:mainExperiment1}
\end{figure}

We also consider in our analysis the case where $M=5$, and $m=10$ using ALOHA protocol, a statistic distribution of the devices about $40\%, 30\%, 20\%, 10\%$ for the four channels, and $N=2000$ IoT devices.
The corresponding results are depicted in Fig.~\ref{fig:43:mainExperiment2}.
In this case the successful transmission rate is degraded compared with achieved results in Fig.~\ref{fig:43:mainExperiment1}, this can be explained with the fact that we are considering in our network more devices that increase the collision probability.
It is important to highlight,that the ``Random" retransmission heuristic shows a poor performance in comparison to the other heuristics, and it can be attributed to the fact that the number of retransmission is increased, and consequently a
learning approach is able to take advantage of it.
Furthermore, the ``Delayed \UCB'' and  the``\UCB'' heuristics show quite similar results, after converging.
%
%Random, \UCB{}, $K$ \UCB{}, Delayed \UCB{} obtain similar results. The less efficient one is the $K$ \UCB{}, and this makes sense as having more learning algorithm needs more for each of them to learn. The most efficient one is the simple \UCB{} procedure, and this was quite surprising.

\begin{figure}[h!]  % [htbp]
	\centering
	\includegraphics[width=0.75\linewidth]{ResultsUCB2.eps}
	\caption[First comparison between the exposed heuristics for the retransmission: Only \UCB, Random, \UCB, $K$ \UCB, and Delayed \UCB]{
		Second scenario: learning helps a lot (a gain of $30\%$ in terms of collision probability), and learning to retransmit smartly is needed.
		We observe that the \textcolor{cyan}{random retransmission} achieves poor performance compared to the others.
	}
	\label{fig:43:mainExperiment2}
\end{figure}

The conclusions we can draw from depicted results are twofold.
First, MAB learning algorithms are very useful to reduce the collision rate in LPWA networks, a gain of up-to $30\%$ of successful transmission rate is observed after convergence.
A second conclusion that can be highlighted is that, using learning mechanisms for retransmissions can be an interesting way to reduce collisions in networks with massive deployments of IoT as this can be checked in Fig.~\ref{fig:43:mainExperiment2}, where the random retransmission heuristic is not very advantageous in front of the  \UCB-based approaches that use learning for channel selection during the retransmission procedure.


\paragraph{Note on the simulation code}
%
The source code (MATLAB or Octave) used for the simulations and the figures of this Section is open-sourced under the MIT License, and published at \href{https://Bitbucket.org/scee_ietr/ucb_smart_retrans}{\texttt{Bitbucket.org/scee\_ietr/ucb\_smart\_retrans}}.
It was written in collaboration with Rémi Bonnefoi and Julio César Mango-Vasquez, in July and August $2018$.
