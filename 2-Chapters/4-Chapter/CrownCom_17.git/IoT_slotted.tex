% - ``Multi-Armed Bandit Learning in IoT Networks and non-stationary settings'', see https://hal.inria.fr/hal-01575419

\graphicspath{{2-Chapters/4-Chapter/CrownCom_17.git/}}

As explained before, setting up the future Internet of Things (IoT) networks will require to support more and more communicating devices.
In this Section, we prove that intelligent devices in unlicensed bands can use Multi-Armed Bandit (MAB) learning algorithms to improve resource exploitation.
%
We evaluate the performance of two classical MAB learning algorithms, \UCB{} and Thomson Sampling, to handle the decentralized decision-making of Spectrum Access, applied to IoT networks; as well as learning performance with a growing number of intelligent end-devices.
%
We show that using learning algorithms does help to fit more devices in such networks, even when all end-devices are intelligent and are dynamically changing channel.
In the studied scenario, stochastic MAB learning provides a up to $16\%$ gain in term of successful transmission probabilities, and has near optimal performance even in non-stationary and non-\iid{} settings with a majority of intelligent devices.

% \TODOL{Based on the publication, ``Multi-Armed Bandit Learning in IoT Networks and non-stationary settings'', see https://hal.inria.fr/hal-01575419}

\TODOL{TODO:

- Clean up notations, be sure it is OK with previous Sections

- Be sure to not repeat any motivations / maths / explanations given before

- Be sure to give enough details about the conclusions etc
}


% % ------------------------------------------------------------------------
% \subsection{Motivation for this first IoT model}

The aim of this Section is to assess the potential gain of learning algorithms in IoT scenarios, even when the number of intelligent devices in the network increases, and the stochastic hypothesis is more and more questionable.
To do that, we suppose an IoT network made of two types of devices: static devices that use only one channel (fixed in time), and dynamic devices that can choose the channel for each of their transmissions. Static devices form an interfering traffic, which could have been generated by devices using other standards as well.
We first evaluate the probability of collision if dynamic devices randomly select channels (naive approach), and if a centralized controller optimally distribute them in channels (ideal approach).
Then, these reference scenarios allow to evaluate the performance of \UCB{} and TS algorithms in a decentralized network, in terms of successful communication rate, as it reflects the network efficiency.
We show that these algorithms have near-optimal performance, even when the proportion of end-devices increases and the interfering traffic from other devices becomes less and less stochastic.

The rest of this Section is organized as follows. The system model is introduced in Section~\ref{sub:41:systemModel}. Reference policies are described in \ref{sub:41:threeReferencePolicies}, and sequential learning policies are introduced in \ref{sub:41:sequentialPolicies}.
Numerical results are presented in \ref{sub:41:numericalResults}.


% ------------------------------------------------------------------------
\subsection{System model and notations}\label{sub:41:systemModel}

As illustrated in Figure \ref{fig:41:protocol}, we suppose a slotted protocol, in both time and frequency.
All devices share a synchronized time, and know in advance $K$, the finite number of available RF channels.
In each time slot, devices try to send packets to the unique Base Station, which listens continuously to all channels, following an ALOHA-based communication (no sensing).
Each time slot is divided in two parts: first for uplink communications in which data packets are sent by end-devices to the base station. If only one packet is sent in this part of the slot, the base station can decode it and sends an acknowledgement to the device in the second part.
If two or more devices send an uplink packet in the same slot, the uplink packets collide (\ie, there is a \emph{collision}), and the acknowledgement \emph{Ack} is not transmitted.
This way, no collision can occur on the downlink messages, easing the analysis of collisions.

\begin{figure}[!t]
    \centering
    \includegraphics[scale=0.45]{protocol.eps}
    \caption{The considered time-frequency slotted protocol. Each frame is composed by a fix-duration uplink slot in which the end-devices transmit their packets. If a packet is well received, the base station replies by transmitting an \emph{Ack}, after the ack delay.}
    \label{fig:41:protocol}
\end{figure}

There are two types of end-devices in the network:
\begin{itemize}
    \item
    \emph{Static} end-devices have poor RF abilities, and each of them uses only one channel to communicate with the base station. Their choice is assumed to be fixed in time (stationary) and independent (\iid). The traffic generated by these devices is considered as an interfering traffic for other devices.
    \item
    \emph{Dynamic} (or \emph{smart}) end-devices have richer RF abilities, they can use all the available channels, by quickly reconfiguring their RF transceiver on the fly. They can also store communication successes or failures they experienced in each channel, in order to change channel, possibly at every time slot.
\end{itemize}

There are $K \geq 1$ channels, $D \geq 0$ dynamic end-devices, and $S \geq 0$ static devices.
Furthermore, in channel $i \in \llbracket 1; K \rrbracket$ there are $0 \leq S_i \leq S$ static devices (so $S = \sum_{i=1}^{K} S_i$).
We focus on \emph{dense networks}, in which the number of devices $S + D$ is very large compared to $K$ (about $1000$ to $10000$, while $K$ is about $10$ to $50$).
As this problem is only interesting if devices are able to communicate reasonably efficiently with the base station, we assume devices only communicate occasionally, \ie, with a low \emph{duty cycle}, as it is always considered for IoT.
We prefer this choice rather than non-crowded networks, \ie, where $S + D \leq K$, as the former makes more sense for IoT networks\footnote{Here, $K$ will typically be about $20$ different radio channels, usually in the same RF band and with separate mean carrier frequencies, and the number of devices will be of the order of $1000$ to $10000$.}.

We suppose that all devices follow the same emission pattern, being fixed in time, and we choose to model it as a simple Bernoulli process:
all devices have the same probability to send a packet in any (discrete) temporal slot, and we denote $p \in (0, 1)$ this probability\footnote{In the experiments below, $p$ is about $10^{-3}$, because in a crowded network $p$ should be smaller than $K / (S + D)$ for all devices to communicate successfully (in average).}.
The parameter $p$ essentially controls the frequency of communication for each device, once the time scale is fixed (\ie, real time during two messages), and $1/p$ is proportional to the \emph{duty cycle}.
For instance for IoT objects, a ``smart'' refrigerator could have to send a daily message, and if a message can be sent every second, then $p = 1 / (12 \times 60 \times 60) \simeq 1.5 \times 10^{-5}$.

The goal is to design a simple sequential algorithm, to be applied identically by each dynamic device, in a fully distributed setting (each device runs its own algorithm, from its observations), in order to minimize collisions and maximize the fraction of successful transmissions of all the dynamic devices.

Before explaining how this goal presents similarity with a \emph{multi-armed bandit problem}, we present some natural baseline policies (\ie, algorithms).




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{System model - FIXME merge with previous section!}

We consider the system model presented in Figure~\ref{fig:42:system_model1}, where a set of object sends uplink packets to the network gateway, in the $433.5\;\mathrm{MHz}$ ISM band.
The communication between IoT devices and this gateway is done through a simple pure ALOHA-based protocol where devices transmit uplink packets of fixed duration whenever they want.
%
The devices can transmit their packets in $K\geq 1$ channels (\emph{e.g.}, $K=4$). In the case where the gateway receives an uplink in one channel, it transmits an acknowledgement to the end-device in the same channel, after a fixed delay (of $1$ s).

These communications operate in unlicensed ISM bands and, consequently, suffer from interference generated by uncoordinated neighboring networks. This interfering traffic is uncontrolled, and can be unevenly distributed over the $K$ different channels.

%To simulate networks designed for the Internet of Things (IoT), we consider a protocol with no sensing, no repetition of uplink messages, and where the gateway is in charge of sending back an acknowledgement, after some fixed-time delay (typically $1\;\mathrm{s}$), to any object who succeeded in sending successfully an uplink packet.
%
%By considering a small number of wireless channels (\emph{e.g.}, $K=8$) and one PHY layer configuration (\emph{i.e.}, modulation, waveform, etc), and in case of a non-uniform traffic in the different channels,
%the object can improve their usage of the network if they are able to \emph{learn} on the fly the best channels to use (\emph{i.e.}, the most vacant one).

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.65\linewidth]{system_model1.eps}
    \caption{In our system model, some dynamic devices transmit packets to a gateway and suffer from the interference generated by neighboring networks.}
    \label{fig:42:system_model1}
\end{figure}

We consider the network from the point of view of one end-user. Every times the end-user has to communicate with the gateway,
it has to choose one channel (at each transmission $t \geq 1, t \in \mathbb{N}$), denoted as $C(t) = k \in\{1,\dots,K\}$.
Then, the end-users starts waiting in this channel $C(t)$ for an acknowledgement sent by the gateway.
Before sending another message (\emph{i.e.}, at time $t+1$), the end-user knows if it received or not this \emph{ACK} message.
%
For this reason, selecting channel (or arm) $k$ at time $t$ yields a (random) feedback, called a \emph{reward}, $r_k(t) \in \{0,1\}$, being $0$ if no \emph{ACK} was received before the next message, or $1$ if \emph{ACK} was successfully received.
The goal of the end-user is to minimize its packet loss ratio, or equivalently, it is to maximize its cumulative reward,
$r_{1 \dots T} := \sum_{t = 1}^T r_{C(t)}(t),$
as it is usually done in MAB problems \cite{Thompson33,Robbins52,LaiRobbins85}.

This problem is a special case of the so-called ``stochastic'' MAB, where the sequence of rewards drawn from a given arm $k$ is assumed to be  \emph{i.i.d.}, under some distribution $\nu_k$, that has a mean $\mu_k$. Several types of reward distributions have been considered in the literature, for example distributions that belong to a one-dimensional exponential family (\emph{e.g.}, Gaussian, Exponential, Poisson or Bernoulli distributions).

Rewards are binary in our model, and so we consider only Bernoulli distributions, in which $r_k(t) \sim \mathrm{Bern}(\mu_k)$, that is, $r_k(t) \in \{0,1\}$ and $\mathbb{P}(r_k(t) = 1) = \mu_k \in [0,1]$.
Contrary to many previous work done in the CR field (\emph{e.g.}, Opportunistic Spectrum Access),
the reward $r_k(t)$ does \emph{not} come from a sensing phase before sending the $t$-th message, as it would do for any ``listen-before-talk'' model. Rewards come from receiving an acknowledgement from the gateway, between the $t$-th and $t+1$-th messages.

The problem parameters $\mu_1,\dots,\mu_K$ are of course unknown to the end-users, so to maximize its cumulated reward, it must learn the distributions of the channels, in order to be able to progressively focus on the best arm (\emph{i.e.}, the arm with largest mean).
%
This requires to tackle the so-called \emph{exploration-exploitation dilemma}: a player has to try all arms a sufficient number of times to get a robust estimate of their qualities, while not selecting the worst arms too many times.


% ------------------------------------------------------------------------
\subsection{Three reference policies}\label{sub:41:threeReferencePolicies}

This section presents three different policies that will be used to assess the efficiency of the learning algorithms presented in the next section.
The first one is naive but can be used in practice, while the two others are very efficient but require full knowledge on the system (\ie, an oracle) and are thus unpractical.


\paragraph{Naive policy: Random Channel Selection}

We derive here the probability of having a successful transmission, for a dynamic device, in the case where all the dynamic devices make a purely random channel selection (\ie, uniform on $i \in \llbracket 1; K \rrbracket = \{1, \dots, K\}$).
% every time they communicate
% This reflects a very naive policy that could be implemented by all the dynamic devices, and it provides a reference scenario to compare against.

In this case, for one dynamic device, a successful transmission happens if it is the only device to choose channel $i$, at that time slot.
The probability of successful transmission is computed as follows, because the $S_i$ static devices in each channel $i$ are assumed to be independent, and static and dynamic devices are assumed to \emph{not} transmit at each time $t$ with a fixed probability $1-p$ :
\begin{equation}
    \Pr(\text{success}|\text{sent}) = \sum_{i=1}^{K} \underbrace{\Pr(\text{success}|\text{sent in channel}\;i)}_{\text{No one else sent in channel}\; i} \; \underbrace{\Pr(\text{sent in channel}\,i)}_{= 1/K, \text{by uniform choice}}
\end{equation}

All dynamic devices follow the same policy in this case, so the probability of transmitting at that time in channel $i$ for any dynamic device is $p / K$, and there are $D-1$ other dynamic devices.
As they are independent, the probability that no other dynamic device sent in $i$
% $\Pr(\text{no other dynamic device sent in}\;i)$,
is $q = \Pr(\bigcap_{k=1}^{D-1} \text{device}\;k\;\text{did not sent in}\;i) = \prod_{k=1}^{D-1} \Pr(\text{device}\;k\;\text{did not sent in}\;i)$. And $\Pr(\text{device}\;k\;\text{sent in}\;i) = p \times 1 / K$, by uniform choice on channels and the Bernoulli emission hypothesis. So $q = \prod_{k=1}^{D-1} (1 - p/K) = (1-p/K)^{D-1}$. Thus we can conclude,
%
\begin{align}\label{eq:41:strategynaive}
    \Pr(\text{success}|\text{sent})
    & = \sum_{i=1}^{K} \underbrace{(1 - p / K)^{D-1}}_{\text{No other dynamic device}} \times \underbrace{(1-p)^{S_i}}_{\text{No static device}} \times\; \frac{1}{K} \nonumber \\
    & = \frac{1}{K} \left(1-\frac{p}{K}\right)^{D-1} \sum_{i=1}^{K} (1-p)^{S_i} .
\end{align}
This expression \eqref{eq:41:strategynaive} is constant (in time), and easy to compute numerically, but comparing the successful transmission rate of any policy against this naive policy is important, as any efficient learning algorithm should outperform it.


\paragraph{(Unachievable) Optimal oracle policy}

We investigate in this section the optimal policy that can be achieved if the dynamic devices have a perfect knowledge of everything, and a fully centralized decision making\footnote{This optimal policy needs an \emph{oracle} seeing the entire system, and affecting all the dynamic devices, once and for all, in order to avoid any signaling overhead.} is possible.
We want to find the stationary repartition of devices into channels that maximizes the probability of having a successful transmission.

If the oracle draws once uniformly at random a configuration of dynamic devices, with $D_i$ devices affected to channel $i$ is fixed (in time, \ie, stationary),
then this probability is computed as before:
\begin{align}\label{eq:41:prob_col}
    \Pr(\text{success}|\text{sent})
    & = \sum_{i=1}^{K} \Pr(\text{success}|\text{sent in channel}\;i) \; \Pr(\text{sent in channel}\;i) \nonumber \\
    & = \sum_{i=1}^{K} \underbrace{(1 - p)^{D_i - 1}}_{\;\;D_i - 1 \;\text{others}\;\;} \times \underbrace{(1 - p)^{S_i}}_{\;\;\text{No static device}\;\;} \times \underbrace{ D_i / D }_{\;\;\text{Sent in channel}\; i\;\;}.
\end{align}

Consequently, the optimal allocation vector $(D_1,\dots,D_{K})$ is the solution of the following real-valued constraint optimization problem :

\begin{equation}
\begin{subequations} \label{eq:41:prob}
    \begin{cases}
    \underset{D_1,\dots,D_{K}}{\arg\max}\; & \sum_{i=1}^{K} D_i (1 - p)^{S_i + D_i -1}, \label{eq:41:optPb}\\
    \text{such that}\;\; & \sum_{i=1}^{K} D_i = D, \label{eq:41:eqCstr}\\
    & D_i \geq 0 \qquad \forall i\in\llbracket 1;K\rrbracket . \label{eq:41:ineqCstr}
    \end{cases}
\end{subequations}
\end{equation}

\begin{proposition}\label{prop:41:Lagrangian}
    The \emph{Lagrange multipliers} method \cite{BoydVanderberghe04} can be used to solve the constraint real-valued maximization problem introduced in equation \eqref{eq:41:prob}.

    It gives a closed form expression for the optimal solution $D_i^*(\lambda)$, depending on the system parameters, and the unknown Lagrange multiplier $\lambda \in \mathbb{R}$.

    \begin{equation}\label{eq:41:Dilambda}
        D_i^*(\lambda) = \left(\frac{1}{\log(1-p)}\left[ \mathcal{W}\left(\frac{\lambda e}{(1-p)^{S_i-1}} \right)-1 \right]\right)^{\dag} .
    \end{equation}
\end{proposition}

\begin{proof}
\begin{itemize}
    \item
    In a realistic scenario, we can assume that $D_i\leq \frac{-2}{\ln\left(1-p\right)} \approx \frac{2}{p},\quad \forall i\in\llbracket 1;K \rrbracket$. For such values for $D_i$, the objective function $f: (D_1, \dots, D_{K}) \mapsto \sum_{i=1}^{K} D_i (1 - p)^{S_i + D_i -1}$ is concave as the sum of concave functions
    \footnote{It worth noting that $f$ is neither concave nor quasi-concave on $[0,\infty)^{K}$ \cite{Luenberger68,Yaari77}.}.
    \item
    The Lagrange multipliers method can be applied to the optimization problem \eqref{eq:41:optPb}, with a concave objective function $f$, linear equality constraints \eqref{eq:41:eqCstr} and linear inequality constraints \eqref{eq:41:ineqCstr}. The strong duality condition is satisfied in this case \cite{BoydVanderberghe04}, so finding the saddle points will be enough to find the maximizers.
    \item
    More details are given in Section~\ref{sec:5:proofLagrangian} in the Appendix of this Chapter.
    % \hfill{}$\square$
    \end{itemize}
\end{proof}

Where $(a)^{\dag} = \max(a,0)$, and $\mathcal{W}$ denotes the $\mathcal{W}$-Lambert function which is the reciprocal bijection of $x \mapsto xe^x$ on $\mathbb{R^+} = [0, +\infty)$ \cite{Corless96}.
Moreover, condition \eqref{eq:41:eqCstr} implies that the Lagrange multiplier $\lambda$ is the solution of the constraint
\begin{equation}\label{eq:41:constraintLambda}
 \sum_{i=1}^{K} D_i^*(\lambda) = D.
\end{equation}


Equation \eqref{eq:41:constraintLambda} can be solved numerically, with simple one-dimensional root finding algorithms.
Solving the optimization problem provides the optimal real number value for $D_i^*$, which has to be rounded to find the optimal number of devices for channel $i$ :
% Any rounding choice will give about the same repartition, up-to a difference of only one device by channel, and so we chose to round from below for the first channels:
$\widehat{D_i} = \lfloor D_i^* \rfloor$ for $1 \leq i < K$, and $\widehat{D_{K}} = D - \sum_{i=1}^{K - 1} \widehat{D_i}$.

\paragraph{A greedy approach of the oracle strategy}

We propose a \emph{sequential} approximation of the optimal policy:
the third solution is a sub-optimal naive policy, simple to set up, but also unpractical as it also needs an oracle.
End-devices are iteratively inserted in the channels with the lowest load (\ie, the index $i$ minimizing $S_i + D_i(\tau)$ at global time step $\tau$). Once the number of devices in each channel is computed, the probability of sending successfully a message is also given by equation \eqref{eq:41:prob_col}.
This is the policy that would be used by dynamic devices if they were inserted one after the other, and if they had a perfect knowledge of the channel loads.

\subsection{Sequential policies based on bandit algorithms}\label{sub:41:sequentialPolicies}

% We now present the stochastic Multi-Armed Bandit (MAB) model,
% and the two stochastic MAB algorithms used in our experiments \cite{Bubeck12}.
While the stochastic MAB model has been used to describe some aspects of Cognitive Radio systems, it is in principle not suitable for our IoT model, due to the non-stationarity of the channels occupancy caused by the learning policy used by dynamic objects.
%Quite surprisingly, we will show in our experiments of Section~\ref{sub:41:numericalResults} that two algorithms inspired by the stochastic bandit modeling, UCB1 and Thompson Sampling, perform well even for coordinating multiple smart devices.


%This section presents the formalism of the Multi-Armed Bandit (MAB) problem, as introduced by \cite{LaiRobbins85}, and two classical algorithms designed to tackle its simplest version (stochastic MAB), \UCB{} from \cite{Auer02} and Thompson Sampling from \cite{Thompson33,Kaufmann12Thompson}.
%For more details, \cite{Bubeck12} is an excellent review.

% \paragraph{Stochastic Multi-Armed Bandits}

% A Multi-Armed Bandit problem is defined as follows \cite{Thompson33,Robbins52,LaiRobbins85}.
% There is a fixed number $K \geq 1$ of levers, or ``arms'', and a player has to choose one lever at each discrete time $t \geq 1, t \in \mathbb{N}$, denoted as $A(t) = k \in\{1,\dots,K\}$.
% Selecting arm $k$ at time $t$ yields a (random) \emph{reward}, $r_k(t) \in \mathbb{R}$, and the goal of the player is to maximize the sum of his rewards, $r_{1 \dots T} = \sum_{t = 1}^T r_{A(t)}(t)$.

% A well-studied version of this problem is the so-called ``stochastic'' MAB, where the sequence of rewards drawn from a given arm $k$ is assumed to be independent and identically distributed (\iid) under some distribution $\nu_k$, that has a mean $\mu_k$. Several types of reward distributions have been considered, for example distributions that belong to a one-dimensional exponential family (\emph{e.g.}, Gaussian, Exponential, Poisson or Bernoulli distributions).
% We consider Bernoulli bandit models, in which $r_k(t) \sim \mathrm{Bern}(\mu_k)$, that is, $r_k(t) \in \{0,1\}$ and $\mathbb{P}(r_k(t) = 1) = \mu_k$.

% The problem parameters $\mu_1,\dots,\mu_K$ are unknown to the player, so to maximize his cumulated rewards, he must learn the distributions of the channels, to be able to progressively focus on the best arm (\ie, the arm with largest mean).
% %
% This requires to tackle the so-called \emph{exploration-exploitation dilemma}: a player has to try all arms a sufficient number of times to get a robust estimate of their qualities, while not selecting the worst arms too many times.

% In a Cognitive Radio application, arms model the \emph{channels}, and players are the \emph{dynamic end-devices}.
% For example in the classical OSA setting with sensing \cite{Jouini10}, a single dynamic device (a player) sequentially tries to access channels (the arms), and collects a reward of 1 if the channel is available %(as there is no primary user, we can assume the transmission to be successful),
% and 0 otherwise. So rewards represent the \emph{availability} of channels, and
% the parameter $\mu_k$ represents the mean availability of channel $k$.

In our model, every dynamic device implements its own learning algorithm, \emph{independently}.
For one device, the time $t$ is the number of time it accessed the network (following its Bernoulli transmission process, \ie, its duty cycle), \emph{not} the total number of time slots from the beginning, as rewards are only obtained after a transmission, and IoT objects only transmit sporadically, due to low transmission duty cycles.


\paragraph{A bandit model for IoT}
%
Our IoT application is challenging in that there are \emph{multiple} players (the dynamic devices) interacting with the \emph{same} arms (the channels), without any centralized communication (they do not even know the total number of dynamic devices).

Considered alone, each dynamic device implements a learning algorithm to play a bandit game, the device is consequently a smart device. In each time slot, if it has to communicate (which happens with probability $p$), then it chooses a channel and it receives a reward $1$ if the transmission is successful, $0$ otherwise.
Each device aims at maximizing the sum of the rewards collected during its communication instants, which shall indeed maximize the fraction of successful transmissions. Besides the modified time scale (rewards are no longer collected at every time step), this looks like a bandit problem.
However, it cannot be modeled as a stochastic MAB, as the rewards are clearly \emph{not} \iid: they not only depend on the (stationary, \iid) behavior of the static devices, but also on the behavior of other smart devices, that is not stationary (because of learning).

Despite this, we show in the next subsection that running a stochastic bandit algorithm for each device based on its own rewards is surprisingly successful.

\paragraph{Two algorithms}

% \paragraph{The \UCB{} Algorithm}\label{sub:42:UCB}

% A naive approach could be to use an empirical mean estimator of the rewards for each channel, and select the channel with highest estimated mean at each time;
% but this greedy approach is known to fail dramatically \cite{LaiRobbins85}. Indeed, with this policy, the selection of arms is highly dependent on the first draws:
% if the first transmission in one channel fails and the first one on other channels succeed, the end-user will \emph{never} use the first channel again, even it is the best one (\emph{i.e.}, the most available, in average).

% Rather than relying on the empirical mean reward, Upper Confidence Bounds algorithms
% instead use a \emph{confidence interval} on the unknown mean $\mu_k$ of each arm,
% which can be viewed as adding a ``bonus'' exploration to the empirical mean.
% % by adding a confidence term to the empirical mean.
% They follow the ``\emph{optimism-in-face-of-uncertainty}'' principle: at each step, they play according to the best model,
% as the statistically best possible arm (\emph{i.e.}, the highest upper confidence bound) is selected.
% % the arm with the highest upper confidence bound is expected to be the true best arm, and is played.

% More formally, for one end-user, let $N_k(t) = \sum_{\tau=1}^t \mathbbm{1}(C(\tau) = k)$ be the number of times channel $k$ was selected up-to time $t \geq 1$.
% The empirical mean estimator of channel $k$ is defined as the mean reward obtained by selecting it up to time $t$, $\widehat{\mu_k}(t) = \left(1 / N_k(t)\right) \sum_{\tau=1}^t r_k(\tau) \mathbbm{1}(C(\tau) = k) $.
% For \UCB, the \emph{confidence} term is $B_k(t) = \sqrt{\alpha \log(t) / N_k(t)}$,
% giving the upper confidence bound $U_k(t) = \widehat{\mu_k}(t) + B_k(t)$, which is used by the end-user to decide the channel for communicating at time step $t+1$: $C(t+1) = \arg\max_{1\leq k \leq K} U_k(t)$.
% \UCB{} is called an \emph{index policy}.

The \UCB{} algorithm uses a parameter $\alpha > 0$, originally $\alpha$ was set to $2$ \cite{Auer02}, but empirically $\alpha = 1/2$ is known to work better (uniformly across problems), and $\alpha > 1/2$ is advised by the theory \cite{Bubeck12}.
% This algorithm is simple to implement and use in practice, even on embedded micro-processors with limited computation and memory capabilities.
%
% \todo[inline]{Maybe in the implementation of UCB the $t$ $\log(t)$ should be the total amount of time the user tried to communicate? It is already the case, but we should say so.}
In our model, every dynamic end-user implements its own \UCB{} algorithm, \emph{independently}.
For one end-user, the time $t$ is the total number of sent messages from the beginning, as rewards are only obtained after a transmission.


% \paragraph{Thompson Sampling}

% Thompson Sampling \cite{Thompson33} was introduced early on, in $1933$ as the very first bandit algorithm, in the context of clinical trials (in which each arm models the efficacy of one treatment across patients). Given a prior distribution on the mean of each arm, the algorithm selects the next arm to draw based on samples from the \emph{conjugated} posterior distribution, which for Bernoulli rewards is a Beta distribution.

% A Beta prior $\mathrm{Beta}(a_k(0)=1,b_k(0)=1)$ (initially uniform) is assumed on $\mu_k \in [0, 1]$, and at time $t$ the posterior is $\mathrm{Beta}(a_k(t),b_k(t))$.
% After every channel selection, the posterior is updated to have $a_k(t)$ and $b_k(t)$ counting the number of successful and failed transmissions made on channel $k$.
% So if the \emph{ACK} message is received, $a_k(t+1) = a_k(t) + 1$, and $b_k(t+1) = b_k(t)$, otherwise $a_k(t+1) = a_k(t)$, and $b_k(t+1) = b_k(t) + 1$.
% Then, the decision is done by \emph{sampling} an \emph{index} for each arm, at each time step $t$, from the arm posteriors: $X_k(t) \sim \mathrm{Beta}(a_k(t), b_k(t))$, and the chosen channel is simply the channel $C(t+1)$ with highest index $X_k(t)$. For this reason, Thompson Sampling is called a \emph{randomized index policy}.

The TS algorithm, although being simple and easy to implement, is known to perform well for stochastic problems, for which it was proven to be asymptotically optimal \cite{AgrawalGoyal11,Kaufmann12Thompson}.
It is known to be empirically efficient, and for these reasons it has been used successfully in various applications, including on problems from Cognitive Radio \cite{Toldov16,Mitton16}, and also in previous work on decentralized IoT-like networks \cite{Darak16}.


\paragraph{Multi-Player MAB with \emph{collision avoidance}?}
%
Another idea could be to try to use a \emph{multi-player MAB} model, as proposed by \cite{Zhao10}, to describe our problem.
We study this other direction in the next Chapter~\ref{chapter:5}.

In that case, the static and dynamic devices effect is decoupled, and arms only model the availability of the channels in the absence of dynamic devices : they are \iid{} with mean $\mu_i = 1 - p S_i$.
Moreover, dynamic devices are assumed to be able to \emph{sense} a channel before sending \cite{Zhao10}, and so communicate only if no static device is detected on the channel.
The smart devices try to learn the arms with highest means, while coordinating to choose different arms, \ie, avoid collisions in their choice, in a decentralized manner.
However, in this model it is assumed that the multiple agents can know that they experienced a collision with another agent, which is non-realistic for our problem at stake, as our model of smart device cannot do sensing nor differentiate collisions between smart and non-smart devices.

%The stochastic MAB model fits well for OSA for Cognitive Radio,
%but for IoT networks, a notable difference with the MAB problem introduced above is that there is not only one device adaptively using the network, but there are $D \geq 1$ dynamic end-devices (and possibly only dynamic devices).
%So in our model of IoT network, the stochastic \iid{} hypothesis for the rewards makes less sense: the (stationary) background traffic interferes with the end-devices transmissions but also the end-devices interfere with each others. To the stochastic static traffic is added a ``game-theoretical'' interaction between dynamic devices (who, let us remind it, cannot communicate with each others).
%Intuitively, the more dynamic devices, the less stationary the network will be.
%The presence of a lot of dynamic devices implies that each should learn both the (stationary) distribution of static devices (\ie, the $\mu_k$), and learn to avoid collisions with other devices.


\paragraph{\emph{Adversarial} bandit algorithms?}
%
Instead of using MAB algorithms assuming a stochastic hypothesis on the system, we could try to use MAB algorithms designed to tackle a more general problem, that makes no hypothesis on the interfering traffic.
The \emph{adversarial MAB} algorithms is a broader family, and a well-known and efficient example is the $\mathrm{Exp}3$ algorithm \cite{Bubeck12}.
Empirically, the $\mathrm{Exp}3$ algorithm turned out to perform worse than both \UCB{} and TS in the same experiments.
%(see Section \ref{sub:41:numericalResults}).
Contrarily to the two stochastic algorithms, the use of $\mathrm{Exp}3$ is correctly justified, even in the non-stationary and non-\iid, as its performance guarantee are true in \emph{any} setting.
But it is not so surprising that it performs worse, as the theoretical performance guarantees of adversarial MAB algorithms are an order of magnitude
%\footnote{We preferred not to talk about \emph{regret} in this study, but in a few words: it is a measure of how \emph{bad} the algorithm performed in terms of its accumulated rewards, in comparison to the best possible policy, and should be as small as possible. For stochastic algorithms, being ``efficient'' means having a regret bounded as $R_T = \mathcal{O}(\log T)$, but for adversarial algorithms, it means having $R_T = \mathcal{O}(\sqrt{K T})$.}
worse than the one for stochastic ones.
% (in their respective case of application).
More is left on this aspect for our future work.



% ------------------------------------------------------------------------
\subsection{Experiments and numerical results}\label{sub:41:numericalResults}

\begin{figure}[!t]
    \centering
    \subfloat[10\% of smart devices]{\includegraphics[scale=0.5]{10intelligent.eps}
    \label{fig:41:10intelligent}}
    \hfill
    \subfloat[30\% of smart devices]{\includegraphics[scale=0.5]{30intelligent.eps}
    \label{fig:41:30intelligent}}
    \vskip\baselineskip
    \vspace*{-20pt}
    \subfloat[50\% of smart devices]{\includegraphics[scale=0.5]{50intelligent.eps}
    \label{fig:41:50intelligent}}
    \hfill
    \subfloat[100\% of smart devices]{\includegraphics[scale=0.5]{100intelligent.eps}
    \label{fig:41:100intelligent}}
    \caption{Performance of two MAB algorithms (\UCB{} and Thompson Sampling), compared to extreme references without learning or oracle knowledge, when the proportion of smart end-devices in the network increases, from $10\%$ to $100\%$ (limit scenario).}
    \label{fig:41:from10to100}
    \vspace*{-10pt}
\end{figure}

% Simulation parameters
We suppose a network with $S + D = 2000$ end-devices, and one IoT base station.
Each device sends packets following a Bernoulli process, of probability $p = 10^{-3}$ (\emph{e.g.}, this is realistic: one packet sent about every $20$ minutes, for time slots of $1\mathrm{s}$).
The RF band is divided in $K = 10$ channels.
Each static device only uses one channel, and their uneven repartition in the $10$ channels is: $(S_1,\cdots, S_{K}) = S\times(0.3, \, 0.2, \, 0.1, \, 0.1, \, 0.05, \, 0.05, \, 0.02, \, 0.08, \, 0.01,$ $0.09)$, to keep the same proportions when $S$ decreases. The dynamic devices have access to all the channels, and use learning algorithms.
%to find the least loaded.
%
We simulate the network during $10^6$ discrete time slots, during which each device transmits on average $1000$ packets (\ie, the learning time is about $1000$ steps, for each algorithm).
We tried similar experiments with other values for $K$ and this repartition vector, and results were similar for non-homogeneous repartitions. Clearly, the problem is less interesting for homogeneous repartition, as all channels appear the same for dynamic devices, and so even with $D$ small in comparison to $S$, the system behaves like in Fig.\ref{fig:41:100intelligent}, where the performance of the five approaches are very close.

Figure \ref{fig:41:from10to100} presents the evolution of the successful transmission rate, as a function of time. %($x$ axis is labeled by hundreds of transmissions). % I will change the x axis
The two MAB algorithms, \UCB{} and Thompson Sampling (TS), are compared against the naive random policy from below, and the two oracle policies (optimal and greedy) from above.
The results are displayed when $10$, $30$, $50$ and $100\%$ of the traffic is generated by dynamic devices.

We can see in Figure \ref{fig:41:from10to100} that the TS algorithm (in red) outperforms the \UCB{} algorithm (in blue), when the number of end-devices is below 50\%. When the number of end-devices is higher, both algorithms have almost the same performance, and perform well after very few transmissions (quick convergence).
Moreover, we can see in Figures \ref{fig:41:10intelligent}, \ref{fig:41:30intelligent}, and \ref{fig:41:50intelligent} that both have better success rate than the random policy and the probability of successful transmission is between the oracle optimal and oracle suboptimal policies.
For instance, for $10\%$ of dynamic devices, after about $1000$ transmissions, using \UCB{} over the naive uniform policy improved the successful transmission rate from $83\%$ to $88\%$, and using Thompson Sampling improved it to $89\%$.
Increasing the number of end-devices decreases the gap between the optimal and random policies: the more dynamic devices, the less useful are learning algorithms, and basically for networks with only dynamic devices, the random policy is as efficient as the optimal one, as seen in Figures \ref{fig:41:100intelligent} and \ref{fig:41:perf_learning}.

To better assess the evolution of the optimal policy compared to the random one, we have displayed on Figure \ref{fig:41:perf_learning} the evolution of the gain, in term of successful transmissions rate, provided by the optimal oracle and the two learning policies, after $10^6$ time slots, \ie, about $1000$ transmissions for each object.
We can see that when the proportion of end-devices is low (\emph{e.g.}, $1\%$ of devices are dynamic), the optimal policy provides an improvement of $16\%$ compared to random channel selection.
The TS algorithm always provides near-optimal performance, but the \UCB{} algorithm has a lowest rate of convergence and performs consequently worse after $1000$ transmissions, for instance it only provides a gain of $12\%$ for the same proportion of dynamic devices ($1\%$).

\begin{figure}[!t]
    \centering
    \includegraphics[scale=0.65]{perf_learning.eps}
    \caption{Learning with \UCB{} and Thomson Sampling, with many smart devices.}
    \label{fig:41:perf_learning}
\end{figure}

Figure \ref{fig:41:perf_learning} also shows that learning keeps near-optimal performance even when the proportion of devices becomes large.
Note that when this proportion increases, the assumptions of a stochastic MAB model are clearly violated, and there is no justification for the efficiency of TS and \UCB{} algorithms.
Hence, it is surprising to have near optimal performance with stochastic MAB algorithms applied to partly dynamic and fully dynamic scenarios.


\paragraph{Another simulation results}

We include another simulation in Figure~\ref{fig:41:figure4appendix}, with a uniform repartition of static device, to check that learning does not help in this case, but does not decrease performance either.
% And we should include another example with a different scenarios, maybe $(0.3, 0.3, 0.3, 0.015, 0.015, 0.014, 0.014, 0.014, 0.014, 0.014)$ (R\'emi already did the simulations).


\begin{figure}[!t]
    \centering
    \subfloat[10\% of intelligent devices]{\includegraphics[scale=0.5]{ch2_10.eps}
    \label{fig:41:ch2_10}}
    \hfill
    \subfloat[30\% of intelligent devices]{\includegraphics[scale=0.5]{ch2_30.eps}
    \label{fig:41:ch2_30}}
    \vskip\baselineskip
    \vspace*{-20pt}
    \subfloat[50\% of intelligent devices]{\includegraphics[scale=0.5]{ch2_50.eps}
    \label{fig:41:ch2_50}}
    \hfill
    \subfloat[100\% of intelligent devices]{\includegraphics[scale=0.5]{ch2_100.eps}
    \label{fig:41:ch2_100}}
    \caption{Performance of the UCB bandit algorithm for the special case of uniform repartition of the static devices, when the proportion of intelligent devices in the network increases, from $10\%$ to $100\%$ (limit scenario).}
    \label{fig:41:figure4appendix}
\end{figure}



\paragraph{Note on the simulation code}
%
The simulation code used for the experiments in Section~\ref{sub:41:numericalResults} is for MATLAB or GNU Octave,
and is open-sourced under the MIT License, at\\
\href{https://Bitbucket.org/scee_ietr/rl_slotted_iot_networks}{\texttt{Bitbucket.org/scee\_ietr/rl\_slotted\_iot\_networks}}.

% % ---------- Bibliography ----------
% %\begin{small}
% \bibliographystyle{ieeetr}
% \bibliography{biblio_RIoT}
% %\end{small}
