%!TEX root = ../PhD_thesis__Lilian_Besson

% First chapter begins here
\chapter{General Conclusion and Perspectives}
% \addcontentsline{toc}{chapter}{General Conclusion}
\label{chapter:conclusion}

% Write miniTOC just after the title
\graphicspath{{2-Chapters/6-Chapter/Images/}}
% ----------------------------------------------------------------------

% % \newpage
% \abstractStartChapter{}%
% %
% This last chapter concludes the document, first by summarizing the previous chapters, and by giving a general conclusion of the thesis.
% %
% We highlight what we consider to be the most important directions of future works, as well as some promising directions that were shortly studied during my PhD but not discussed in the manuscript.

% \minitocStartChapter{}

% ----------------------------------------------------------------------------
% ----------------------------------------------------------------------------
\section{General Conclusion}

\TODOL{Je dois faire quelques phrases résumant la thèse, mais plus court}

This manuscript was organized in two parts, Part~\ref{part:Introduction} and Part~\ref{part:MABIOT}.
%
The main problem we studied was \emph{``Can we adapt the decision making tools already successfully applied to Cognitive Radio for Opportunistic Spectrum Access to the specific needs of CR for the (future) Internet of Things networks?''}
%
We detailed the historical, scientific and technical contexts of the problems we considered for our research, and presented the main contributions produced during the last three years.

% \begin{itemize}
    % \item
The first half, Part~\ref{part:Introduction}, started by motivating our work and situating the context of this research.
% Chapter 2
In Chapter~\ref{chapter:2}, we presented the multi-armed bandit models, the hypotheses we usually make, and the most important algorithms that solve different kinds of MAB problems.
% Chapter 3
Then in Chapter~\ref{chapter:3} we discussed in details about our open-source Python library, SMPyBandits, developed to run numerical simulations of MAB problems.
%
Finally, we presented in Chapter~\ref{chapter:25} a first mathematical contribution, our algorithm \Aggr, as an empirically efficient answer to the online algorithm selection problem.

    % \item
The second half of this thesis, Part~\ref{part:MABIOT}, presented our main contributions.
% Chapter 4
In Chapter~\ref{chapter:4}, we first proposed some models for realistic IoT wireless networks, in discrete times and with no central coordination between end-devices.
In two different models, with or without retransmissions, we showed that end-devices can independently learn to increase their successful transmission rates in the network, by using low-cost MAB algorithms.
We also presented a proof-of-concept that implements such network on real radio hardware and validates our model and proposed algorithms.
% Chapter 5
Because the models proposed for realistic IoT networks were found to be intractable to analyze from a theoretical point of view, mainly due to the large number of algorithms interacting at random times in an unpredictable environment,
in Chapter~\ref{chapter:5}, we studied a weaker but still interesting model.
We presented different variants of the multi-players MAB model, and we proposed the state-of-the-art algorithm for the variant with collision information (\MCTopM-\klUCB).
% Chapter 6
Finally, in Chapter~\ref{chapter:6}, we relaxed the stationary hypothesis on the single-player MAB problem, and we analyzed a new actively adaptive algorithm, for which we also proved state-of-the-art results (\GLRklUCB).
%
For the last two chapters, our proposed algorithms attain state-of-the-art performances both on numerical simulations and on the regret upper bounds obtained in our theoretical analyses.
% \end{itemize}

A more precise summary of each chapter is given below.


% % ----------------------------------------------------------------------------
% \paragraph{Conclusion about \textbf{Chapter~\ref{chapter:1}}.}


% ----------------------------------------------------------------------------
\paragraph{Conclusion about \textbf{Chapter~\ref{chapter:2}}.}

We started by presenting the multi-armed bandit model, with a finite number of arms and real-valued rewards.
Our main focus is on one-dimensional exponential families of distributions, and on stochastic and stationary problems.
The notations used in the manuscript were introduced by showcasing an interactive demonstration.
% \footnote{See \href{https://perso.crans.org/besson/phd/MAB\_interactive\_demo/}{\texttt{perso.crans.org/besson/phd/MAB\_interactive\_demo/}}}.


% ----------------------------------------------------------------------------
\paragraph{Conclusion about \textbf{Chapter~\ref{chapter:3}}.}

Then we presented our Python library SMPyBandits \cite{SMPyBanditsJMLR,SMPyBandits}, by detailing
its purpose and qualities, its organization, and an overview of its usage.
%
Our library allows any researcher to easily implement numerical simulations of stochastic or piece-wise stochastic problems of single- or multi-players multi-armed bandits.
SMPyBandits is distributed on GitHub and Pypi freely, under an open-source license (MIT License), and it is extensively documented (at \href{https://SMPyBandits.GitHub.io}{\texttt{SMPyBandits.GitHub.io}}).
% Our library allows any researcher to easily run numerical simulations of different kinds of multi-armed bandit problems, requiring only a small knowledge of Python thanks to its documentation, its well-designed API, and many examples of simulation scripts and configuration files included.

We detailed how SMPyBandits is implementing arms, problems, algorithms, and how the library uses these components to implement a simulation loop, with various visualizations being performed after a simulation.
Even if SMPyBandits is still restricted to the finite-arm case, it supports a wide range of arm distributions (and rested/restless Markov models).
Different kinds of models are implemented, from stationary single-player to piece-wise stationary multi-players with different collision models.
One of the main qualities of our library is that it is quite exhaustive, as all the main families of algorithms covering these different models have been implemented, even very recent algorithms from the literature.
% We indeed followed actively the state-of-the-art research, since December $2016$ to June $2019$.
More than $65$ algorithms (or variants) are implemented for the single-player case, $5$ for the experts aggregation problem, about $15$ for the multi-players case, and about $20$ for the piece-wise stationary case.
The entire codebase is fully documented, and the library is using continuous integration to run automated tests on the code after every modification.
%
When comparing algorithms on a problem, the main performance measure is the regret, but the library also computes, stores and visualizes other measures, such as best-arm selection rate or mean cumulated reward, as well as real time and memory costs.
%
Our library is used to run numerical simulations in Chapters~\ref{chapter:2}, \ref{chapter:5} and \ref{chapter:6}, and in other contributions such as our article \cite{Besson2018DoublingTricks}.


% ----------------------------------------------------------------------------
\paragraph{Conclusion about \textbf{Chapter~\ref{chapter:25}}.}
%
This first part ends by presenting one of our first contributions \cite{Besson2018WCNC}.
We tackle the question of how to select a particular bandit algorithm when a practitioner is facing a particular (unknown) bandit problem.
Instead of always choosing her favorite algorithm, or running costly benchmarks before real-world deployment of the chosen algorithm, a third solution for a practitioner can be to select a few candidate algorithms, where at least one is expected to be very efficient for the given problem, and use online algorithm selection to automatically and dynamically decide the best candidate.
We proposed an extension of the \ExpFour{} algorithm for this problem, \Aggr, and illustrate its performance on some bandit problems.


% ----------------------------------------------------------------------------
\paragraph{Conclusion about \textbf{Chapter~\ref{chapter:4}}.}

This chapter started the second part of the manuscript, Part~\ref{part:MABIOT}, and presents three main contributions.

First in Section~\ref{sec:4:firstModel}, we proposed an evaluation of the performance of applying MAB learning algorithms in IoT networks,
with a focus on the convergence of algorithms, in terms of successful transmission rates.
We restrict to the case of a protocol slotted in both time and frequency, and we assume a certain clock pre-agreement and time synchronisation between devices and the base station.
We studied the behavior of the network when the proportion of intelligent dynamic devices changes.
Concretely, increasing this probability allows to insert more devices in the same network, while maintaining a good Quality of Service.
Another way of viewing this result is the opposite point-of-view: if the number of devices remains constant, increasing the successful transmission rate directly improves the network QoS.
%  as the base station receives more accurately the data sent by the end-devices since less transmissions failed.
We show that \UCB{} and Thompson sampling algorithms both have near-optimal performance, even when their underlying \iid{} assumption (see Chapter~\ref{chapter:2}) is violated by the presence of many ``intelligent'' end-devices which all follow a random activation process.
%
This was both surprising and encouraging, as it shows that applying bandit algorithms tailored for a stochastic model is still useful in broader settings.
The fully \emph{decentralized} application of classic stochastic MAB algorithms are almost as efficient as the best possible centralized policy in this setting, after a short learning period, even though the dynamic devices \emph{cannot} communicate with each other, and \emph{do not} know the system parameters.


We presented in Section~\ref{sec:4:gnuradio} a proof-of-concept, demonstrated in $2018$ at the ICT conference \cite{Besson2018ICT}, and further detailed in the companion paper \cite{Besson2019WCNC}.
We gave all the necessary details on both the PHY and the MAC layer, as well as details on the User Interface developed for the demo.
Results obtained in practice were discussed, to highlight the interest of using learning algorithms for cognitive radio online optimization problems, and especially multi-armed bandit learning algorithms.
%
By using such low-cost algorithms, we confirmed empirically that a dynamically re-configurable device can learn on its own to favor a certain channel, if the environment traffic is not uniform between the $K$ different channels, by using only the acknowledgement (\Ack) feedback sent from the base station.


Finally in Section~\ref{sec:4:retransmissions}, we presented an extension of our model of Low-Power Wide-Area (LPWA) networks based on an ALOHA protocol, again under the hypothesis of a protocol slotted in time and frequency, and assuming a perfect synchronisation.
% , in which dynamic IoT devices can again use machine learning algorithms, to improve their Packet Loss Ratio (PLR) when accessing the network.
If the priority is the Quality of Service (QoS), for instance with renewable energy capabilities, this second model is more appropriate.
We presented and evaluated heuristics that learn how to transmit and retransmit in a smarter way, by using the \UCB{} algorithm for channel selection for the first transmission (first-stage), and different proposals based on \UCB{} for the retransmissions upon collisions (second-stage).
The main novelty of this model is to also learn the most efficient way to retransmit packets upon radio collision, by also using a MAB algorithm for this second-stage optimization problem.
%
We showed that incorporating learning for the first transmissions can help to achieve optimal performance, with significant gains in terms of successful transmission rate in networks with a large number of devices (up-to $30\%$ in the example network).
Our simulations showed that the proposed heuristics outperform a naive random access scheme.
Surprisingly, we showed that a simple \UCB{} learning approach, that simply retransmits in the same channel, turns out to perform almost as well as more complicated heuristics, that can change the channel used for retransmissions.


Thus in Chapter~\ref{chapter:4}, we proposed two models of Internet of Things (IoT) networks, composed of many independent IoT end-devices,
that can all use low-cost RL (Reinforcement Learning) algorithms to learn to improve their spectrum access.
%
Decentralized RL for IoT lets the devices use the acknowledgement sent by their Base Station as a reward, instead of requiring some sensing capacity, like in the OSA case.
We consider many independent ``dynamic'' devices, communicating with a small probability at every instant.
The first model (without retransmission of packets),
is interesting for its simplicity, and because considering no retransmission can improve the battery life of the IoT devices.
% , at the cost of a lower Quality of Service (QoS).
%
However, in case of failed transmission of a message, a dynamic device can also retransmit it up-to a fixed number of retransmission (\eg, $10$ times).
%
The second model is more interesting if the main target is an improvement of the QoS, rather than a longer battery life.

\TODOL{Je dois clarifier les conclusions des deux modèles, est-ce que l'un est vraiment mieux pour améliorer la QoS mais coûte en batterie, et l'autre améliore la batterie mais coûte en QoS ? Pas très clair en ayant la tête dans le guidon là !...}


% ----------------------------------------------------------------------------
\paragraph{Conclusion about \textbf{Chapter~\ref{chapter:5}}.}

The second chapter of Part~\ref{part:MABIOT} of this manuscript studied Multi-Players Multi-Arm Bandits models.
%
We present three variants of this model,
with different level of feedback being available to the decentralized players, under which we proposed efficient algorithms.
For the two easiest models (\ie, with sensing), our theoretical contribution improves the state-of-the-art upper bounds on the regret.
In the absence of sensing, we also provide some motivation for the practical use of the \Selfish{} heuristic, a simple index policy based on hybrid indices that are directly taking  into account the collision information.
\Selfish{} corresponds to the heuristic used in the first model studied in Chapter~\ref{chapter:4}.
%
We also reviewed various variants of this model, and for some interesting variants we discussed the related literature, which has proven to be very active in the last two years.
For some of these variants, we explained why our approach does not work efficiently without modifications, but we detailed and illustrated how to adapt \MCTopM{} to other settings.
For example, it assumes to know the number of players $M$ before-hand, but we illustrated that a previously introduced technique to estimate $M$ can also be applied successfully to our proposal.
%  and give satisfactory empirical performances.
% Further works would be required to adapt the theoretical analysis to these various extensions.


% ----------------------------------------------------------------------------
\paragraph{Conclusion about \textbf{Chapter~\ref{chapter:6}}.}

In the last chapter, we were interested in relaxing the stationary hypothesis made in Chapter~\ref{chapter:2}.
%
We first describe the break-point (or change-point) detection problem, that we solve by the Bernoulli GLR change-point detector, and by analyzing its properties at finite-time.
This sequential break-point detection test can be applied for bounded rewards, and we bound its false alarm probability as well as its detection delay.
For the piece-wise stationary bandit model,
we proposed a new efficient algorithm, \GLRklUCB, which combines the \klUCB{} algorithm with the Bernoulli GLR change-point detector.
This actively adaptive method attains state-of-the-art regret upper-bounds when tuned with a prior knowledge of the number of changes $\Upsilon_T$, but without any other prior knowledge on the problem, unlike \CUSUMUCB{} and \MUCB{} that require to know a lower bound on the smallest magnitude of a change.
We also gave numerical evidence of the efficiency of our proposal, which performs usually much better than all the passively adaptive approaches as well as better or comparably to the previous actively adaptive ones.


% ----------------------------------------------------------------------------
% \newpage

% ----------------------------------------------------------------------------
% ----------------------------------------------------------------------------
\section{Perspectives}

As discussed at the end of each chapter,
our works suggest possible directions of future studies.


% ----------------------------------------------------------------------------
% \paragraph{Perspectives about \textbf{Chapter~\ref{chapter:1}}.}



% ----------------------------------------------------------------------------
\paragraph{Perspectives about \textbf{Part~\ref{part:Introduction}}.}


% ----------------------------------------------------------------------------
% \paragraph{Perspectives about }.}

As detailed in Chapter~\ref{chapter:2}, we focused in this thesis on MAB problems with a finite number of arms, and binary, bounded or real-valued rewards.
Thus two possible directions of future research could be to extend our works
to MAB models with either multidimensional rewards, like contextual bandits \cite{Li10,Luo18,ChenLeeLuoWei2019}, or infinite arms \cite{valko2016bandits,Combes17}, like Lipschitz bandits.
The hypotheses we made were motivated by the target applications, but these other models have also started to be used for cognitive radio applications.


% ----------------------------------------------------------------------------
% \paragraph{Perspectives about \textbf{Chapter~\ref{chapter:3}}.}

A few tasks are left on our library SMPyBandits, a first one could be to implement new variants of the single-player stochastic models, as well as variants for the multi-players or the non-stationary cases.
For more details, see the issue tickets at \href{https://github.com/SMPyBandits/SMPyBandits/issues/}{\texttt{GitHub.com/SMPyBandits /SMPyBandits/issues/}}.
%
A second interesting task could be to extend the library for problems with non-finite arms, such as linear or contextual bandits (\href{https://github.com/SMPyBandits/SMPyBandits/issues/117}{ticket 117}),
or to add support for the ``dynamic case'' of multi-players bandits to allow arrivals or departures of players (\href{https://github.com/SMPyBandits/SMPyBandits/issues/124}{ticket 124}).
%
A third task that I would have liked to complete is to interface the library with a web-based interactive demonstration, in order to allow anyone to launch simulations without any knowledge of programming.
It is already possible to reproduce some of the experiments presented in this thesis,
by following the instructions given in the documentation (see Section~\ref{sub:3:listResearchWorksUsingSMPyBandits}),
by using the Jupyter notebooks \cite{jupyter} made available
\footnote{~These notebooks are hosted on both the GitHub repository of SMPyBandits and on my website on \href{https://perso.crans.org/besson/PhD/notebooks/}{\texttt{perso.crans.org/besson/PhD/notebooks/}}. They can be used locally if you install the library, but can also be used on cloud platforms, such as Binder, on \href{https://mybinder.org/v2/gh/SMPyBandits/SMPyBandits/master}{\texttt{mybinder.org/v2/gh/SMPyBandits/SMPyBandits/master}} or Google Colab, on \href{https://colab.research.google.com/github/SMPyBandits/SMPyBandits/tree/master/notebooks/}{\texttt{colab.research.google.com/github/SMPyBandits/SMPyBandits/tree/master/notebooks/}}, from your browser.}.



% ----------------------------------------------------------------------------
\paragraph{Perspectives about \textbf{Chapter~\ref{chapter:4}}.}

% Future works related to this chapter include the following directions.


% \paragraph{Possible extensions of the first model.}

The first model presented in Section~\ref{sec:4:firstModel} could easily be generalized with two probabilities $p_S$ and $p_D$, if the $S$ static and $D$ dynamic devices have different transmission patterns, and less easily with one probability per device.
Also, other emission patterns could be considered, instead of a Bernoulli process for each user.
In this whole Chapter~\ref{chapter:4}, we prefer to consider that all devices have the same activation probability, to keep the notation as simple as possible.
%
Moreover, for the sake of simplicity we supposed that all devices use the same standard.
Future works could consider more realistic interference scenarios and IoT networks, with, \eg, non-slotted time, more than one base station etc.

Another extension could be to not have a Bernoulli process (or any random process), but a fixed rate of transmission, \eg, one transmission a day.
So additionally to deciding the channel for communication (\ie, \emph{where} to communicate), each device has to also decide \emph{when} to communicate.
% This is another direction of research, that we will investigate in the future.
However, this clearly leads to a much larger action space, as there are many time slots in one day (for example), and thus we believe that as soon as the action space becomes too large in this extension, the simple MAB-based learning approach could be no longer appropriate.
It is well-known in the MAB literature that the larger the action space, the slower is the convergence speed of any stationary MAB algorithms.
Thus, it could be exciting to study the possible application of \emph{contextual} MAB \cite{Li10,Luo18} or structured MAB \cite{Combes17} models and algorithms for this extension.

% We will investigate this behavior in order to understand it better theoretically.
% We will also experiment more with adversarial algorithms, to confirm that they work less efficiently than stochastic bandit algorithms in our non-stochastic setting.

% \paragraph{Extensions of the demonstration.}

% Possible future extensions of our demonstration include the following points.
% We could consider more dynamic devices (\eg, $100$) but it would either cost more in terms of equipment, or in terms of software engineering to simulation more devices with the same card.
% We could also implement a real-world IoT communication protocol (like the LoRaWAN standard), which we prefer not to do as it would cost a significant effort of development.
% Finally, we could also study the interference in case of other gateways located nearby, and this could be done without needing a lot of new hardware (using one extra USRP card to simulate another gateway).


% \paragraph{Possible extensions for the second model.}

% Finally, the utility and impact of the proposed approaches for LPWA networks motivates us to address several subjects as future works. Among them, the non-stationarity of the channel occupancy caused by the learning policy employed by the IoT devices.
% %
% For that end, modifications of MAB algorithms have been proposed, such as Sliding-Window-\UCB{} or Discounted-\UCB{} \cite{Garivier11UCBDiscount}
% or more recently M-\UCB{} \cite{CaoZhenKvetonXie18},
% or more recently GLR-\UCB{} \cite{Besson2019GLRT} which is presented in Chapter~\ref{chapter:6},
% that nevertheless have not been explored for the targeted problem.

In order to validate our results in a realistic experimental setting and not only with simulations, future works should include a hardware implementation of the analyzed models, in order to complete our demonstration presented in Section~\ref{sec:4:gnuradio} \cite{Besson2018ICT,Besson2019WCNC}.
%
A hardware demonstrator could be also benefit to study other settings by relaxing some hypotheses, for instance by studying a similar model in non-slotted time.


% ----------------------------------------------------------------------------
\paragraph{Perspectives about \textbf{Chapter~\ref{chapter:5}}.}

Our study on multi-players bandits suggests several further research directions.
First, one could want to investigate the notion of \emph{optimal algorithms} in the decentralized multi-players model with sensing information.
So far we provided the first matching upper and lower bound on the expected number of sub-optimal arms selections, which suggests some form of (asymptotic) optimality.
However, sub-optimal draws turn out not be the dominant terms in the regret, both in our upper bounds and in practice, thus a promising future work is to identify some notion of \emph{minimal number of collisions}, like a recent work \cite{wang2019distributed} studied the minimal amount of communication needed to achieve logarithmic regret, in a similar model that authorizes direct communications between players.
% Similarly to what was done very recently in \cite{wang2019distributed} for the communications between players who collaborate with each other, it would be interesting to characterize the number of collisions needed to achieve logarithmic regret.

We also presented many extensions of the multi-players bandit model,
and even if some have already been implemented, a major future work is to implement the most interesting ones in our library SMPyBandits
(see tickets \href{https://github.com/SMPyBandits/SMPyBandits/issues/120}{120}, \href{https://github.com/SMPyBandits/SMPyBandits/issues/124}{124}, \href{https://github.com/SMPyBandits/SMPyBandits/issues/185}{185}).


% ----------------------------------------------------------------------------
\paragraph{Perspectives about \textbf{Chapter~\ref{chapter:6}}.}

We believe that our new proof technique could be used to analyze \GLRklUCB{} under less stringent assumptions than the one made in this chapter (and in previous works), that would require only a few ``meaningful'' changes to be detected.
This promising research direction is left for future work,  but the hope is that the regret would be expressed in term of this number of meaningful changes instead of the number of break-points $\Upsilon_T$.
We shall also investigate whether actively adaptive approaches can attain a $\cO(\sqrt{\Upsilon_T T})$ regret upper-bound without the knowledge of $\Upsilon_T$.
We also believe that combining change-point \emph{localization} with an efficient change-point detection algorithm, such as \GLRklUCB, could lead to an interesting class of more efficient algorithms, as suggested by \cite{Maillard2018GLR}.

We would also like to study in the future possible extension of our approach to the slowly varying model, as studied in \cite{Besbes14stochastic,Louedec16,WeiSrivastava18Abruptly}, or the rotting bandit model \cite{Seznec2018}.

Another interesting direction of future work is to study non-stationary distributed multi-players bandits, to unify the models from Chapter~\ref{chapter:5} and~\ref{chapter:6}.
A natural extension of the non-stationary model is to consider non-communicating players cooperating in a decentralized way to play the same bandit game, as it was proposed recently in \cite{WeiSrivastava18Distributed}.
%
We could also build on the proof technique used in \cite{WeiSrivastava18Abruptly}, even if we are interested in removing the hypothesis that player $j$ knows its ID $j\in[M]$ before starting to play.
%
A promising direction is to directly try to join our contributions from Chapter~\ref{chapter:5} and~\ref{chapter:6}, and propose an efficient algorithm using three parts:
\klUCB{} indexes for arm selection,
\MCTopM{} for orthogonalization (\ie, dealing with collisions),
and the Bernoulli GLR break-point detector for non-stationarity (\ie, dealing with abrupt changes).
The three parts should be connected, the same way we built \GLRklUCB{} by connecting both components.
We propose the idea of incorporating the detected change-points by B-\GLR{} test in the orthogonalization procedure \MCTopM: after any change-point is detected, the player is not sitting anymore.
Analyzing this strategy is left as a future work.

% \TODOL{Explain that we think the following approach can work very efficiently:

%     - Combine \MCTopM{} with \GLRklUCB{} instead of \klUCB,
%     - and incorporate also the detected change-points by B-\GLR{} test in the orthogonalization procedure: after any change-point, the player is not sitting anymore.
%     - Modification: only a change-point on the arm on which player is sitting can make him move? It's already the case: the \GLR{} test can only detect change-point on an arm that was played, and the \MCTopM{} orthogonalization procedure forces to play the same arm as long as the player is sitting.
% }

% ----------------------------------------------------------------------------
\paragraph{Other directions of future works.}

Another very interesting direction of research is to propose a unique ``unified'' algorithm that can be used in all the different settings studied in this thesis, and automatically adapt to the setting at hand.
For instance, even if \GLRklUCB{} is very efficient for piece-wise stationary problems, its forced exploration makes it sub-optimal for stationary problems.
Another example is for multi-players bandits, where \MCTopM-\klUCB{} performs sub-optimally on piece-wise stationary problems.
% if we consider the proposed extension of \MCTopM-\klUCB{} that estimates the number $M$ of players, i
One approach to obtain a unified ``master'' algorithm could be to use expert aggregation on the different state-of-the-art algorithms presented in this thesis, and as each one is efficient in a different setting, the resulting aggregated algorithm could also be efficient in each of the different settings.
But as we illustrated in Section~\ref{sub:25:theory}, this approach does not seem theoretically promising.

\TODOL{On doit encore en écrire un peu ?}



% % ----------------------------------------------------------------------------
% % ----------------------------------------------------------------------------
% \section{Personal Summary}

% I would like to highlight that from a personal point of view, this thesis was awesome blabla.
