%!TEX root = ../PhD_thesis__Lilian_Besson

% First chapter begins here
\chapter{General Conclusion and Perspectives}
% \addcontentsline{toc}{chapter}{General Conclusion}
\label{chapter:conclusion}
\minitoc

% Write miniTOC just after the title
\graphicspath{{2-Chapters/6-Chapter/Images/}}
% ----------------------------------------------------------------------

% \newpage
\paragraph{Abstract}

This last chapter concludes the thesis, first by summarizing the previous chapters, and by giving a general conclusion of the thesis.
%
We highlight what we consider to be the most important directions of future works, as well as some promising directions that were shortly studied during my PhD but not discussed in the manuscript.


% ----------------------------------------------------------------------------
% ----------------------------------------------------------------------------
\section{General Conclusion}

\TODOL{Je dois faire quelques phrases résumant la thèse}

This manuscript was organized in two parts, Part~\ref{part:Introduction} and Part~\ref{part:MABIOT}.
It presented the problems we considered for my PhD research, and we discussed about the main contributions produced.

% \begin{itemize}
%     \item
The first half, Part~\ref{part:Introduction}, starts by motivating our work and situating the context of this research.
% Chapter 2
In Chapter~\ref{chapter:2}, we present the multi-armed bandit models, the hypotheses we usually make, and the most important algorithms used to solve MAB problems.
% Chapter 3
Then in Chapter~\ref{chapter:3} we discuss in details about our open-source Python library developed to run numerical simulations of MAB problems, SMPyBandits.

    % \item
The second half of this thesis, Part~\ref{part:MABIOT}, presents our main contributions.
% Chapter 4
In Chapter~\ref{chapter:4}, we first propose some models for realistic IoT wireless networks, in discrete times and with no central coordination between end-devices.
% In two different models, with or without retransmissions, we showed that end-devices can learn independently to increase their successful transmission rates in the network, by using low-cost MAB algorithm. We also present a proof-of-concept that implements such network on real radio hardware and validates our model and proposed algorithms.
% Chapter 5
Because the models studied for realistic IoT networks were found to be intractable to analyze from a theoretical point of view, mainly due to the large number of algorithms interacting at random times in a noisy environment,
in Chapter~\ref{chapter:5}, we study a weaker but still very interesting model.
We present different variants of the multi-player MAB model, and we propose the state-of-the-art algorithm for the variant with collision information (\MCTopM-\klUCB).
% Chapter 6
Finally, in Chapter~\ref{chapter:6}, we remove the stationary hypothesis on the single-player MAB problem, and we analyze a new actively adaptive algorithm (\GLRklUCB).
%
For the last two chapters, our proposed algorithms attain state-of-the-art performances both on numerical simulations and on the regret bounds obtained in our theoretical analyses.
% \end{itemize}

A more precise summary of each chapters is included below.


% % ----------------------------------------------------------------------------
% \paragraph{Conclusion about \textbf{Chapter \ref{chapter:1}}}


% ----------------------------------------------------------------------------
\paragraph{Conclusion about \textbf{Chapter \ref{chapter:2}}}

We started to present the multi-armed bandit model, with a finite number of arms and real-valued rewards.
Our main focus is on one-dimensional exponential families of distributions, and on stochastic and stationary problems.
The notations used in the manuscript were introduced by showcasing an interactive demonstration made available online\footnote{See \href{https://perso.crans.org/besson/phd/MAB\_interactive\_demo/}{\texttt{perso.crans.org/besson/phd/MAB\_interactive\_demo/}}}.

The first contribution of this manuscript \cite{Besson2018WCNC} concluded this chapter, in Section~\ref{sec:2:chooseYourPreferredBanditAlgorithm}. We tackle the question of how to select a particular bandit algorithm when a practitioner is facing a particular (unknown) bandit problem.
Instead of always choosing a fixed algorithm, or running costly benchmarks before real-world deployment of the chosen algorithm, we advise to select a few candidate algorithms, where at least one is expected to be very efficient for the given problem, and use online algorithm selection to automatically and dynamically decide the best candidate.
We proposed an extension of the Exp4 algorithm for this problem, \Aggr, and illustrate its performance on some bandit problems.


% ----------------------------------------------------------------------------
\paragraph{Conclusion about \textbf{Chapter \ref{chapter:3}}}

Then we presented our Python library SMPyBandits \cite{SMPyBandits,SMPyBanditsJMLR}, by detailing
its purpose and qualities, its organization, and an overview of its usage.
%
SMPyBandits allows any researcher to easily implement numerical simulations of stochastic or piece-wise stochastic problems of single- or multi-player multi-armed bandits.
SMPyBandits is distributed on GitHub and Pypi freely, under an open-source license, and it is extensively documented (at \href{https://SMPyBandits.GitHub.io}{\texttt{SMPyBandits.GitHub.io}}).
% Our library allows any researcher to easily run numerical simulations of different kinds of multi-armed bandit problems, requiring only a small knowledge of Python thanks to its documentation, its well-designed API, and many examples of simulation scripts and configuration files included.

We detailed how SMPyBandits is implementing arms, problems, algorithms, and use these components to implement a simulation loop, with various visualizations being performed after the simulation.
As far as now, SMPyBandits is restricted to the finite-arm cases, but it supports a wide range of arms distributions.
Different kinds of models are implemented, from stationary single-player to piece-wise stationary multi-player with different collision models.
One of the main quality of the library is that it is quite exhaustive, as all the main families of algorithms covering these different models have been implemented, even very recent algorithms from the literature, as we tried our best in following the active research since December $2016$ to June $2019$.
More than $65$ algorithms or variants of algorithms are implemented for the single-player case, $5$ for the expert aggregation problem, about $15$ for the multi-player case, and about $20$ for the piece-wise stationary problem.
All the codebase is fully documented, and the library is using continuous integration to run automated tests on the code after every modification.
%
When comparing algorithms on a problem, the main performance measure is the regret, but the library also computes, stores and visualizes other measures, such as best-arm selection rate or mean cumulated reward, as well as real time and memory costs.
%
Our library is used to run numerical simulations on Chapters~\ref{chapter:2}, \ref{chapter:5} and \ref{chapter:6}, and in other contributions such as our article \cite{Besson2018DoublingTricks}.


% ----------------------------------------------------------------------------
\paragraph{Conclusion about \textbf{Chapter \ref{chapter:4}}}

This chapter started the second part of the manuscript, Part~\ref{part:MABIOT}, and presents three main contributions.

First in Section~\ref{sec:4:firstModel}, we proposed an evaluation of the performance of MAB learning algorithms in IoT networks,
with a focus on the convergence of algorithms, in terms of successful transmission rates, when the proportion of intelligent dynamic devices changes.
Concretely, increasing this probability allows to insert more devices in the same network, while maintaining a good Quality of Service.
Similarly, if the number of devices remain constant, increasing the successful transmission rate directly extends the IoT devices battery life, as they suffer less from failed transmissions.
We show that \UCB{} and TS have near-optimal performance, even when their underlying \iid{} assumption (see Chapter~\ref{chapter:2}) is violated by the presence of many ``intelligent'' end-devices which follow a random activation process.
%
This is both surprising and encouraging, it shows that applying bandit algorithms tailored for a stochastic model is still useful in broader settings.
The fully \emph{decentralized} application of classic stochastic MAB algorithms are almost as efficient as the best possible centralized policy in this setting, after a short learning period, even though the dynamic devices \emph{cannot} communicate with each others, and \emph{do not} know the system parameters.


We presented in Section~\ref{sec:4:gnuradio} a proof-of-concept, demonstrated in $2018$ at the ICT conference \cite{Besson2018ICT}, and further detailed in the companion paper \cite{Besson2019WCNC}.
We gave all the necessary details on both the PHY and the MAC layer, as well as details on the User Interface developed for the demo.
Results obtained in practice were discussed, to highlight the interest of using learning algorithms for radio online optimization problem, and especially multi-armed bandit learning algorithms.
%
By using such low-cost algorithms, we demonstrated empirically that a dynamically re-configurable device can learn on its own to favor a certain channel, if the environment traffic is not uniform between the $K$ different channels, by using the acknowledgement (\Ack) feedback sent from the base station.


Finally in Section~\ref{sec:4:retransmissions}, we presented an extension of our model of LPWA networks based on an ALOHA protocol, slotted both in time and frequency.
% , in which dynamic IoT devices can again use machine learning algorithms, to improve their Packet Loss Ratio (PLR) when accessing the network.
If the priority is the Quality of Service (QoS), for instance with renewable energy capabilities, this second model is more appropriate.
The main novelty of this model is to address the packet retransmissions upon radio collision, by using a Multi-Armed Bandit framework.
We presented and evaluated several heuristics that try to learn how to transmit and retransmit in a smarter way, by using the \UCB{} algorithm for channel selection for first transmission, and different proposals based on \UCB{} for the retransmissions upon collisions.
%
We showed that incorporating learning for the transmission is needed to achieve optimal performance, with significant gain in terms of successful transmission rate in networks with a large number of devices (up-to $30\%$ in the example network).
Our empirical simulations show that each of our proposed heuristic outperforms a naive random access scheme.
Surprisingly, the main take-away message is that a simple \UCB{} learning approach, that retransmit in the same channel, turns out to perform as well as more complicated heuristics.


% ----------------------------------------------------------------------------
\paragraph{Conclusion about \textbf{Chapter \ref{chapter:5}}}

The second chapter of Part~\ref{part:MABIOT} of this manuscript focussed on Multi-Player Multi-Arm Bandits models.
%
We present three variants of this model,
with different level of feedback being available to the decentralized players, under which we proposed efficient algorithms.
For the two easiest models --with sensing--, our theoretical contribution improves both the state-of-the-art upper and lower bounds on the regret. In the absence of sensing, we also provide some motivation for the practical use of the interesting \Selfish{} heuristic, a simple index policy based on hybrid indices that are directly taking  into account the collision information.
%
We also reviewed various variants of this model, and for some interesting variants we discussed the related literature, which has proved to be very active in the last two years. For some models, we explained why our approach does not work efficiently without modifications, but we detailed and illustrated how to adapt \MCTopM{} to other settings.
For example, it assumes to know the number of players $M$ before-hand, but we illustrated that previously introduced technique to estimate $M$ can also be applied to our proposal and give satisfactory empirical performances.
% Further works would be required to adapt the theoretical analysis to these various extensions.


% ----------------------------------------------------------------------------
\paragraph{Conclusion about \textbf{Chapter \ref{chapter:6}}}

In the last chapter, we studied and presented the piece-wise stationary bandit model.
%
We proposed a new algorithm for this problem, \GLRklUCB, which combines the \klUCB{} algorithm with the Bernoulli GLR change-point detector.
This actively adaptive method attains state-of-the-art regret upper-bounds when tuned with a prior knowledge of the number of changes $\Upsilon_T$, but without any other prior knowledge on the problem, unlike \CUSUMUCB{} and \MUCB{} that require to know a lower bound on the smallest magnitude of a change.
We also gave numerical evidence of the efficiency of our proposal, which performs usually much better than all the passively adaptive approaches as well as better or comparably to the previous actively adaptive ones.



% ----------------------------------------------------------------------------
% ----------------------------------------------------------------------------
\section{Perspectives}

As discussed at the end of each chapters, our works suggest many interesting directions of future studies.


% ----------------------------------------------------------------------------
% \paragraph{Perspectives about \textbf{Chapter \ref{chapter:1}}}


% ----------------------------------------------------------------------------
% \paragraph{Perspectives about }}

As detailed in Chapter~\ref{chapter:2}, we focussed in this thesis on finite-arms and one-dimensional bandit problems,
and thus two possible directions of future works could be to extend our works
to MAB models with either multidimensional rewards, like contextual bandits, or infinite arms, like Lipschitz bandits.
The hypotheses we made were motivated by the target applications, but these other models have also started to be used for cognitive radio applications.


% ----------------------------------------------------------------------------
% \paragraph{Perspectives about \textbf{Chapter \ref{chapter:3}}}

An interesting future work left on our library SMPyBandits could be to implement new variants of the single-player stochastic models, as well as variants for the multi-player or the non-stationary cases.
For more details, see the issue tickets at \href{https://github.com/SMPyBandits/SMPyBandits/issues/}{\texttt{GitHub.com/SMPyBandits /SMPyBandits/issues/}}.
%
It would be interesting to extend the library for problems with non-finite arms, \eg, linear or contextual bandit (\href{https://github.com/SMPyBandits/SMPyBandits/issues/117}{ticket 117}),
or to add support for the ``dynamic case'' of multi-player bandits to allow arrivals or departures of players (\href{https://github.com/SMPyBandits/SMPyBandits/issues/124}{ticket 124}).



% ----------------------------------------------------------------------------
\paragraph{Perspectives about \textbf{Chapter \ref{chapter:4}}}

% Future works related to this chapter include the following directions.


% \paragraph{Possible extensions of the first model}

The first model presented in Section~\ref{sec:4:firstModel} could easily be generalized with two probabilities $p_S$ and $p_D$, if static and dynamic devices have different transmission pattern, and less easily with one probability per device. Also, other emission pattern could be considered, instead of a Bernoulli process for each user.
In this whole Chapter~\ref{chapter:4}, we prefer to consider that all devices have the same activation probability, to keep the notation as simple as possible.
%
Moreover, for sake of simplicity we supposed that all devices use the same standard.
Future works could consider more realistic interference scenarios and IoT networks, with, \eg, non-slotted time, more than one base station etc.

Another extension could be to not have a Bernoulli process (or any random process), but a fixed rate of transmission, \eg, one transmission a day.
So additionally to deciding the channel for communication (\ie, \emph{where} to communicate), each device has to also decide \emph{when} to communicate.
% This is another direction of research, that we will investigate in the future.
However, this clearly leads to a much larger action space, as there are many time slots in one day (for example), and thus we believe that as soon as the action space becomes too large in this extension, the simple MAB-based learning approach could be no longer appropriate.
It is well-known in the MAB literature that the larger the action space, the slower is the convergence speed of any stationary MAB algorithms.
It could be interesting to study the possible application of \emph{contextual} MAB \cite{Li10,Luo18} or structured MAB \cite{Combes17} models and algorithms for this extension.

% We will investigate this behavior in order to understand it better theoretically.
% We will also experiment more with adversarial algorithms, to confirm that they work less efficiently than stochastic bandit algorithms in our non-stochastic setting.

% \paragraph{Extensions of the demonstration}

% Possible future extensions of our demonstration include the following points.
% We could consider more dynamic devices (\eg, $100$) but it would either cost more in terms of equipment, or in terms of software engineering to simulation more devices with the same card.
% We could also implement a real-world IoT communication protocol (like the LoRaWAN standard), which we prefer not to do as it would cost a significant effort of development.
% Finally, we could also study the interference in case of other gateways located nearby, and this could be done without needing a lot of new hardware (using one extra USRP card to simulate another gateway).


% \paragraph{Possible extensions for the second model}

% Finally, the utility and impact of the proposed approaches for LPWA networks motivates us to address several subjects as future works. Among them, the non-stationarity of the channel occupancy caused by the learning policy employed by the IoT devices.
% %
% For that end, modifications of MAB algorithms have been proposed, such as Sliding-Window-\UCB{} or Discounted-\UCB{} \cite{Garivier11UCBDiscount}
% or more recently M-\UCB{} \cite{CaoZhenKvetonXie18},
% or more recently GLR-\UCB{} \cite{Besson2019GLRT} which is presented in Chapter~\ref{chapter:6},
% that nevertheless have not been explored for the targeted problem.

In order to validate our results in a realistic experimental setting and not only with simulations, future works include a hardware implementation of the analyzed models to complete our demonstration \cite{Besson2018ICT,Besson2019WCNC}.
%
A hardware demonstrator could be also benefit to study other settings by removing some hypotheses, for instance by studying a similar model in non-slotted time.


% ----------------------------------------------------------------------------
\paragraph{Perspectives about \textbf{Chapter \ref{chapter:5}}}

Our study on multi-player bandits suggests several interesting further research directions.
First, one could want to investigate the notion of \emph{optimal algorithms} in the decentralized multi-player model with sensing information. So far we provided the first matching upper and lower bound on the expected number of sub-optimal arms selections, which suggests some form of (asymptotic) optimality. However, sub-optimal draws turn out not be the dominant terms in the regret, both in our upper bounds and in practice, thus an interesting future work is to identify some notion of \emph{minimal number of collisions}.
Similarly to what was done very recently in \cite{wang2019distributed} for the communications between players who collaborate with each other, it would be interesting to characterize the number of collisions needed to achieve logarithmic regret.

We also presented many extensions of the multi-player bandit model,
and even if some have already been implemented, a major future work is to implement the most interesting ones in our library SMPyBandits
(see tickets \href{https://github.com/SMPyBandits/SMPyBandits/issues/120}{120}, \href{https://github.com/SMPyBandits/SMPyBandits/issues/124}{124}, \href{https://github.com/SMPyBandits/SMPyBandits/issues/185}{185}).


% ----------------------------------------------------------------------------
\paragraph{Perspectives about \textbf{Chapter \ref{chapter:6}}}

We believe that our new proof technique could be used to analyze \GLRklUCB{} under less stringent assumptions than the one made in this chapter (and in previous work), that would require only a few ``meaningful'' changes to be detected. This interesting research direction is left for future work,  but the hope is that the regret would be expressed in term of this number of meaningful changes instead of $\Upsilon_T$. We shall also investigate whether actively adaptive approaches can attain a $\cO(\sqrt{\Upsilon_T T})$ regret upper-bound without the knowledge of $\Upsilon_T$.
We also believe that combining change-point \emph{localization} with an efficient change-point detection algorithm, such as \GLRklUCB, could lead to an interesting class of algorithms, as suggested by \cite{Maillard2018GLR}.

We would also like to study in the future possible extension of our approach to the slowly varying model, as studied in \cite{Besbes14stochastic,Louedec16,WeiSrivastava18Abruptly}, or the rotted bandit model.

Another interesting direction of future work is to study non-stationary distributed multi-player bandits, to unify the models from Chapter~\ref{chapter:5} and \ref{chapter:6}.
A natural extension of the non-stationary model is to consider non-communicating players cooperating in a decentralized way to play the same bandit game, as it was proposed recently in \cite{WeiSrivastava18Distributed}.
%
We could also build on the proof technique used in \cite{WeiSrivastava18Abruptly}, even if we are interested in removing the hypothesis that player $j$ knows its ID $j\in[M]$ before starting to play.
%
A promising direction is to directly try to join our contributions from Chapter~\ref{chapter:5} and \ref{chapter:6}, and propose an efficient algorithm using three parts:
\klUCB{} indexes for arm selection,
\MCTopM{} for orthogonalization (\ie, dealing with collisions),
and \GLRklUCB{} for non-stationarity (\ie, dealing with abrupt changes).


\TODOL{Explain that we think the following approach can work very efficiently:

    - Combine \MCTopM{} with \GLRklUCB{} instead of \klUCB,
    - and incorporate also the detected change-points by B-\GLR{} test in the orthogonalization procedure: after any change-point, the player is not sitting anymore.
    - Modification: only a change-point on the arm on which player is sitting can make him move? It's already the case: the \GLR{} test can only detect change-point on an arm that was played, and the \MCTopM{} orthogonalization procedure forces to play the same arm as long as the player is sitting.
}

% ----------------------------------------------------------------------------
\paragraph{Other directions of future works}

\TODOL{On doit encore en écrire un peu ?}



% % ----------------------------------------------------------------------------
% % ----------------------------------------------------------------------------
% \section{Personal Summary}

% I would like to highlight that from a personal point of view, this thesis was awesome blabla.
