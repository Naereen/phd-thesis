%!TEX root = ../PhD_thesis__Lilian_Besson

% \chapter[Multi-Players Multi-Armed Bandits Models]{Multi-Players Multi-Armed Bandits Models and Algorithms}
\chapter{Multi-Players Multi-Armed Bandits}
\label{chapter:5}

\graphicspath{{2-Chapters/5-Chapter/Images/}}
\graphicspath{{2-Chapters/5-Chapter/ALT_2018__MPBandits.git/figures/}}

\paragraph{Abstract.}
%
In this Chapter~\ref{chapter:5}, we are interested in a more formal approach to the decentralized learning problem presented in Chapter~\ref{chapter:4}.
We restrict to the easier case of at most $M \leq K$ devices in a network with $K$ channels, because it was found very hard to analyze the aforementioned IoT network model.
%
We discuss three feedback levels that give variants of the multi-players MAB model previously studied in the literature.
Using a decomposition of the centralized system regret, we start by explaining the intuition about what an efficient decentralized algorithm should do, and then we propose \RandTopM{} and \MCTopM, two new orthogonalization schemes.
Combining them with the \klUCB{} index policy gives order-optimal algorithms, that achieve state-of-the-art performance,
as illustrated by numerical simulations.
We conclude by reviewing different extensions of the multi-players models.


% \TODOL{C'est bizarre ce problème d'espacement vertical ici... je ne sais pas comment le corriger !}


\minitoc
% ----------------------------------------------------------------------


\newpage

% ----------------------------------------------------------------------------
\input{2-Chapters/5-Chapter/ALT_2018__MPBandits.git/mpBandits.tex}


\newpage  % WARNING
% ----------------------------------------------------------------------------
% ----------------------------------------------------------------------------
\section{Literature review of many extensions of the models}
\label{sec:5:literatureReviewOtherModels}
% ----------------------------------------------------------------------------

Before concluding this chapter, we review many extensions to the three models presented above in Section~\ref{sec:5:model} that were introduced in recent literature.
%
We wanted to highlight that the community has been quite active on research on multi-players bandits in the last $10$ years, but especially active since last spring $2018$.
For instance, our article \cite{Besson2018ALT} was the first to propose the ``no sensing'' case, and it was studied by (at least) two independent group of researchers since its publication in April $2018$ \cite{BoursierPerchet18,LugosiMehrabian18}.
Another example is the model with different arm means among players, who was initially studied in \cite{Anandkumar10,Kalathil12} and then more recently in \cite{Bistritz18,KaufmannAbbas19}.
%
% FIXME écrit pas ça c'est inutile !
% By following actively the research on multi-players bandit models since the last three years, we believe to be able to give an exhaustive literature review, but it is of course possible that we missed an important research work. In this case, it is our mistake and not an intentional choice as we included all the different extensions of our models that we were aware of.

Many extensions of the simpler model of multi-players bandits have been considered.
The number of players $M$ can be fixed but initially unknown to the players, the sensing information can be absent (like the model \modeltrois{} presented above), there could be communications between players (happening at each time step or only occasionally), or $M$ could evolve over time to model the possibility of arrivals or departures of players (\ie, connexions and disconnections of devices in a wireless networks).
Also inspired by real-world wireless networks, the arm means may vary among players, for instance to model players located at different distance of the gateway, and we can also consider networks with some ``jammers'' whose purpose is not to communicate to the gateway in a collaborative way, like the $M$ devices, but to interfere with the communications of the $M$ devices.

Finally, we can also be interested by models where $M$ stays fixed, but the environment evolves, for instance with abrupt changes in the means of arms. This last model makes a good connexion between this chapter and the next one, and an exciting future work is to further study this model in order to tackle this kind of problems by merging our contributions for stationary multi-players bandits and for non-stationary single-player bandits.

% \TODOL{
%     Pour les extension présentées ici, je veux suivre la même organisation :

%     1. j'explique l'extension, et en quoi c'est difficile (= en quoi notre approche échoue si on fait rien de plus),

%     2. j'explique les travaux existants, et les résultats qu'ils obtiennent e.g. en terme de bornes de regret

%     3. si j'ai déjà le code pour, je peux montrer UNE simulation
% }


% ----------------------------------------------------------------------------
\subsection{Unknown (fixed) number of players}
\label{sub:5:unknownNumberOfPlayers}

In the model we presented above, we assumed the number of players $M$ to be fixed in time during the learning process.
Without removing this hypothesis (we study this case below in Section~\ref{sub:5:arrivalDepartures}), it is interesting to remark that we made no hypothesis on whether the players can know this value $M$ or not.
Our proposals, \RandTopM{} and \MCTopM, both assume to know $M$ beforehand, like it was done for their main inspiration, \RhoRand.


\paragraph{Performance of our proposals for a wrong value of $M$.}
\label{par:5:usingWrongValueofM}
%
We can start by asking whether the most efficient algorithm, \MCTopM-\klUCB, is still efficient if it uses a value $M'$ different than the real number of players $M$.
Even though we did not include numerical simulations for this case in Section~\ref{sec:5:experiments} above, we did some tests, which confirmed two disappointing results that were also proven analytically.
First, the three algorithms (\RhoRand, \RandTopM{} and \MCTopM) give linear regret if they are using a value $M'$ strictly smaller than $M$ (and if $\mu^*_{M-1} > \mu^*_M$), as players will converge to play about $T-\bigO{\log(T)}$ times the $M-1$ best arms, leading to at least one collision most of the time, and thus a linear regret.
Second, if the $M$ players falsely use a value $M' > M$, then there is also a certain (fixed) probability to achieve a linear regret. Indeed, imagine one player ($M=1$) running \MCTopM{} with the false knowledge of $M'=2$, and if it learned to accurately identify the set of the $2$ best arms, then the \MCTopM{} orthogonalization scheme can make it play the worst of the two arms, and as $M=1$ this player will never encounter any collision, thus playing this suboptimal arm for about $T-\bigO{\log(T)}$ times, also leading to linear regret.


\paragraph{Interpretation of the hypothesis of knowning $M$ for real-world networks.}
\label{par:5:knowingYourIDinMPBanditsGame}
%
One could criticize our approach if we study a wireless networks with no central coordination from the gateway,
in particular where the devices cannot be assigned to a channel or be assigned a unique ID by the gateway when they first log in the network.
Then in such networks, if one wants to apply an algorithm like \MCTopM-\klUCB,
the $M$ devices need to know their number $M$, and have to receive it from the gateway, as it is the only part of this example of network which knowns $M$.
It seems unrealistic to ask the gateway to send the fixed value $M$ to each device, and not a unique ID to each device, for instance $\mathrm{id}^j\in[K]$.

% \TODOL{Expliquer pourquoi ce n'est pas très réaliste ? Le paragraphe ci dessous n'est pas très simple à comprendre, je peux faire mieux!}

If the $M$ players each have a unique ID, then a simple explore-then-commit algorithm (see Algorithm~\ref{algo:2:naiveStrategies}), running on top of a round-Robin phase can achieve order-optimal regret.
If the players know the time horizon $T$, then they can fix a confidence level $\delta$. For the first $T_0$ time steps, the $M$ players will use a simple round-Robin game, using their (unique) ID to stay orthogonal: player $j$ starts at arm $j$, then $j+1$, then cycle in $[K]$. They encounter no collision in these time steps, and then user $j$ targets the $j$-th best arm among the set of $M$-best arm.
If $T_0$ is large enough, all players have built the same estimate of the ranking of the arms (and not only correctly identified the set of $M$-best arms), with high probability (at least $\delta$), and thus they will also encounter no collision and no regret from after time $T_0$.
The mean regret of such approach is easily bounded by $R_T \leq M T_0 + (1 - \delta) (T - T_0)$, so by using $\delta = 1-1/T$, $R_T = \bigO{T_0}$.
By calibrating $T_0$ based on $\delta$ (which needs prior knowledge of $T$, see the proof we gave in Section~\ref{proof:2:tuningExploreThenCommit}) and the minimal gap $\Delta$ between \Mbest{} and \Mworst, one can show, using similar arguments as used by \cite{Rosenski16} for the Musical Chair policy, that with large probability a constant regret is obtained. A logarithmic regret can be obtained from the same bound, proving the order-optimality of this simple approach, if we assume that user $j$ knows it is user number $j$.
%
Without giving more details, we let the interested refer to what is explained for the algorithms presented in \cite{DarakHanawal18,JoshiKumar2018,KumarDarak2019}.
%
These works assume a prior knowledge of $\Delta$, or other measures of the difficulty of the problem, and as such we do not find them comparable with the approach chosen here.


\paragraph{Two ideas to estimate $M$.}

Some works studied the same model as our model \modeldeux{} ``with sensing'', under the hypothesis that players do not know in the value of $M$.
As illustrated above, if we consider algorithms building on the same ideas as \RhoRand{} or \MCTopM, it seems mandatory to first build an estimate of the value of $M$ then run the initial algorithm that assumed a perfect knowledge of $M$.
Two possible directions exist to estimate $M$ on the fly.

The first idea comes from \cite{Anandkumar11}, and the intuition behind it is quite simple, even if the mathematical derivations are not.
All players will build an estimate $\hat{M}^j(t)$ of the number of player, that start by $\hat{M}^j(0)=1$. As soon as one collision is observed, a player knows that $M\geq2$, so $\hat{M}^j(t+1) = 2$.
Then, based on probabilistic computations on the expected number of collisions if there were $m$ players, all following the same strategy (\eg, \RhoRand{} in the case of ), and because the formula is simple to compute for different $m$, the authors proposed a statistical test of the hypothesis $M \leq \hat{M}^j(t)$ against $M > \hat{M}^j(t)$ which is expressed as a simple comparison of the current number of collisions (since last update of $\hat{M}^j(t)$) against a threshold.
If a player observed ``too many'' collisions (that are unlikely to be caused by only $\hat{M}^j(t) - 1$ other players), then she increases her current estimate $\hat{M}^j(t+1) = \hat{M}^j(t) + 1$.

The second idea comes from \cite{Rosenski16}, and suppose to know beforehand both the horizon $T$ and a certain measure of the difficulty of the problem (\ie, a lower bound on the minimal gap between two means).
If all the $M$ players start to play for a long enough time $T_0$ uniformly at random among all the $K$ arms, then the (expected) number of collisions observed $\E[\cC^j_{T_0}]$ by any player $j$ is a (relatively simple) function of $M$. By knowing $K$ and $T_0$ and if all players use the same mechanism, then they can invert the formula to obtain the most likely estimate of $M$ which explained the observations of $\cC^j_{T_0}$ collisions.
This second approach works empirically very fine, but it requires a fine tuning of the stopping time $T_0$.
Rosenski et al. studies the performance of their algorithm with high-probability bounds \cite{Rosenski16}, and the tuning of $T_0$ they propose depends on prior knowledge on the problem.
%
For this reason, we are not fond of this approach, as this hypothesis is quite unrealistic if one wants to apply this kind of algorithms for real-world wireless networks.
We note that all the following works assume some sort of prior knowledge on the difficulty of the problem:
\cite{kumar2017channel,KumarYadav2018,SawantKumar2018,JoshiKumar2018,DarakHanawal18,KumarDarak2019,Tibrewal2019}.


\paragraph{Extension of our proposals to learn the value of $M$.}

Similarly to what was proposed for the \rhoRandEst{} algorithm in \cite{Anandkumar11}, we could have worked on proposing and analyzing an extension of our proposals that could efficiently learn the value of $M$ the number of player.
We implemented this \rhoRandEst{} policy in our library SMPyBandits \cite{SMPyBandits}, as well as this mechanism for \RandTopM{} and \MCTopM.
Building from the theoretical analysis given for \rhoRandEst{} in \cite{Anandkumar11}, and the analysis of \MCTopM-\klUCB, we believe it is possible to show that the aforementioned extension also achieves sub-linear regret without requiring players to know $M$ in the beginning of the bandit game.
More precisely, we believe that \MCTopM-\klUCB{} can still give order-optimal logarithmic regret if $M$ players use it, and this extension of Theorem~\ref{thm:5:LogarithmicRegret_MCTopMklUCB} is not included due to space constraints.
Writing its proof and performing more simulations are left as possible future work.

\paragraph{Simulations.}

We consider a bandit problem with $K=9$ arms, of means $0.1,\dots,0.9$, and three different cases of $M=3,6,9$ players, for $100$ independent repetitions and an horizon of $T=10000$.
As before, we include the centralized multiple-play \klUCB{} as an unachievably efficient baseline, the \Selfish-\klUCB{} algorithm as an heuristic which does not require to know $M$.
Then we compare the \RhoRand, \RandTopM{} and \MCTopM{} algorithms, using \klUCB, which know $M$ beforehand, with their extensions implementing the same algorithm as \rhoRandEst, to estimate $M$ on the fly.

% \TODOL{Include a simulation to show the behavior of \rhoRand{} vs \rhoRandEst, and \RandTopM{} and \MCTopM{} with prior knowledge of $M$ vs the ``estimate'' versions.}
% Je dois refaire ces simulations, les chiffres ici sont avec $T=1000$ et $N=4$ répétitions (juste faites vite fait dans le train, je dois les lancer sur la machine du bureau).
% for M in 3 6 9; do DEBUG=False SAVEALL=True NOPLOTS=True M=$M K=9 N_JOBS=-1 N=100 T=10000 make moremultiplayers; read; done



\begin{table}[ht]
\begin{small}  % WARNING
    \centering
    \begin{tabular}{cc|ccc}
    \textbf{Algorithms} & \textbf{Hyp. on $M$} & $M=3$ players & $M=6$ players & $M=9$ players \\
        \hline
        Centralized multiple-play
        & Known $M$ & $\mathbf{92 \pm 20}$ & $\mathbf{70 \pm 16}$ & $\mathbf{0}$ \\
        \hline
        \Selfish
        & Don't need $M$ & $250 \pm 34$ & $735 \pm 85$ & $3010 \pm 472$ \\
        \hline
        \multirow{2}{*}{\RhoRand}
        & Known $M$ & $417 \pm 103$ & $2481 \pm 449$ & $6639 \pm 1035$ \\
        & \textcolor{red}{Estimate $M$} & \textcolor{red}{$1422 \pm 1051$} & \textcolor{red}{$9030 \pm 1922$} & \textcolor{red}{$7264 \pm 1009 $} \\
        \hline
        \multirow{2}{*}{\RandTopM}
        & Known $M$ & $268 \pm 45$ & $941 \pm 217$ & $437 \pm 367$ \\
        & \textcolor{red}{Estimate $M$} & \textcolor{red}{$688 \pm 614$} & \textcolor{red}{$4256 \pm 2701$} & \textcolor{red}{$1155 \pm 544$} \\
        \hline
        \multirow{2}{*}{\MCTopM}
        & Known $M$ & $244 \pm 39$ & $401 \pm 55$ & $44 \pm 13$ \\
        & \textcolor{red}{Estimate $M$} & \textcolor{red}{$563 \pm 546$} & \textcolor{red}{$1560 \pm 1293$} & \textcolor{red}{$618 \pm 29$} \\
        \hline
    \end{tabular}
    \caption{Mean regret $\pm$ $1$ std-dev, for different algorithms on the same problem with $M=3,6,9$, comparing algorithms which knows $M$ against algorithms which estimate $M$ on the fly. All use \klUCB.}
    \label{table:5:meanRegretSimulationsEstimatingM}
\end{small}  % WARNING
\end{table}

One can observe in Table~\ref{table:5:meanRegretSimulationsEstimatingM} the empirical performances of different algorithms in an example problem, in each case of low, medium and maximum number of players ($M=3,6,9$ for $K=9$ arms).
The three algorithms \RhoRand, \RandTopM{} and \MCTopM{} all suffer from similar increase on their regret when they have to estimate $M$ on the fly (written ``Estimate $M$'' in red, in Table~\ref{table:5:meanRegretSimulationsEstimatingM}). Our proposals are still much more efficient than \RhoRand{} when using the procedure to estimate $M$ described from \cite{Anandkumar11}.
For the two non-extreme cases ($M=3,6$), the performance drop when having to estimate $M$ is quite large,
% (on the three algorithms and two cases, about $\times 3.4$),
and consistent on the different algorithms.
The extreme case of $M=K$ players is of highest interest, as \MCTopM{} achieves a very small regret (proven to be $\cO(1)$ as it is just the expected time of the orthogonalization process), while the same algorithm with unknown $M$ suffers from a much larger regret.
In this extreme case, \RhoRand{} and its extension both perform very closely, as expected, and much worse than \MCTopM.
%
Additional simulations, for increasing time horizons $T$, confirmed the expected order-optimal regret of this extension of \MCTopM-\klUCB.


% ----------------------------------------------------------------------------
\subsection{Arrival and departures of players: the ``dynamic setting''}
\label{sub:5:arrivalDepartures}

As reminded above, the model assumes that the number of players $M$ remains fixed during all the bandit game.
However, in real-world wireless networks, when players model communicating devices connected to a single gateway, devices can arrive or leave the network at any time.
The existing previous work on multi-players bandit models are all motivated by possible applications to wireless networks, but most of them assume $M$ to be fixed.
The first work studying the relaxation of this hypothesis is \cite{Rosenski16}, where this case is called the ``dynamic setting''.
If the arrival or departures of players is not random, but determined in advance while still being unknown to any player, the natural notion of regret is the following, where the expectancy $\E[\bullet]$ is capturing the randomness in the sensing information, as well as in the players' decisions (\ie, collisions):
\begin{equation}\label{eq:5:defRegretDynamicSetting}
    R_T^{\text{dyn}} \eqdef \sum_{t=1}^T \sum_{k=1}^{M(t)} \mu^*_{k} - \E\left[ \sum_{t=1}^T \sum_{j=1}^{M(t)} r^j(t) \right].
\end{equation}

In \cite{Rosenski16}, the authors explain that if the model allows arrival or departures of players at \emph{any time step}, then the game is much harder, and sub-linear regret is most likely un-achievable, if we consider the natural extension of the definition of regret as in our model presented in Section~\ref{sec:5:model} above.
But this negative results depends on how arrival of players are modeled:
if an arriving player has no prior memory of the previous observations, \ie, if it model a new device, then if there is no restriction on the number or frequency of arrival or departures, we can most likely prove that sub-linear regret is not achievable in general.
A simple but extreme example shows that regret has to be linear for any bandit strategy (in the observation model with sensing).
Consider $K=2$ Bernoulli-distributed arms of means $\mu_1=\mu_2=1/2$, and one player is always active and play any bandit strategy (\eg, \MCTopM-\klUCB).
Every time step, another player is either arriving, without any knowledge of the problem (\eg, it is a fresh and new IoT device). Then no matter the strategy of these new players, even if they all play \MCTopM-\klUCB{} for instance, if they play a uniformly efficient strategy there is a non-zero probability that player $1$ suffers from a collision at each time step, thus resulting in linear centralized system regret.

The authors of \cite{Rosenski16} also assume that the players all know a lower-bound $L$ on the length of all the intervals during which arrival or departures of players are allowed.
A naive idea, if $L$ is large enough, is simply to restart the underlying algorithm every $L$ time steps, in order to directly benefit from the theoretical guarantees of the ``static setting'' (where $M$ stays constant).
Unfortunately this idea yields a large regret if the length of ``static'' intervals $L$ is too small.
Applied to the \MusicalChair{} algorithm, the authors in \cite{Rosenski16} call this basic extension Dynamic \MusicalChair
Under the simple hypothesis that the overall number of players entering and leaving the game is sub-linear in $T$ (\ie, a $\smallO{T}$), they analyze the regret of Dynamic \MusicalChair{} and prove it is also sub-linear (see Theorem~2 in Section~3.4).
%
Under the same hypothesis, and if the lower-bound $L$ is known before-hand and is large enough, then we could also apply the same idea as from \cite{Rosenski16} to our approach. We believe that we could easily prove a sub-linear regret bound, if all players run \MCTopM-\klUCB, and if currently active players restart their memory of the past observations every $L$ time steps.
As the regret guarantee is stronger (\ie, smaller regret upper-bound) for \MCTopM{} than \MusicalChair, we also believe that in this ``slowly varying'' dynamic setting, applying the idea of Dynamic \MusicalChair{} from \cite{Rosenski16} to \MCTopM{} would also give a small regret upper-bound.

After being introduced in \cite{Rosenski16}, some more recent works studied the case of arrival or departures of players, usually referred to as ``dynamic setting'' or ``dynamic case''.
For instance, \cite{BoursierPerchet18} studies in Section~4 the first algorithm proposed for the dynamic case under the no-sensing model.
They study the same notion of regret as the one proposed in \cite{Rosenski16} and given above in \eqref{eq:5:defRegretDynamicSetting}, see Equation $(10)$ in Section~4.2.1.
With the assumption of a non-decreasing number of players, \ie, if only arrivals of players are considered,
they prove a regret upper-bound of the \textsc{Dyn-MMAB} algorithm in Theorem~3 .
% Denote $M(t)$ the number of players at each time $t$, then they propose to study $R_T$ defined as
% \[ R_T = \sum_{t=1}^T \sum_{k=1}^{M(t)} \mu_^*{k} - \sum_{t=1}^T \sum_{j=1}^{M(t)} r^j(t). \]
If all arriving players are using their \textsc{Dyn-MMAB} algorithm, the ``dynamic'' regret is bounded by $\bigO{M^2 K \log(T) / \mu^*_{M}}$, if $M=M(T)$ is the total number of players involved in the problem.

% \TODOL{Present the model, quote the main papers, explain the difficulty and how XXX algorithm solves it!}


% \cite{KumarYadav2018}

% Distributed Learning and Stable Orthogonalization in Ad-Hoc Networks with Heterogeneous Channels
% Sumit J Darak, Manjesh K. Hanawal
% https://arxiv.org/pdf/1812.11651
% \cite{DarakHanawal18}


% ----------------------------------------------------------------------------
\subsection{Without sensing information}
\label{sub:5:withoutSensing}

We presented in Section~\ref{sec:5:model} the model \modeltrois, without sensing information. Players can only observe $Y_{k,t}$, the product of the \iid{} random sensing information from arm $k$ and of the no-collision indicator.
This model seems harder than the model with sensing information, and even though the proposal \Selfish{} works fine in simulations (in terms of average regret), we proved that it can yield linear regret.
In our paper \cite{Besson2018ALT} as well as in the previous sections, it was left as a future work to know if an algorithm can achieve sub-linear regret in this harder model without sensing information.

Inspired by \cite{Besson2018ALT},
Boursier and Perchet studied this question in the summer $2018$ following its publication \cite{BoursierPerchet18}.
Their answer is that the model without sensing information is essentially not harder than the model with sensing information.
%
Similarly, Lugosi and Mehrabian studied the same problem \cite{LugosiMehrabian18}.
In both works, the authors detail algorithms that give a logarithmic regret if $M \leq K$ players all independently implement the proposed algorithm.


\paragraph{The ``communication trick''.}
%
% \TODOL{Write about half a page on the article \cite{BoursierPerchet18}.}

The recent work \cite{BoursierPerchet18} proposed an idea they called the ``communication trick''.
It essentially allow any player to send one bit to all the others, at some pre-agreed time steps, with no modification on the model.
Thanks to this ``communication trick'', recent research efforts have been more focus on studying the basic model --without explicit communications between players--, while proposing algorithms that can rely on some communication between players.
This trick essentially use the fact that the players share a synchronized time, thus they can use a collision as a way to directly exchange one bit of information between two players.
This process is slow and not efficient, as all but two players must do nothing when player $i$ is sending a bit to player $j$, but it does build a communication protocol in the model without explicit communication between players.
This ``communication trick'' is used for instance in \cite{KaufmannAbbas19}.
We let the interested reader refer to these last two works \cite{BoursierPerchet18,KaufmannAbbas19} as they are both solid, and well explained.


\paragraph{Estimating collisions through uniform exploration.}
%
\cite{LugosiMehrabian18} gives a first algorithm with expected regret of $\bigO{M K \log(T) / \Delta^2}$, if $\Delta = \mu^*_{M} - \mu^*_{M+1} \neq 0$, achieving a better dependency regarding $M$ when compared to our result (our bound is $\bigO{M^3}$) but at the cost of (much) larger constants hidden in the $\cO$ notation, and a non-fully explicit algorithms which usually obtain bad (or worse) empirical performance.
Then they study an interesting extension that works also if $\Delta = 0$ or if $\Delta$ is so close than the bound in $1/\Delta^2$ becomes useless for ``small horizons''. This behavior is well known for classical bandits, where bounds of the form $\log(T)/\Delta^2$ become worse than a linear regret $T$ if $\Delta$ is very small (\ie, $\Delta\ll \sqrt{\log(T)/T}$).
For this extension, their proposed algorithm is proven to achieve $\bigO{K^2 M (\log(T))^2 / \mu + K M \min(\sqrt{T \log(T)}, \log(T)/\Delta')}$ regret, if $\mu$ is a lower-bound on $\mu^*_{M}$ and $\Delta' = \max(\Delta, \min\{|\mu^*_M-\mu^*_i| : \mu^*_M > \mu^*_i \})$ (that both have to be known beforehand by the algorithm).
%
The proposed algorithms in \cite{LugosiMehrabian18} are all based on the Musical Chair algorithm from \cite{Rosenski16}, and the curious reader should read for instance their Algorithm~2 (page 15) for more details.

The results presented in \cite{LugosiMehrabian18} are based on some hypotheses, for instance the tuning they propose for the length $g$ of the uniform exploration phase in the beginning of their ``Musical Chair''-like algorithm is based on a prior knowledge of the horizon $T$.
In Section~4 they explain how to relax the assumptions of their results.
For the same example, the ``doubling trick'' technique can be used to obtain a regret upper bound for an algorithm unaware of the value of $T$, within a constant multiplicative factor of the upper bound given for the algorithm aware of $T$ (Section~4.1). Because the regret bound is of the form $\bigO{\sqrt{T} \log(T)}$, a simple ``doubling trick'' of increasing horizons of lengths $T_i = 2^i$ works well, as proposed in \cite{CesaLugosi06} and as studied in depth in our article \cite{Besson2018DoublingTricks}, and quickly presented in Appendix~\ref{app:2:DoublingTricks}.
%
They also study in Section~4.3 an extension of the model for the case with more players than arms, but we do not give more details on this aspect. Their definition of a centralized regret for this case is an interesting and natural generalization of the definition, and they also proposed an algorithm achieving $\bigO{M K \log(T) \exp(4M/K) / \Delta^2}$ regret in this case.
Finally, they also proposed in Section~4.4 an extension of their algorithm to estimate the number of players $M$, which is analyzed as for the Musical Chair algorithm in \cite{Rosenski16}, and also achieve logarithmic regret.
Note that it is significantly harder to estimate $M$ without collision information, and Algorithm~3 in page 24 is quite complex.
We did not implement it, and it would be interesting to run some numerical simulations in order to validate it empirically.


\paragraph{About \Selfish-\UCB{} inefficiency.}
%
It is also proven in Appendix~A of \cite{BoursierPerchet18} that \Selfish-\UCB{} has a linear regret, in the theoretical case, with a very neat argument from number theory (using Lindemann-Weierstrass theorem). As we conjectured, there is a gap between the theoretical result and its practical consequence, as the Theorem~4 they gave is only valid for real-valued number, and not for hardware-represented floating point number.
Their proof is supporting what we illustrated in Figure~4 in \cite{Besson2018ALT}.
Their argument is essentially to prove that for \Selfish-powered players using the simple \UCB-indexes, with a probability $p$ at time $t$ (both independent from $T$), two players might have the same number of pulls and the same observed rewards for each arm. In that case, the two players would pull the same arms and thus collide for a long time, until they reach a ``tie breaking point'' where they could choose different arms thanks to a random tie breaking rule (\eg, if two values of their \UCB{} indexes are the same, the $\argmax$ is a uniform random choice among the two arms).
They prove that it is unlikely to encounter any of these ``tie breaking points'', in theory if the \UCB{} indexes are real-valued number (that can be rational or irrational).


\paragraph{Additionnal numerical simulations.}

To illustrate the difficulty of the ``no sensing'' case, we illustrate here the performances of different algorithms designed specifically for this setting.
As above, we consider a bandit problem with $K=9$ arms, of means $0.1,\dots,0.9$, and three different cases of $M=3,6,9$ players, for $N=100$ independent repetitions, and horizon $T=10000$.
We include the \Selfish-\klUCB{} algorithm as an heuristic, as well as the Improved \MusicalChair{} algorithm from \cite{LugosiMehrabian18} and \textsc{Sic-MMAB} from \cite{BoursierPerchet18}.
It is also interesting to add the centralized multiple-play \klUCB{} is included as an unrealistic efficient baseline, as it does not use the sensing information but only the joint information (the reward) $r^j(t)$, because it directly affects the player in an orthogonal configuration and thus never encounters any collision (thus $r^j(t)=Y_{t,A^j(t)}$ for each $j,t$).
%
For the two other algorithms, we use the advised tuning of their parameters:
for \textsc{Sic-MMAB}, we used $T_0 = \lceil K \log(T) \rceil$,
% as advised by the authors.
for Improved-MC, we used $c=1$ which gives $g=235$, whereas in the paper the authors use $c=128$ for their analysis.
We found empirically no difference when using different values of the constant $c$ or $g$, and unfortunately the regret of Improved-MC was always found to be linear\footnote{For instance Improved-MC obtained a mean regret $12150$ for $T=10000$, for $M=3,K=9$, but seeing one value for one horizon does not mean anything, and we also experimented with larger values of $T$ and found a similar linear behavior.}.

% for T in 10000; do for K in 9; do for N in 4 100; do for M in 3 6 9; do DEBUG=False N_JOBS=4 N=$N K=$K M=$M T=$T make --noFreeSMS moremulti && cp logs/main_multiplayers_more_py3_log.txt logs/main_multiplayers_more_py3_log__K${K}_N${N}_M${M}_T${T}_$(date +"%d-%m-%Y_%H-%M-%S").txt && FreeSMS.py "Done for simulations for $K arms, $M players, T=$T N=$N and lots algorithms for experiments for Chapter 5 of my PhD. Love from Rennes."; done; done; done; done

We give in Table~\ref{table:5:meanRegretSimulationsNoSensing} the mean regret obtained for these different algorithms, and we observe a drastic difference between the unrealistic centralized algorithm, which achieves a very small regret, the heuristic \Selfish-\klUCB{} which achieve small mean regret (but is small to fail in theory and in some instances), and the two other algorithms.
We were unable to find any bug in our implementation of Improved \MusicalChair{} from \cite{LugosiMehrabian18} and all the different tuning of $c$ (or $g$) explored gave the same disappointing result (linear regret).
The \textsc{Sic-MMAB} algorithm performs better than the \Selfish{} heuristic for the extreme case of $M=K$, but its large regret in this case shows that the orthogonalization protocol developed by \cite{BoursierPerchet18} is not fast to converge (for illustration, for the same problem for $M=K=9$ for the ``sensing'' case, \MCTopM-\klUCB{} achieves a mean regret of about $40$, two orders of magnitude smaller!).
%
Further empirical evaluation would be needed to fully understand the situation, and a first future work would be to either fix our implementation of the algorithm proposed by \cite{LugosiMehrabian18} or propose an efficient modification, and illustrate in some problems that it can indeed achieve sub-linear regret (maybe it does but only for large horizons, even if we did try larger values of $T$ like up-to $T=200000$).

\begin{table}[ht]
    % \begin{footnotesize}
    \centering
    \begin{tabular}{c|ccc}
    \textbf{Algorithms} $\;$ \textbackslash $\;$ \textbf{Number of players} & $M=3$ & $M=6$ & $M=9$ \\
        \hline
        Centralized Multiple play \klUCB{} & $\mathbf{91 \pm 21}$ & $\mathbf{70 \pm 16}$ & $\mathbf{0 \pm 0}$ \\
        \Selfish-\klUCB{} & $249 \pm 35$ & $728 \pm 94$ & $2953 \pm 501$ \\
        \hline
        \textsc{Sic-MMAB} & $1705 \pm 340$ & $3915 \pm 300$ & $1713 \pm 52$ \\
        \textcolor{red}{Improved \MusicalChair{}} & $\textcolor{red}{12149 \pm 60}$ & $\textcolor{red}{22341 \pm 77}$ & $\textcolor{red}{27462 \pm 85}$ \\
        \hline
    \end{tabular}
    % \caption{Mean regret $\pm$ $1$ std-dev, for different algorithms on the same problem with $M=3,6,9$, for the ``no sensing'' case.}
    \caption{Comparison of the mean regret $\pm$ $1$ std-dev, for different algorithms, on the same problem with $M=3,6,9$ players, for the ``no sensing'' case. More work is needed on our implementation on Improved \MusicalChair. The results on \textsc{Sic-MMAB} confirm the numerical experiments of \cite{BoursierPerchet18}.}
    \label{table:5:meanRegretSimulationsNoSensing}
% \end{footnotesize}
\end{table}

% % M=3
% SIC-MMAB(kl-UCB, $T_0=226$) : 1705 ± 340
% TSN($T_{RH} = 140, T_{SH} = 1.53e+04$) : 9001 ± 2
% Selfish-kl-UCB : 249 ± 35
% CentralizedMultiplePlay(kl-UCB) : 91 ± 21
% MusicalChair($T_0=1000$) : 1410 ± 641
% MusicalChair($T_0=450$) : 1268 ± 1110
% MusicalChair($T_0=900$) : 1355 ± 681
% MusicalChair($T_0=1350$) : 1791 ± 561
% MusicalChair($T_0=95117$) : 12145 ± 59
% MusicalChair($T_0=118764$) : 12142 ± 59
% MusicalChair($T_0=220257$) : 12146 ± 59
% MCNoSensing($M=3$, $T=10000$, $c=1$, $g=235$) : 12149 ± 60
% MCNoSensing($M=3$, $T=10000$, $c=10$, $g=2.35e+03$) : 12148 ± 66
% MCNoSensing($M=3$, $T=10000$, $c=128$, $g=3.01e+04$) : 12149 ± 56

% % M=6

% SIC-MMAB(kl-UCB, $T_0=226$) : 3915 ± 300
% TSN($T_{RH} = 140, T_{SH} = 1.53e+04$) : 9008 ± 5
% Selfish-kl-UCB'  : 728 ± 94
% CentralizedMultiplePlay(kl-UCB)' : 70 ± 16
% MusicalChair($T_0=1000$) : 3130 ± 1583
% MusicalChair($T_0=450$) : 2690 ± 2473
% MusicalChair($T_0=900$) : 3015 ± 1839
% MusicalChair($T_0=1350$) : 3403 ± 1202
% MusicalChair($T_0=95117$) : 22363 ± 89
% MusicalChair($T_0=118764$) : 22352 ± 74
% MusicalChair($T_0=220257$) : 22352 ± 75
% MCNoSensing($M=6$, $T=10000$, $c=1$, $g=235$) : 22341 ± 77
% MCNoSensing($M=6$, $T=10000$, $c=10$, $g=2.35e+03$) : 22365 ± 80
% MCNoSensing($M=6$, $T=10000$, $c=128$, $g=3.01e+04$) : 22349 ± 84

% % M=9
% SIC-MMAB(kl-UCB, $T_0=226$) : 1713 ± 52
% TSN($T_{RH} = 140, T_{SH} = 1.53e+04$) : 39 ± 18
% Selfish-kl-UCB : 2953 ± 501
% CentralizedMultiplePlay(kl-UCB) : 0 ± 0
% MusicalChair($T_0=1000$) : 2775 ± 27
% MusicalChair($T_0=450$) : 1455 ± 1059
% MusicalChair($T_0=900$) : 2506 ± 30
% MusicalChair($T_0=1350$) : 3737 ± 36
% MusicalChair($T_0=95117$) : 27466 ± 90
% MusicalChair($T_0=118764$) : 27480 ± 80
% MusicalChair($T_0=220257$) : 27467 ± 95
% MCNoSensing($M=9$, $T=10000$, $c=1$, $g=235$) : 27462 ± 85
% MCNoSensing($M=9$, $T=10000$, $c=10$, $g=2.35e+03$) : 27463 ± 101
% MCNoSensing($M=9$, $T=10000$, $c=128$, $g=3.01e+04$) : 27462 ± 76


% ----------------------------------------------------------------------------
\subsection{With communication or coordination between players}
\label{sub:5:withCommunicationOrCoordination}

In the models of IoT networks considered in this thesis, we assume that \emph{the different IoT devices cannot communicate with each other}, and can only communicate with their gateway.
This hypothesis is realistic, mostly for energy consumption and spectrum efficiency reasons.
Of course, we can relax this hypothesis, and in practice in some families of wireless networks, communication between devices are possible.
Note that this extension is \emph{not} considering a graph of distributed agents all playing cooperatively to solve a unique bandit game, like it is studied in the ``\emph{graph bandit}'' problem \cite{valko2016bandits}.
We are interested here by an extension of the model where players can send some bits to one or all the other players, at some or every time steps.

Clearly, the problem is easier by allowing communication, and it is quite immediate to see that the communication capacity between players must be limited otherwise the problem is already known and solved.
Indeed, imagine that at each time step $t$, all the $M$ players could share an unbounded number of bits with the other players, at no cost (even if this hypothesis is clearly unrealistic for wireless networks).
Then they can share all their observations, and they can all run the same multiple-plays MAB algorithm \cite{Anantharam87a}, like for instance the extension of Thompson sampling for multiple-plays studied in \cite{Komiyama15}, or extensions of \KLUCB{} from \cite{Luedtke16}.
In this setting, a logarithmic regret is easily obtained, and the regret upper-bound of the two aforementioned algorithms asymptotically achieve the lower-bound from \cite{Anantharam87a}.

A more interesting extension is thus to limit the communication between players, either to a small number of bits or just one bit, at every or only some time steps.
We identified that the state-of-the-art on this direction of research consists in the two very recent works
% published in April 2019, while I was finishing this chapter:
\cite{tao2019collaborative} and \cite{wang2019distributed}.
Distributed pure exploration is studied in \cite{tao2019collaborative}, where the proposed new lower bounds for the regret of any algorithm for the distributed best arm identification problem, under the fixed time or fixed confidence settings.
The also propose effective algorithms that asymptotically match their lower-bounds (up-to logarithmic factors, in some cases).
The second work studies distributed learning for (not necessarily stochastic) multi-armed bandits as well as linear bandits \cite{tao2019collaborative}.
For a distributed $K$-armed bandit with $M$ agents, they developed two protocols achieving near-optimal regret $\cO(\sqrt{M K T \log(T)})$,
and requiring little communication cost,
one is independent of the time horizon $T$ and use $\cO(M \log(T))$ communications,
and the other is independent of the number of arms $K$ and use $\cO(M K \log(M))$ communications.
Their model and algorithms fit in the distributed, decentralized framework we advertise in all this thesis, and this last work is very interesting.

Additionally, after the recent work \cite{BoursierPerchet18} introduced the ``communication trick'',
some recent research efforts have been more focus on studying the basic model --without explicit communications between players--, while proposing algorithms that can rely on some communication between players.
This ``communication trick'' is used for instance in \cite{KaufmannAbbas19}.
Due to space constraint, we let the interested reader refer to these last two works \cite{BoursierPerchet18,KaufmannAbbas19} as they are both solid, and well explained.

Another line of research is to consider models that are closer to realistic wireless communication networks, as it is done in \cite{Avner16,AvnerMannor18}.
Explaining in details their model and proposed solutions would be quite lengthy, and we rather prefer to let the interested reader refer to the later work \cite{AvnerMannor18}.
We sum-up its contributions quickly.
They address several aspects of the challenge of communication networks shared by many users simultaneously: learning unknown stochastic network characteristics, sharing resources with other users while keeping coordination overhead to a minimum.
The solution they proposed combines Multi-Armed Bandit learning with a lightweight signalling-based coordination scheme, and ensures convergence to a stable allocation of resources.
Their work considers single-user level algorithms for two scenarios: an unknown fixed number of users, and a dynamic number of users, both for different arms means for each user (players).
Analytic performance guarantees, proving convergence to stable marriage configurations, are presented for both setups. The algorithms are based on a system-wide perspective, rather than focusing on single user welfare.

% \TODOL{A conclusion from this sub-section presenting this model?}


% ----------------------------------------------------------------------------
\subsection{With different arm utilities among players}
\label{sub:5:withDifferentMeansAmongPlayers}

In the multi-players model presented in this chapter, we assume that the arm distributions are the same for all the players.
For cognitive radio applications, where arms model channels and players are radio devices, it can be unrealistic to consider that two players, maybe located at different distances from the gateway or equipped with different hardwares, encounter the same mean quality when accessing the same channel.
%
Motivated by this weakness, some researchers studied an interesting extension of the model of interest, considering multi-players MAB models with different arms distributions among players.
%
Starting in 2012 by \cite{Kalathil12} where arms are Markov chain, this model was studied more actively recently, \cite{DarakHanawal18,Bistritz18,KaufmannAbbas19,Tibrewal2019}.

In such models, instead of considering $K$ arms characterized by a vector of distributions, $(\nu_k)_{1\leq k \leq K}$, if there is $M$ players we consider a \emph{matrix of distributions}, $(\nu_k^j)_{1 \leq k \leq K, 1 \leq j \leq M}$. Two users $j$ and $j'$ can experience different utilities for the same arm $k$, \ie, $\nu_k^{j} \neq \nu_k^{j'}$.
The goal stays the same, each player wants to maximize its cumulated reward, with or without explicit communications between players, with or without sensing information, but always without central supervision.
%
Like before, maximizing the rewards of each player simultaneously is maybe not possible.
As soon as the matrix is not invariant under permutation of the users, the problem nature changes fundamentally:
instead of finding an optimal orthogonal assignment of the $M$ players to the \Mbest{} arms,
the goal of the system is now to reach an equilibrium position, also referred to as a stable marriage.
Assignment are also called matching, and total (mean) reward of an assignment is its utility.
Such equilibrium position means that the utility obtained by the $M$ player cannot be increased by swapping two users.
Indeed imagine just $M=3$ players and $K=3$ arms, and Bernoulli distributions of means $[0.1, 0.5, 0.5]$, $[0.1, 0.5, 0.5]$ and $[0.9, 0.5, 0.1]$ for player $1$, $2$ and $3$. Then two optimal affectations of players to arms are $[3,2,1]$ or $[2,3,1]$, both giving the same utility of $1.9$.

In this extension, performance is still evaluated by a system regret, now defined as the difference between the sum of the cumulated rewards by the $M$ players, and the utility of any matching.
The question is to know if it is possible to obtain a logarithmic --or even sub-linear-- regret in this problem is more difficult that for the model presented in Section~\ref{sec:5:model}.
This question was first answered in \cite{Bistritz18}, and proposed an algorithm based on alternating three phases, and increasing their lengths after the end of each epoch ($2^p$ for the $p$-th epoch).
First, players explore in order to estimate the expectations of the arm rewards ; then players use their ``Game of Thrones'' dynamics (GoT, inspired by Musical Chair \cite{Rosenski16}) and play the optimal solution most of the time ; finally players play the action they played most of the time in the recent GoT phases.
% combining Musical Chair with an epoch-based alternative phases of forced exploration and pure exploitation.
They analyze their GoT algorithm and proven that it achieves a regret of $\cO((\log(T))^{2+\kappa})$ for any positive constant $\kappa$, as small as possible, if $\kappa$ is known by the algorithm.

Until the very recent work of \cite{KaufmannAbbas19}, it was unknown if a logarithmic regret was possible.
They proposed an algorithm that is based on two phases: first, players will learn $M$ and learn orthogonal ranks, using the ``communication trick'' of~\cite{BoursierPerchet18}, and then one player is elected as a leader and the others are followers.
This second step, after initialization, is also using epoch of increasing lengths $2^p$.
Until the optimal solution, each epoch is alternating three phases.
Followers start by a Round-Robin uniform exploration with no collision, then they use the ``communication trick'' to communicate their samples to the leader, and finally they start an exploitation phase until the end of the game if the leader player tells them so.
The leader can thus collect enough samples from all arms and all players, and is able to solve iteratively the stable marriage problem, and sends back the successive estimate of the solution to the players.
%
In the easier case when their is a unique optimal matching, their algorithm Multiplayer Explore-Then-Commit (M-ETC) is the first one to achieve logarithmic regret, in the form of $R_T = \cO(M K \log(K T) + K M^3 \log(M K T) / \Delta) + K M^2 (\log(M \log(K T) / \Delta))^2$, if $\Delta$ is defined as the gap between the utility of the best matching and the utility of that of the matching with second best utility.
In the generic case, their algorithm M-ETC with Elimination achieves a regret of $\cO((\log(T))^{1+\kappa})$ for any positive constant $\kappa$.

% \TODOL{Present the model, quote the main papers, explain the difficulty and how XXX algorithm solves it!}

Finally, we note that previous works all focused on the ``sensing'' case, but most likely sub-linear regret can be achieve by decentralized algorithms that leverage the same techniques as introduced by \cite{BoursierPerchet18,LugosiMehrabian18} for the ``no sensing case''.
%
We conclude by noting that this model was recently studied by two very recent other works, \cite{DarakHanawal18,Tibrewal2019}, who obtained results comparable to the results from the two works presented above.
They both also study the case of dynamic settings, with arrival or departures of players, as presented in Section~\ref{sub:5:arrivalDepartures}.
Both articles \cite{DarakHanawal18,Tibrewal2019} use the terminology of ad-hoc networks, and they compare empirically their proposal with some previous works, while \cite{KaufmannAbbas19} do not include numerical simulations, and while \cite{Bistritz18} illustrate the performance of their GoT algorithm on a simple example, they do not compare with other algorithms.
%
It would be interesting to compare empirically all the different approaches.
Another interesting directions are to study the possible extension of this model with different arm utilities by players to the non-stationary case, or real-world validation of such decentralized algorithms in real IoT networks.

% Distributed Learning and Stable Orthogonalization in Ad-Hoc Networks with Heterogeneous Channels
% Sumit J Darak, Manjesh K. Hanawal
% https://arxiv.org/pdf/1812.11651
% \cite{DarakHanawal18}

% Distributed Learning and Optimal Assignment in Multiplayer Heterogeneous Networks
% H Tibrewal, S Patchala, MK Hanawal
% https://arxiv.org/pdf/1901.03868.pdf
% \cite{Tibrewal2019}
% ``The channel characteristics are unknown  and  could  be  different  for  each  user  (heterogeneous)''


% ----------------------------------------------------------------------------
\subsection{Modeling more closely a real wireless network}
\label{sub:5:moreRealisticModels}

We simply quote here three recent articles which proposed a similar approach of using decentralized reinforcement learning algorithms on the device-side on wireless networks,
but proposed models closer to the reality of wireless networks.
%
The first work is \cite{NaparstekCohen17}, which proposes to use a decentralized learning based on deep learning, in the ``no sensing'' case, but in a model where the users have to learn not only the channels to use for their uplink messages (only from the feedback through the acknowledgements, like in Section~\ref{sec:4:firstModel}), but where they have to learn the whole ``spectrum access actions''.
% Deep Multi-User Reinforcement Learning for Dynamic Spectrum Access in Multichannel Wireless Networks
% FIXME continue!
%
%
The work of \cite{AvnerMannor18} is discussed above for the case with communicating players, and their model is very interesting from the point-of-view of real-world wireless communication protocols.

Finally, the very recent work of \cite{Zafaruddin2019} is the first one to present experiments of reinforcement learning done on simulated LTE and 5G channels.
The mathematics behind their model are actually quite close to the model, but they explain it in terms of OFDMA and Quality-of-Service (QoS).
Their algorithm is essentially based on a pre-agreement of the $M$ players, that will deterministically run an alternance of exploration phase, auction phase and exploitation phase, of (exponentially) increasing durations.
By diving into the details of the modulation and giving explicitly the form of the uplink messages sent by the devices, the authors are able to set-up two different uplink packets, to efficiently perform the auction phase (see Figure~2).
They prove a regret upper-bound of the order $R_T = \bigO{\log(T)}$, with no special care regarding the constants, but we can also note that their algorithm require a prior knowledge of $\Delta_{\min}$ a problem-dependent constant (Theorem~3).


% ----------------------------------------------------------------------------
\subsection{Inspirations from Cognitive Radio Proof-of-Concepts}
\label{sub:5:USRPdemos}

Similarly to our demonstration of multi-armed bandit learning in an IoT network that we presented in the previous Chapter~\ref{chapter:4} in Section~\ref{sec:4:gnuradio},
we list here some related works, dating back from $2016$, who proposed a similar approach for OSA.
All the similar works that we are aware of also used USRP boards, and implemented the demonstration using either the Simulink and MATLAB softwares, or the GNU Radio software like we did \cite{Besson2018ICT}.
%
Indeed, the demonstrations and the works presented below are coming from our team experiments, and have been extended by the team of Sumit Darak at IIIT Delhi after his postdoctoral fellowship in our team.

In \cite{darak2016bayesian,Darak16}, the authors study the same model and focused on the \RhoRand{} policy, which was the state-of-the-art back in $2016$, combined with \UCB{} or \klUCB, as well as an extension using Bayes-UCB from \cite{Kaufmann12BUCB}.
Their idea is to use the same skeleton as \RhoRand, that is to assign a \emph{rank} to each player, and make players change their rank after any collision. But instead of selecting a new rank uniformly at random among $[M]$, they proposed to use a ``second-stage'' learning policy\footnote{This is a very natural idea: use a bandit algorithm to balance the exploration/exploitation aspect of rank selection, instead of a random hoping.}, based on Bayes-UCB, to (try to) learn the best rank while still exploring each rank from time to time.
This algorithm is implemented in SMPyBandits and named \rhoLearn,
% \footnote{See \href{https://smpybandits.github.io/docs/PoliciesMultiPlayers.rhoLearn.html}{\texttt{SMPyBandits.github.io/docs/PoliciesMultiPlayers.rhoLearn.html}} for its documentation.},
where it can use Bayes-UCB or any of the $65$ or more bandit algorithms available in the library.
Even if no theoretical guarantee backs up this idea of second-stage learning with Bayes-UCB, empirical simulations found that it can be efficient.
We illustrate this in Table~\ref{table:5:comparisonRhoRandRhoLearn} below, where we consider the same problem as described above in Section~\ref{sub:5:withoutSensing} (Table~\ref{table:5:meanRegretSimulationsNoSensing}).
We include different \rhoLearn{} algorithms, that uses \klUCB{}, Bayes-UCB or Exp3 for the second-stage learning,
and we include both the centralized multiple-play version of \klUCB{} and \MCTopM-\klUCB{} for comparison.

% DONE run the simulations with N=100 and not N=4
% for M in 3 6 9; do DEBUG=False SAVEALL=False NOPLOTS=True M=$M K=9 N_JOBS=-1 N=100 T=10000 make moremultiplayers; read; done
\begin{table}[ht]
% \begin{footnotesize}
    \centering
    \begin{tabular}{c|ccc}
    \textbf{Algorithm} $\;$ \textbackslash $\;$ Number of players & $M=3$ & $M=6$ & $M=9$ \\
        \hline
        Centralized multiple-play \klUCB{} & $\mathbf{92 \pm 20}$ & $\mathbf{70 \pm 16}$ & $\mathbf{0}$ \\
        \hline
        \RhoRand-\klUCB{} & $417 \pm 103$ & $2481 \pm 449$ & $6639 \pm 1035$ \\
        \hline
        \rhoLearn-\klUCB{} + \klUCB{} & $546 \pm 190$ & $1172 \pm 295$ & $1416 \pm 347$ \\
        \rhoLearn-\klUCB{} + Bayes-UCB & $561 \pm 219$ & $1204 \pm 394$ & $1363 \pm 331$ \\
        \rhoLearn-\klUCB{} + Exp3 & $529 \pm 175$ & $1659 \pm 331$ & $5134 \pm 976$ \\
        \hline
        \MCTopM-\klUCB{} & $244 \pm 39$ & $401 \pm 55$ & $44 \pm 13$ \\
        \hline
    \end{tabular}
    \caption{Comparing \RhoRand{} and \rhoLearn{} on a simple MP-MAB problem with $K=9$ arms.}
    \label{table:5:comparisonRhoRandRhoLearn}
% \end{footnotesize}
\end{table}


% % FIXME much more details?
% Other works in the same line of research included \cite{modiDemo2016} implemented using the same USRP testbed as for our demonstration in Chapter~\ref{chapter:4} (Section~\ref{sec:4:gnuradio}),
% and the first work of a research team based in IIIT Dehli, \cite{kumar2016two}.
% %
% More recent works by the same team, \cite{KumarYadav2018,SawantKumar2018,JoshiKumar2018}, all follow the same approach and include an experimental section that shows the results of an implementation of the proposed algorithms on USRP boards.
% % \TODOL{I should explain more, no? Who does what and why? So long...}


% ----------------------------------------------------------------------------
\subsection{With malicious jammers}
\label{sub:5:withMaliciousJammers}

In all this chapter and the previous research literature, another common hypothesis is that all the $M$ players are pursuing the same goal, and are all behaving nicely by following the same algorithm.
% Efficient utilization of licensed spectrum in the cognitive radio network is challenging due to lack of coordination among the Secondary Users (SUs).
Distributed algorithms proposed in the literature aim to maximize the network throughput by ensuring orthogonal channel allocation for the SUs.
However, these algorithms work under the assumption that all the SUs faithfully follow the algorithms which may not always hold due to the decentralized nature of the network.
In the paper \cite{SawantKumar2018}, the authors study for the first time distributed algorithms that are robust against malicious behavior, also called \emph{jamming attack}.
They consider both the cases of \emph{jammers} launching coordinated and uncoordinated attacks, and consider a set of $J$ jammers.
In the coordinated attack, the jammers select non-overlapping channels to attack in each time slot and can significantly increase the number of collisions for SUs.
They setup the problem in each scenario as a multi-players bandit and develop algorithm,
and their analysis shows that when the SUs faithfully implement proposed algorithms, the regret is constant with high probability.
They validate their claims through exhaustive synthetic experiments and also through a realistic USRP.
Their synthetic experiments consider different cases, for different values of $K$ the number of channels, $M$ the number of players and $J$ the number of jammers.

% \TODOL{Present the model, quote the main papers, explain the difficulty and how XXX algorithm solves it!}


% ----------------------------------------------------------------------------
\subsection{Towards non-stationary multi-players MAB models}
\label{sub:5:towardsNonStationaryModels}

Since the beginning of this thesis, all the studied bandit problems were stationary: the rewards coming from choosing arm $k$ are \iid{} and follow the same distribution $\nu_k$.
The next Chapter~\ref{chapter:6} is focused on the piece-wise stationary bandit model, a relaxation of this hypothesis.

When we were working on multi-players bandits in Autumn $2017$ \cite{Besson2018ALT}, we left the study of the piece-wise stationary case as a future work, and shortly after we were excited to see that three independent works tackling this question were published, in December $2018$ \cite{WeiSrivastava18Distributed} and in February $2019$ \cite{AlaturLevyKrause19,bande2019adversarial}.
Without diving too much into the details, we review here these three works.
Without a more serious analysis, we conjecture that it should not be difficult to merge the regret lower-bound for stationary \emph{multi-players} MAB (Theorem~\ref{thm:5:BetterLowerBound}) and the lower-bound for \emph{piece-wise stationary} single-player MAB from \cite{Garivier11UCBDiscount}.
We conjecture that any reasonable decentralized bandit algorithm must suffer a regret at least $\Omega(M \sqrt{K \Upsilon_T T})$ for $M \leq K$ players, $K$ arms, and $\Upsilon_T$ stationary intervals.

On the one hand, piece-wise stationary multi-players MAB can be tackled by extending algorithms developed for the single-player case.
In \cite{WeiSrivastava18Distributed}, the authors study exactly the same multi-players bandit model as our model, and they propose two distributed algorithms that can efficiently be used by $M \leq K$ players to achieve sub-linear regret for piece-wise stationary problems, essentially by considering it as a harder case of a stationary problem. \\
\indent
The proposed the RR-SW-UCB\# algorithm, which combines their SW-UCB\# algorithm, previously proposed in \cite{WeiSrivastava18Abruptly}, and a Round-Robin hoping, under the assumption that each user $j\in[M]$ knows its ID $j$ (we criticize this unrealistic assumption in Section~\ref{par:5:knowingYourIDinMPBanditsGame} above).
The SW-UCB\# algorithm is discussed more in details in the literature review of Chapter~\ref{chapter:6} below.
%
If $\Upsilon_T$ is bounded by $\Upsilon_T = \bigO{T^{\gamma}}$, for a known $\gamma$ but an unknown $\Upsilon_T$, they prove for instance in Theorem~2 \cite{WeiSrivastava18Distributed} that the expected cumulative regret of their RR-SW-UCB\# algorithm is bounded by $R_T = \bigO{T^{\frac{1+\gamma}{2}} \log(T)}$ (what we call the centralized system regret).
This bound is of the same order as the bound obtained by the same authors for the SW-UCB\# algorithm in \cite{WeiSrivastava18Abruptly} for the single-player case.
If $\Upsilon_T = \bigO{T^{\gamma}}$, this bound is comparable to the results obtained for most of the research literature on piece-wise stationary bandits (earlier works like D-UCB in \cite{Garivier11UCBDiscount} have the $\log(T)$ outside the square root, while more recent works all improved this aspect and have the $\log(T)$ in the square root, like for instance our algorithm \GLRklUCB{} in \cite{Besson2019GLRT} and presented in Chapter~\ref{chapter:6}).
Even if their analysis is not explicit regarding the constant, in the case where $\Upsilon_T = \bigO{T^{\gamma}}$ for a known $\gamma$, their regret upper-bound actually matches the conjectured lower-bound, up to a logarithmic factor $\log(T)$. \\
%
\indent
And finally, even though numerical simulations in their papers \cite{WeiSrivastava18Abruptly,WeiSrivastava18Distributed} are interesting and confirm the regret upper-bounds, they do not compare with any other algorithm, and it is left as an interesting future work to study this direction in more details.
Sadly, a major drawback of their work is that assume that player $j\in[M]$ knows its index $j$, and as we explained above this small hypothesis is actually quite strong, as it allows players to be already orthogonal, and it reduces greatly the difficulty of the decentralized bandit problem.

On the other hand, another possibility is to extend ideas developed for the adversarial setting.
In \cite{AlaturLevyKrause19}, the authors essentially consider the piece-wise stationary as an easier case of an adversarial problem.
The recent work \cite{bande2019adversarial} also proposes a decentralized algorithm that can achieve sub-linear regret ($\cO(T^{3/4})$) under an adversarial multi-players bandit model.
\\
\indent
In \cite{AlaturLevyKrause19}, the authors build on the \MusicalChair{} algorithm from \cite{Rosenski16}, to let the $M$ players converge to a ranking in an efficient and decentralized way (cf. Algorithm 1), and they apparently discovered the ``communication trick'' independently from \cite{BoursierPerchet18} to use (virtual) communications between players to set up a ``coordinator'' player and $M-1$ ``followers'' (running respectively their Algorithms 2 and 3).
%
Their key algorithmic technique is to imitate the idealized case where there is full communication between the players. Then, to address the no-communication constraint, we enforce the players to keep the same decisions (arms) within long periods of time (blocks). This gives them the chance to coordinate between themselves via a simple protocol that uses collisions as a primitive, yet effective manner of communication.\\
%
\indent
Their proposal is using the Exp3 algorithm from \cite{Auer02NonStochastic} on a combinatorial problem: they consider ``meta'' arms that are affectations of the $M$ players to $M$ distinct arms among the $K$ arms.
As soon as the ``coordinator'' can effectively use one algorithm to decide the arms played by all the $M$ players, they show that the regret will grow as $R_T = \bigO{M^{4/3} K^{2/3} (\log(K))^{1/3} T^{2/3}}$ as shown in Theorem~4.1
(they used $K$ for $M$ the number of players, and $N$ for $K$ the number of arms).
This bound is much worse than the one obtained for the first article \cite{WeiSrivastava18Distributed}, but is more general, and it is much worse than the bound of $\bigO{\sqrt{K T \log(K)}}$ obtained for Exp3 for the single-player adversarial setting in \cite{Auer02NonStochastic}.
Note that this bound in $T^{2/3}$ is of the same order as the one given in \cite{Avner15}, for the \MEGA{} algorithm.
However, we note that the dependency in $M$ is surprisingly better for their result than for the regret upper-bound for \MCTopM-\klUCB{} (while scales in $M^3$ in Theorem~\ref{thm:5:LogarithmicRegret_MCTopMklUCB}).\\
\indent
Moreover, at first sight, this idea of ``meta'' arms implies an exponential blow up in terms of computational and storage cost, as there is ${M \choose K}$ such ``meta arms'', but
the authors provide in Section~4.1 and Lemma~4.1 an interesting discussion regarding the time efficiency of their proposed algorithm, and they show it can stay polynomial in $M$ and $K$ by leveraging techniques from the Determinental Point Processes (DPP, see \cite{GaBaVa18} for a good review).
%
In this second work also, we can note the poor experimental section, as the proposed algorithm is only compared against the \MusicalChair{} algorithm, in ``easy'' problems (small number of break-points $\Upsilon_T$).
Despite being more complicated than other approaches, their algorithm ``\texttt{C \& P}'' should not be too hard to implement by ourselves, and studying its empirical performance is an interesting future work.


% \TODOL{Explain that we think the following approach can work very efficiently:

%     - Combine \MCTopM{} with \GLRklUCB{} instead of \klUCB,
%     - and incorporate also the detected change-points by B-\GLR{} test in the orthogonalization procedure: after any change-point, the player is not sitting anymore.
%     - Modification: only a change-point on the arm on which player is sitting can make him move? It's already the case: the \GLR{} test can only detect change-point on an arm that was played, and the \MCTopM{} orthogonalization procedure forces to play the same arm as long as the player is sitting.
% }

% \TODOL{Je peux faire des expériences pour cette approche, est-ce que j'en fais ? Ici, ou à la fin du chapitre 6 ? Ou en conclusion ?}


\newpage  % FIXME
% ----------------------------------------------------------------------------
\section{Conclusion}
\label{sec:5:conclusion}
% -----------------------------------------------------------------

To sum up, we presented in this chapter three variants of Multi-Players Multi-Arm Bandits,
with different levels of feedback being available to the decentralized players, under which we proposed efficient algorithms.
The two easiest models are the ones with sensing information (\ie, for OSA), for which our theoretical contribution improves both the state-of-the-art upper and lower bounds on the regret. In the absence of sensing, we also provide some motivation for the practical use of the interesting \Selfish{} heuristic, a simple index policy based on hybrid indices that are directly taking  into account the collision information.
%
We also reviewed various variants of this model, and for some interesting variants we discussed the related literature, which has proven to be very active in the last two years. For some models, we explained why our approach does not work efficiently without modifications, but we detailed and illustrated how to adapt \MCTopM{} to other settings.
For example, it assumes to know the number of players $M$ before-hand, but we illustrated that previously introduced technique to estimate $M$ can also be applied to our proposal and give satisfactory empirical performances.
Further works would be required to adapt the theoretical analysis to these various extensions.

This chapter suggests several interesting further research directions.
First, one could want to investigate the notion of \emph{optimal algorithms} in the decentralized multi-players model with sensing information. So far we provided the first matching upper and lower bound on the expected number of sub-optimal arms selections, which suggests some form of (asymptotic) optimality. However, sub-optimal draws turn out to not be the dominant term in the regret, both in our upper bound and in practice, thus an interesting future work is to identify some notion of \emph{minimal number of collisions}.
Similarly to what was done very recently in \cite{wang2019distributed}, for communications between players who collaborate with each other, it would be interesting to characterize the (minimal) number of collisions needed to achieve logarithmic regret.

% Second, it remains an open question to know if a simple decentralized algorithm can be as efficient as \MCTopM{} without knowing $M$ in advance, or in dynamic settings (when $M$ can change in time). One could start by proposing variants of our algorithm that are inspired by the \rhoRandEst{} variant of \rhoRand{} proposed by \cite{Anandkumar11}.
% Finally, one could want to strengthen the guarantees obtained in the absence of sensing, that is to know whether logarithmic regret is achievable.


Additionally, we have implemented in SMPyBandits \cite{SMPyBandits} the following extensions: evaluating $M$ on the fly (Section~\ref{sub:5:unknownNumberOfPlayers}), and piece-wise stationary multi-players bandits (Section~\ref{sub:5:towardsNonStationaryModels}).
We have not yet implemented the other extensions, but we are interested to do it, and it is one of the major future works left on our library.
For more details, see the issue tickets at \href{https://github.com/SMPyBandits/SMPyBandits/issues/}{\texttt{GitHub.com/SMPyBandits/SMPyBandits/issues/}}, for tickets number \href{https://github.com/SMPyBandits/SMPyBandits/issues/120}{120}, \href{https://github.com/SMPyBandits/SMPyBandits/issues/124}{124}, \href{https://github.com/SMPyBandits/SMPyBandits/issues/185}{185}.

% ----------------------------------------------------------------------------
% \subsection{Some future work on SMPyBandits}
\paragraph{Reproducility of the experiments.}

The experiments in this chapter use SMPyBandits,
and we refer to the following page for details on how to reproduce them,\\
\href{https://SMPyBandits.GitHub.io/MultiPlayers.html}{\texttt{SMPyBandits.GitHub.io/MultiPlayers.html}}

% -----------------------------------------------------------------


% \paragraph{Acknowledgments.}
% Thanks to Odalric-Ambrym Maillard at Inria Lille for useful discussions, and thanks to Christophe Moy at University Rennes 1.

% \paragraph{A note on the simulation code.}


\newpage
% ----------------------------------------------------------------------------
\section{Appendix}
\label{sec:5:appendix}

% We include here some missing proofs and additional simulation results for this Chapter.

% ----------------------------------------------------------------------------
\input{2-Chapters/5-Chapter/ALT_2018__MPBandits.git/mpBandits_appendix.tex}

