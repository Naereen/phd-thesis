%!TEX root = ../PhD_thesis__Lilian_Besson

\chapter[Multi-Player Multi-Armed Bandits Models]{Multi-Player Multi-Armed Bandits Models and Algorithms}
\label{chapter:5}
\minitoc


\paragraph{Abstract}

In this Chapter~\ref{chapter:5}, we are interested in a more formal approach to the decentralized learning problem presented in Chapter~\ref{chapter:4}.
We consider $M$ identical dynamic IoT devices, communicating with a unique gateway, in $K$ orthogonal channels and in a acknowledgment-based wireless protocol slotted in time.
A perfect time and frequency synchronization is assumed,
and some \iid{} background traffic is assumed to be non-uniformly in the $K$ channels.
As before, if two or more devices decide to use the same channel at the same time, a \emph{collision} arises and none of sent uplink packet can be received by the gateway.
%
Such networks can be modelled using a decentralized multi-player multi-armed bandit problem, where arms are channels and players are dynamic IoT devices.
Unfortunately, it is very hard to formally analyze the IoT network model we presented in Chapter~\ref{chapter:4}, mainly because of the random activation process of all the dynamic learning devices.
Even if there are many identical bandit algorithms learning independently and in a decentralized way, the difficulty mainly comes from the fact that all of them are only communicating at some (random) time steps, and at each time step the number of communicating devices is random and unpredictable.

Mainly for these two reasons, for this Chapter~\ref{chapter:5} we prefer to only consider the easier case of at most $M \leq K$ devices, communicating at each time step.
Each device will use a Multi-Armed Bandit algorithm, to maximize its number of successful communications, by using the received acknowledgement \Ack{} as a (random) binary reward after each uplink message (\ie, at each time step).
%
We start by reviewing previous works on multi-player MAB models, which all considered the easier case of \emph{sensing feedback}.
Each time frame is separated as before in a \emph{sensing} phase (during which the device senses for the background traffic),
an \emph{uplink} phase (during which the device sends a packet to the gateway if it sensed the chosen channel to be free),
and a \emph{downlink} phase (during which it waits for an \Ack{} from the gateway).
In this first model, the binary reward is $1$ only if the channel was sensed to be free of background traffic \textbf{and} if \Ack{} was received, and the device has access to both information.
For this first model, we present two algorithms, \RandTopM{} and \MCTopM, based on the combination of an efficient MAB index policy (we chose \klUCB) and a smart orthogonalization procedure, based on a random hoping procedure called Musical Chair.
Like in previous works, we consider the centralized system regret (multi-player regret), or simply referred to as regret.
We start by showing an improved asymptotical regret lower-bound for any algorithm of a certain class, including previous solutions such as \rhoRand{} and our two solutions.
We then analyze our \MCTopM{} algorithm and we show that its regret upper-bound is logarithmic and order-optimal, improving over the previous state-of-the-art.
%  with a regret bounded by $\bigO{\log(T)}$ for a game at horizon $T$.
We also present extensive numerical simulations that show that our proposal outperforms all previous solutions and is much more efficient in this easier model of sensing feedback with a fixed and known number of players $M$ accessing $K \geq M$ channels.

We are also interested in the harder model where devices do not have access to the two feedback information (sensing and \Ack) but only have access to the \Ack.
Our article \cite{Besson2018ALT} was the first to propose this ``no sensing'' model, for which we only proposed an heuristic, the naive \Selfish{} strategy as it was already used in Chapter~\ref{chapter:4}.
Even if empirical simulations showed that \Selfish-\klUCB{} performs very well, it is a mistake to only consider mean regret, as we found on numerical simulations as well as formal derivation on a simple example of $K=M=2$ that the \Selfish{} heuristic can has a linear regret with a low probability (and so asymptotically it has linear expected regret and fails to solve ``no sensing'' multi-player MAB problems).
We did not propose any other efficient algorithm, but our work presented in April~$2018$ inspired some other articles \cite{LugosiMehrabian18,BoursierPerchet18}.
These two articles confirmed our finding that \Selfish{} can have a linear regret, and they both proposed new algorithms.
We include some numerical simulations to compare some of them, and we present in details the current state-of-the-art of research on multi-player MAB models without sensing.

Another interesting extension of the studied model is to consider different channels utility for each devices. It is a very interesting extension of our model, as for instance the devices could be located on different parts of a building or a field and suffer from different mean qualities of access to each channels.
This extension is not studied per say but we quickly review the existing literature, consisting in the two recent works \cite{Bistritz18,KaufmannAbbas19}, and we discuss the possible real-world usage of the existing algorithms.

Finally, we review other extensions of our model, and give a overview of the current state-of-the-art as exhaustive as possible.


\vfill{}

\paragraph{Publications}

This chapter is mainly based on the following publication: \cite{Besson2018ALT}.


\newpage
% Write miniTOC just after the title
\graphicspath{{2-Chapters/5-Chapter/Images/}}
\graphicspath{{2-Chapters/5-Chapter/ALT_2018__MPBandits.git/figures/}}

% ----------------------------------------------------------------------------
\input{2-Chapters/5-Chapter/ALT_2018__MPBandits.git/mpBandits.tex}


% ----------------------------------------------------------------------------
% ----------------------------------------------------------------------------
\section{Literature review of many extensions of our models}
\label{sec:5:literatureReviewOtherModels}
% ----------------------------------------------------------------------------

Before concluding this chapter, we give a literature review of the many extensions of the three models presented above in Section~\ref{sec:5:model}.
We wanted to highlight that the community was quite active on research on multi-player bandits in the last $10$ years, but especially active since last spring $2018$.
For instance, our article \cite{Besson2018ALT} was the first to propose the ``no sensing'' case, and it was studied by (at least) two independent group of researchers since its publication in April $2018$ \cite{BoursierPerchet18,LugosiMehrabian18}.
Another example is the model with different arm means among players, who was studied in \cite{Anandkumar10,Kalathil12} and then more recently in \cite{Bistritz18,KaufmannAbbas19}.
%
By following actively the research on multi-player bandit models since the last three years, we believe to be able to give an exhaustive literature review, but it is of course possible that we missed an important research work. In this case, it is our mistake and not an intentional choice as we included all the different extensions of our models that we were aware of.

Many extensions of the simpler model of multi-player bandits have been considered.
The number of players $M$ can be fixed but initially unknown to the players, the sensing information can be absent (like our \modeltrois{} presented above), there could be communications between players (happening at each time step or only occasionally), or $M$ could evolve over time to model the possibility of arrivals or departures of players (\ie, connexions and disconnections of devices in a wireless networks).
Also inspired by real-world wireless networks, the arm means may vary among players, for instance to model players located at different distance of the gateway, and we can also consider networks with some ``jammers'' whose purpose is not to communicate to the gateway in a collaborative way, like the $M$ devices, but to interfere with the communications of the $M$ devices.

Finally, we can also be interested by models where $M$ stays fixed, but the environment evolves, for instance with abrupt changes in the means of arms. This last model makes a good connexion between this chapter and the next one, and an exciting future work is to further study this model in order to tackle this kind of problems by merging our proposals for stationary multi-player bandits and for non-stationary single-player bandits.

\TODOL{
    Pour chaque extension présentée ci dessous, je veux suivre la même organisation :

    1. j'explique l'extension, et en quoi c'est difficile (= en quoi notre approche échoue si on fait rien de plus),

    2. j'explique les travaux existants, et les résultats qu'ils obtiennent e.g. en terme de bornes de regret

    3. si j'ai déjà le code pour, je peux montrer UNE simulation
}


% ----------------------------------------------------------------------------
\subsection{Unknown (fixed) number of players}
\label{sub:5:unknownNumberOfPlayers}

In the model we presented above, we assumed the number of players $M$ to be fixed in time during the learning process.
Without removing this hypothesis (we study this case below in Section~\ref{sub:5:arrivalDepartures}), it is interesting to remark that we made no hypothesis on whether the players can known this value $M$ or not.
Our proposals, \RandTopM{} and \MCTopM, both assume to know $M$ beforehand, like it was done for our main inspiration \RhoRand.


\paragraph{Performance of our proposals for a wrong value of $M$?}
%
We can start by asking whether our most efficient proposal, \MCTopM-\klUCB, is still efficient if it uses a value $M'$ different than the real number of players $M$.
Even though we did not include numerical simulations for this case in Section~\ref{sec:5:experiments} above, we did some tests, which confirmed two disappointing results that were also proved analytically.
First, the three algorithms (\RhoRand, \RandTopM{} and \MCTopM) give linear regret if they are using a value $M'$ strictly smaller than $M$ (and if $\mu^*_{M-1} > \mu^*_M$), as players will converge to play about $T-\bigO{\log(T)}$ times the $M-1$ best arms, leading to at least one collision most of the time, and thus a linear regret.
Second, if the $M$ players falsely use a value $M' > M$, then there is also a certain (fixed) probability to achieve a linear regret. Indeed, imagine one player ($M=1$) running \MCTopM{} with the false knowledge of $M'=2$, and if it learned to accurately identify the set of the $2$ best arms, then the \MCTopM{} orthogonalization scheme can make it play the worst of the two arms, and as $M=1$ this player will never encounter any collision, thus playing this suboptimal arm for about $T-\bigO{\log(T)}$ times, also leading to linear regret.


\paragraph{Interpretation of the hypothesis of knowning $M$ for real-world networks.}
%
One could criticize our approach if we study a wireless networks with no central coordination from the gateway,
in particular where the devices cannot be assigned to a channel or be assigned a unique ID by the gateway when they first log in the network.
Then in such networks, if one wants to apply an algorithm like \MCTopM-\klUCB,
the $M$ devices need to known their number $M$, and have to receive it from the gateway, as it is the only part of this example of network which knowns $M$.
It seems unrealistic to ask the gateway to send the fixed value $M$ to each device, and not a unique ID to each device, for instance $\mathrm{id}^j\in\{1,\dots,K\}$.
%
% \TODOL{Explain why the problem is MUCH easier if we have before hand these unique ID!}
If the $M$ players each have a unique ID, then a simple explore-then-commit algorithm running on top of a round-Robin phase can achieve order-optimal regret. Without giving more details, we let the interested refer to what is is explained for the RR-SW-UCB\# algorithm in \cite{WeiSrivastava18Distributed} and the algorithms presented in \cite{DarakHanawal18,JoshiKumar2018,KumarDarak2019}.


\paragraph{Two ideas to estimate $M$.}

Some works studied the same model as our model \modeldeux{} ``with sensing'', under the hypothesis that players do not know in the value of $M$.
As illustrated above, if we consider algorithms building on the same ideas as \RhoRand{} or \MCTopM, it seems mandatory to first build an estimate of the value of $M$ then run the initial algorithm that assumed a perfect knowledge of $M$.
Two possible directions exist to estimate $M$ on the fly.

The first idea comes from \cite{Anandkumar11}, and the intuition behind it is quite simple, even if the mathematical derivations are not.
All players will build an estimate $\hat{M}^j(t)$ of the number of player, that start by $\hat{M}^j(0)=1$. As soon as one collision is observed, a player knows that $M\geq2$, so $\hat{M}^j(t+1) = 2$.
Then, based on probabilistic computations on the expected number of collisions if there were $m$ players, all following the same strategy (\eg, \RhoRand{} in the case of ), and because the formula is simple to compute for different $m$, the authors proposed a statistical test of the hypothesis $M \leq \hat{M}^j(t)$ against $M > \hat{M}^j(t)$ which is expressed as a simple comparison of the current number of collisions (since last update of $\hat{M}^j(t)$) against a threshold.
If a player observed ``too many'' collisions (that are unlikely to be caused by only $\hat{M}^j(t) - 1$ other players), then she increases her current estimate $\hat{M}^j(t+1) = \hat{M}^j(t) + 1$.

The second idea comes from \cite{Rosenski16}, and suppose to know beforehand both the horizon $T$ and a certain measure of the difficulty of the problem (\ie, a lower bound on the minimal gap between two means).
If all the $M$ players start to play for a long enough time $T_0$ uniformly at random among all the $K$ arms, then the (expected) number of collisions observed $\E[\cC^j_{T_0}]$ by any player $j$ is a (relatively simple) function of $M$. By knowing $K$ and $T_0$ and if all players use the same mechanism, then they can invert the formula to obtain the most likely estimate of $M$ which explained the observations of $\cC^j_{T_0}$ collisions.
This second approach works empirically very fine, but it requires a fine tuning of the stopping time $T_0$.
Rosenski et al. studies the performance of their algorithm with high-probability bounds \cite{Rosenski16}, and the tuning of $T_0$ they propose depends on prior knowledge on the problem.
%
For this reason, we are not fond of this approach, as this hypothesis is quite unrealistic if one wants to apply this kind of algorithms for real-world wireless networks.
We note that all the following works assume some sort of prior knowledge on the difficulty of the problem:
\cite{kumar2017channel,KumarYadav2018,SawantKumar2018,JoshiKumar2018,DarakHanawal18,KumarDarak2019,Tibrewal2019}.


\paragraph{Extension of our proposals to learn the value of $M$.}

Similarly to what was proposed for the \rhoRandEst{} algorithm in \cite{Anandkumar11}, we could have worked on proposing and analyzing an extension of our proposals that could efficiently learn the value of $M$ the number of player.
We implemented this \rhoRandEst{} policy in our library SMPyBandits \cite{SMPyBandits}, as well as this mechanism for our proposals \RandTopM{} and \MCTopM.
Building from the theoretical analysis given for \rhoRandEst{} in \cite{Anandkumar11}, and our analysis of \MCTopM-\klUCB, we believe it is possible to show that the aforementioned extension of our proposal also achieves sub-linear regret without requiring players to know $M$ in the beginning of the bandit game.
More precisely, we believe that \MCTopM-\klUCB{} can still give order-optimal logarithmic regret if $M$ players use it, and this extension of Theorem~\ref{thm:5:LogarithmicRegret_MCTopMklUCB} is not included due to space constraints.
Writing its proof and performing more simulations are left as possible future work.

\paragraph{Simulations.}

We consider a bandit problem with $K=9$ arms, of means $0.1,0.2,\dots,0.8,0.9$, and three different cases of $M=3,6,9$ players, for $100$ independent repetitions and an horizon of $T=10000$.
As before, we include the centralized multiple-play \klUCB{} as an unachievably efficient baseline, the \Selfish-\klUCB{} algorithm as an heuristic which does not require the know $M$.
Then we compare the \RhoRand, \RandTopM{} and \MCTopM{} algorithms, using \klUCB, which know $M$ beforehand, with their extensions implementing the same algorithm as \rhoRandEst{} to estimate $M$ on the fly.

\TODOL{Include a simulation to show the behavior of \rhoRand{} vs \rhoRandEst, and \RandTopM{} and \MCTopM{} with prior knowledge of $M$ vs the ``estimate'' versions.

FIXME Je dois refaire ces simulations, les chiffres ici sont avec $T=1000$ et $N=4$ répétitions (juste faites vite fait dans le train, je dois les lancer sur la machine du bureau).
}


\begin{table}[ht]
    % \begin{footnotesize}
        \centering
        \begin{tabular}{c|ccc}
        \textbf{Algorithms} $\;$ \textbackslash $\;$ \textbf{Number of players} & $M=3$ & $M=6$ & $M=9$ \\
            \hline
            Centralized multiple-play \klUCB{} & $\mathbf{52 \pm 15}$ & $\mathbf{35 \pm 15}$ & $\mathbf{0}$ \\
            \hline
            \RhoRand-\klUCB{} & $297 \pm 18$ & $1411 \pm 144$ & $2351 \pm 55$ \\
            \RhoRand-\klUCB{} + \textcolor{red}{Estimate $M$} & \textcolor{red}{$326 \pm 86$} & \textcolor{red}{$1213 \pm 168$} & \textcolor{red}{$2403 \pm 121$} \\
            \hline
            \RandTopM-\klUCB{} & $188 \pm 18$ & $671 \pm 159$ & $790 \pm 470$ \\
            \RandTopM-\klUCB{} + \textcolor{red}{Estimate $M$} & \textcolor{red}{$238 \pm 27$} & \textcolor{red}{$778 \pm 156$} & \textcolor{red}{$1098 \pm 153$} \\
            \hline
            \MCTopM-\klUCB{} & $134 \pm 6$ & $304 \pm 19$ & $42 \pm 5$ \\
            \MCTopM-\klUCB{} + \textcolor{red}{Estimate $M$} & \textcolor{red}{$210 \pm 66$} & \textcolor{red}{$465 \pm 39$} & \textcolor{red}{$602 \pm 54$} \\
            \hline
            \Selfish-\klUCB{} & $136 \pm 17$ & $468 \pm 87$ & $1379 \pm 16$ \\
            \hline
        \end{tabular}
        \caption{Mean regret $\pm$ $1$ std-dev, for different algorithms on the same problem with $M=3,6,9$, comparing algorithms which knows $M$ against algorithms which estimate $M$ on the fly. The three algorithms \RhoRand, \RandTopM{} and \MCTopM{} all suffer from similar costs on their regret when they have to estimate $M$ on the fly. Our proposals are still much more efficient than \RhoRand{} when using the procedure to estimate $M$ described from \cite{Anandkumar11}.}
        \label{table:5:meanRegretSimulationsEstimatingM}
    % \end{footnotesize}
    \end{table}

\TODOL{Give interpretation from these results!}


% ----------------------------------------------------------------------------
\subsection{Without sensing information}
\label{sub:5:withoutSensing}

We presented in Section~\ref{sec:5:model} the model \modeltrois, without sensing information. Players can only observe $Y_k(t)$, the product of the \iid{} random sensing information from arm $k$ and of the no-collision indicator.
This model seems harder than the model with sensing information, and even though our proposal \Selfish{} works fine in simulations (in terms of average regret), we proved that it can yield linear regret.
In our paper \cite{Besson2018ALT} as well as the previous sections, it was left as a future work to know if an algorithm can achieve sub-linear regret in this harder model without sensing information.

Inspired by our article \cite{Besson2018ALT},
Boursier and Perchet studied this question in the summer $2018$ following its publication \cite{BoursierPerchet18}.
Their answer is that the model without sensing information is essentially not harder than the model with sensing information.
%
Similarly, Lugosi and Mehrabian studied the same problem \cite{LugosiMehrabian18}.
In both works, the authors detail algorithms that are unfortunately suboptimal empirically, but give in both cases a logarithmic regret if $M \leq K$ players all independently implement the proposed algorithm.

\paragraph{The ``communication trick''}
%
\TODOL{Write about half a page on the article \cite{BoursierPerchet18}.}


\paragraph{Estimating collisions through uniform exploration}
%
\cite{LugosiMehrabian18} gives a first algorithm with expected regret of $\bigO{M K \log(T) / \Delta^2}$, if $\Delta = \mu^*_{M} - \mu^*_{M+1} \neq 0$, achieving a better dependency regarding $M$ when compared to our result (our bound is $\bigO{M^3}$) but at the cost of (much) larger constants hidden in the $\cO$ notation, and a non-fully explicit algorithms which usually obtain bad (or worse) empirical performance.
Then they study an interesting extension that works also if $\Delta = 0$ or if $\Delta$ is so close than the bound in $1/\Delta^2$ becomes useless for ``small horizons''. This behavior is well known for classical bandits, where bounds of the form $\log(T)/\Delta^2$ become worse than a linear regret $T$ if $\Delta$ is very small (\ie, $\Delta\ll \sqrt{\log(T)/T}$).
For this extension, their proposed algorithm is proved to achieve $\bigO{K^2 M (\log(T))^2 / \mu + K M \min(\sqrt{T \log(T)}, \log(T)/\Delta')}$ regret, if $\mu$ is a lower-bound on $\mu^*_{M}$ and $\Delta' = \max(\Delta, \min\{|\mu^*_M-\mu^*_i| : \mu^*_M > \mu^*_i \})$ (that both have to be known beforehand by the algorithm).
%
The proposed algorithms in \cite{LugosiMehrabian18} are all based on the Musical Chair algorithm from \cite{Rosenski16}, and the curious reader should read their Algorithm~2 (page 15) for more details.

The results presented in \cite{LugosiMehrabian18} are based on some hypotheses, for instance the tuning they propose for the length $g$ of the uniform exploration phase in the beginning of their ``Musical Chair''-like algorithm is based on a prior knowledge of the horizon $T$.
In Section~4 they explain how to relax the assumptions of their results.
For the same example, the ``doubling trick'' technique can be used to obtain a regret upper bound for an algorithm unaware of the value of $T$, within a constant multiplicative factor of the upper bound given for the algorithm aware of $T$ (Section~4.1). Because the regret bound is of the form $\bigO{\sqrt{T} \log(T)}$, a simple ``doubling trick'' of increasing horizons of lengths $T_i = 2^i$ works well, as proposed in \cite{CesaLugosi06} and as studied in depth in our article \cite{Besson2018DoublingTricks}.
They also study in Section~4.3 an extension of the model for the case with more players than arms, but we do not give more details on this aspect. Their definition of a centralized regret for this case is an interesting and natural generalization of our definition, and they also proposed an algorithm achieving $\bigO{M K \log(T) \exp(4M/K) / \Delta^2}$ regret in this case.
Finally, they also proposed in Section~4.4 an extension of their algorithm to estimate the number of players $M$, which is analyzed as for the Musical Chair algorithm in \cite{Rosenski16}, and also achieve logarithmic regret.
Note that it is significantly harder to estimate $M$ without collision information, and Algorithm~3 in page 24 is quite complex.
We did not implement it, and it is an interesting future work to run some numerical simulations in order to validate it empirically.


\paragraph{About \Selfish-\UCB{} inefficiency.}
%
It is also proved in Appendix~A of \cite{BoursierPerchet18} that \Selfish-\UCB{} has a linear regret, in the theoretical case, with a very neat argument from number theory (using Lindemann-Weierstrass theorem). As we conjectured, there is a gap between the theoretical result and its practical consequence, as the Theorem~4 they gave is only valid for real-valued number, and not for hardware-represented floating point number.
Their proof is supporting what we illustrated in Figure~\ref{fig:5:oneGameTree_SelfishKLUCB}.
Their argument is essentially to prove that for \Selfish-powered players using the simple \UCB-indexes, with a probability $p$ at time $t$ (both independent from $T$), two players might have the same number of pulls and the same observed rewards for each arm. In that case, the two players would pull the exact same arms and thus collide for a long time, until they reach a ``tie breaking point'' where they could choose different arms thanks to a random tie breaking rule (\eg, if two values of their \UCB{} indexes are the same, the $\argmax$ is a uniform random choice among the two arms).
They prove that it is unlikely to encounter any of these ``tie breaking points'', in theory if the \UCB{} indexes are real-valued number (that can be rational or irrational).


\subsubsection{Additionnal numerical simulations}

\TODOL{Do some basic simulations and show :

    - Selfish-UCB from \cite{Besson2018ALT}.
    - Improved Musical Chair from \cite{LugosiMehrabian18}.
    - SIC-MMAB from \cite{BoursierPerchet18}.
}

As above, we consider a bandit problem with $K=9$ arms, of means $0.1,0.2,\dots,0.8,0.9$, and three different cases of $M=3,6,9$ players, for $100$ independent repetitions and an horizon of $T=10000$.
We include the \Selfish-\klUCB{} algorithm as an heuristic, as well as the Improved \MusicalChair{} algorithm from \cite{LugosiMehrabian18} and \textsc{Sic-MMAB} from \cite{BoursierPerchet18}.


\begin{table}[ht]
    % \begin{footnotesize}
        \centering
        \begin{tabular}{c|ccc}
        \textbf{Algorithms} $\;$ \textbackslash $\;$ \textbf{Number of players} & $M=3$ & $M=6$ & $M=9$ \\
            \hline
            \Selfish-\klUCB{} & $\mathbf{XXX \pm XXX}$ & $\mathbf{XXX \pm XXX}$ & $\mathbf{XXX \pm XXX}$ \\
            \hline
            Improved \MusicalChair{} & $\mathbf{XXX \pm XXX}$ & $\mathbf{XXX \pm XXX}$ & $\mathbf{XXX \pm XXX}$ \\
            \hline
            \textsc{Sic-MMAB} & $\mathbf{XXX \pm XXX}$ & $\mathbf{XXX \pm XXX}$ & $\mathbf{XXX \pm XXX}$ \\
            \hline
        \end{tabular}
        \caption{Mean regret $\pm$ $1$ std-dev, for different algorithms on the same problem with $M=3,6,9$, for the ``no sensing'' case.}
        \label{table:5:meanRegretSimulationsNoSensing}
    % \end{footnotesize}
    \end{table}

\TODOL{Give interpretation from these results!}


% ----------------------------------------------------------------------------
\subsection{With communication or coordination between players}
\label{sub:5:withCommunicationOrCoordination}

In all the models of IoT networks considered in this thesis, we assume that \emph{the different IoT devices cannot communicate with each other}, and can only communicate with a unique gateway.
Of course, this hypothesis can be removed, and in practice in some families of wireless networks, communication between players are possible.
Note that this extension is not considering a graph of distributed agents all playing cooperatively to solve a unique bandit game, like it is studied in the ``graph bandit'' problem.
What we are interested here is an extension of our model where players can send some bits to one or all the other players, at every time steps or some time steps. Clearly, the problem is easier by allowing communication, and it is quite immediate to see that the communication capacity between players must be limited otherwise the problem becomes trivial to solve.
Indeed, imagine that at each time step $t$, all the $M$ players could share an unbounded number of bits with the other players, at no cost, even if this hypothesis is clearly unrealistic for wireless networks.
Then they can share all their observations, and they can all run the same multi-plays MAB algorithm \cite{Anantharam87a} (in a deterministic way), like for instance the extension of Thompson sampling for multi-plays studied in \cite{Komiyama15}, or extensions of \KLUCB{} from \cite{Luedtke16}.
In this setting, a logarithmic regret is obtained easily, and the regret upper-bound of the two aforementioned algorithms asymptotically achieve the lower-bound from \cite{Anantharam87a}.

An interesting extension is thus to limit the communication between players, either to a small number of bits, or just one bit.
We do not review papers that study this model, because the recent work \cite{BoursierPerchet18} introduced the ``communication trick'', that essentially allow any player to send one bit to all the others, at some pre-agreed time steps, with no modification on the model.
Thanks to this ``communication trick'', recent research efforts have been more focus on studying the basic model --without explicit communications between players--, while proposing algorithms that can rely on some communication between players.
This trick essentially use the fact that the players share a synchronized time, thus they can use a collision as a way to directly exchange one bit of information between two players.
This process is slow and not efficient, as all but two players must do nothing when player $i$ is sending a bit to player $j$, but it does build a communication protocol in the model without explicit communication between players.
This ``communication trick'' is used for instance in \cite{KaufmannAbbas19}.
We let the interested reader refer to these last two works \cite{BoursierPerchet18,KaufmannAbbas19} as they are both solid, and well explained.

Another line of research is to consider models that are closer to realistic wireless communication networks, as it is done in \cite{Avner16,AvnerMannor18}.
Explaining in details their model and proposed solutions would be quite lengthy, and we rather prefer to let the interested reader refer to the later work \cite{AvnerMannor18}.
% where they essentially propose an efficient algorithm for this problem, that also reach a logarithmic regret when $M \leq K$ players are running independently the same algorithm.
\TODOL{FIXME don't copy/paste their abstract...}
We sum-up its contributions quickly.
They we address several aspects of the challenge of communication networks shared by many users simultaneously: learning unknown stochastic network characteristics, sharing resources with other users while keeping coordination overhead to a minimum.
The proposed solution combines Multi-Armed Bandit learning with a lightweight signalling-based coordination scheme, and ensures convergence to a stable allocation of resources.
They work considers single-user level algorithms for two scenarios:an unknown fixed number of users, and a dynamic number of users.
Analytic performance guarantees, proving convergence to stable marriage configurations, are presented for both setups. The algorithms are designed based on a system-wide perspective, rather than focusing on single user welfare. Thus, maximal re-source utilization is ensured.

\TODOL{A conclusion from this sub-section presenting this model?}


% ----------------------------------------------------------------------------
\subsection{Arrival and departures of players: the ``dynamic setting''}
\label{sub:5:arrivalDepartures}

As reminded above, our model assumes the number of players $M$ remain fixed during all the bandit game.
However, in real-world wireless networks, when players model communicating devices connected to a single gateway, devices can arrive or leave the network at any time.
The existing previous work on multi-player bandit models are all motivated by possible applications to wireless networks, but most of them assume $M$ to be fixed.
The first work studying the relaxation of this hypothesis is \cite{Rosenski16}, where this case is called the ``dynamic setting''.
If the arrival or departures of players is not random, but determined in advance while still being unknown to any player, the natural notion of regret is the following, where the expectancy $\E[\bullet]$ is capturing the randomness in the rewards (\ie, sensing) as well as in the players' decisions (\ie, collisions):
\begin{equation}\label{eq:5:defRegretDynamicSetting}
    R_T^{\text{dyn}} = \sum_{t=1}^T \sum_{k=1}^{M(t)} \mu^*_{k} - \E\Bigl[ \sum_{t=1}^T \sum_{j=1}^{M(t)} r^j(t) \Bigr].
\end{equation}

In \cite{Rosenski16}, the authors explain that if the model allows arrival or departures of players at \emph{any time step}, then the game is much harder, and sub-linear regret is most likely un-achievable, if we consider the natural extension of the definition of regret as in our model presented in Section~\ref{sec:5:model} above.
However, this negative results depends on how arrival of players are modeled:
if an arriving player has no prior memory of the previous observations, \ie, if it model a new device, then if there is no restriction on the number or frequency of arrival or departures, we can most likely prove that sub-linear regret is not achievable in general.
A simple but extreme example shows that regret has to be linear for any bandit strategy (in the observation model with sensing).
Consider $K=2$ Bernoulli-distributed arms of means $\mu_1=\mu_2=1/2$, and one player is always active and play any bandit strategy (\eg, \MCTopM-\klUCB).
Every time step, another player is either arriving, without any knowledge of the problem (\eg, it is a fresh and new IoT device). Then no matter the strategy of these news players, even if they all play \MCTopM-\klUCB{} for instance, if they player a uniformly-efficient strategy there is a non-zero probability that player $1$ suffer from a collision at each time step, thus resulting in linear centralized system regret.

In \cite{Rosenski16}, it is assumed that the players all know a lower-bound $L$ on the length of all the intervals during which arrival or departures of players are allowed.
A naive idea, if $L$ is large enough, is simply to restart the underlying algorithm every $L$ time steps, in order to directly benefit from the theoretical guarantees of the ``static setting'' (where $M$ stays constant).
However, this idea yields a large regret if the length of ``static'' intervals $L$ is too small.
Applied to the \MusicalChair{} algorithm, the authors in \cite{Rosenski16} call this basic extension Dynamic \MusicalChair
Under the simple hypothesis that the overall number of players entering and leaving the game is sub-linear in $T$ (\ie, a $\smallO{T}$), they analyze the regret of Dynamic \MusicalChair{} and prove it is also sub-linear (see Theorem~2 in Section~3.4).
%
Under the same hypothesis, and if the lower-bound $L$ is known before-hand and is large enough, then we could also apply the same idea as from \cite{Rosenski16} to our approach. We believe that we could easily prove that a sub-linear regret is achievable, if all players run \MCTopM-\klUCB{} (and if currently active players restart their memory of the past observations every $L$ time steps).
As the regret guarantees are stronger (\ie, smaller regret upper-bound) for \MCTopM{} than \MusicalChair, we also believe that in this ``slowly varying'' dynamic setting, using the idea of Dynamic \MusicalChair{} from \cite{Rosenski16} to our proposal \MCTopM{} would also give a small regret upper-bound.

After being introduced in \cite{Rosenski16}, some more recent works studied the case of arrival or departures of players, usually referred to as ``dynamic setting'' or ``dynamic case''.
For instance, \cite{BoursierPerchet18} studies in Section~4 the first algorithm proposed for the dynamic case under the no-sensing model.
They study the same notion of regret as the one proposed in \cite{Rosenski16} and given above in \eqref{eq:5:defRegretDynamicSetting}, see Equation $(10)$ in Section~4.2.1.
With the assumption of a non-decreasing number of players, \ie, if only arrivals of players are considered,
they prove a regret upper-bound of the \textsc{Dyn-MMAB} algorithm in Theorem~3 .
% Denote $M(t)$ the number of players at each time $t$, then they propose to study $R_T$ defined as
% \[ R_T = \sum_{t=1}^T \sum_{k=1}^{M(t)} \mu_^*{k} - \sum_{t=1}^T \sum_{j=1}^{M(t)} r^j(t). \]
If all arriving players are using their \textsc{Dyn-MMAB} algorithm, the ``dynamic'' regret is bounded by $\bigO{M^2 K \log(T) / \mu^*_{M}}$, if $M=M(T)$ is the total number of players involved in the problem.

\TODOL{Present the model, quote the main papers, explain the difficulty and how XXX algorithm solves it!}

Note that we have not yet implemented this extension in SMPyBandits \cite{SMPyBandits}, but we are interested to do it, and it is one of the major future work left on our library\footnote{See the issue ticket open at \href{https://github.com/SMPyBandits/SMPyBandits/issues/124}{\texttt{GitHub.com/SMPyBandits/SMPyBandits/issues/124}}}.

\cite{KumarYadav2018}

Distributed Learning and Stable Orthogonalization in Ad-Hoc Networks with Heterogeneous Channels
Sumit J Darak, Manjesh K. Hanawal
https://arxiv.org/pdf/1812.11651
\cite{DarakHanawal18}


% ----------------------------------------------------------------------------
\subsection{With different arm utilities among players}
\label{sub:5:withDifferentMeansAmongPlayers}

\TODOL{Present the model, quote the main papers, explain the difficulty and how XXX algorithm solves it!}

\cite{Bistritz18}

\cite{KaufmannAbbas19}



Note that we have not yet implemented this extension in SMPyBandits \cite{SMPyBandits}, but we are interested to do it, and it is one of the major future work left on our library\footnote{See the issue ticket open at \href{https://github.com/SMPyBandits/SMPyBandits/issues/185}{\texttt{GitHub.com/SMPyBandits/SMPyBandits/issues/185}}}.


Distributed Learning and Stable Orthogonalization in Ad-Hoc Networks with Heterogeneous Channels
Sumit J Darak, Manjesh K. Hanawal
https://arxiv.org/pdf/1812.11651
\cite{DarakHanawal18}


Distributed Learning and Optimal Assignment in Multiplayer Heterogeneous Networks
H Tibrewal, S Patchala, MK Hanawal
https://arxiv.org/pdf/1901.03868.pdf
\cite{Tibrewal2019}
``The channel characteristics are unknown  and  could  be  different  for  each  user  (heterogeneous)''


% ----------------------------------------------------------------------------
\subsection{With malicious jammers}
\label{sub:5:withDifferentMeansAmongPlayers}

\TODOL{Present the model, quote the main papers, explain the difficulty and how XXX algorithm solves it!}

Learning to Coordinate in a Decentralized Cognitive Radio Network in Presence of Jammers
Suneet Sawant, Rohit Kumar, Manjesh K. Hanawal, Sumit J. Darak
https://arxiv.org/abs/1803.06810
\cite{SawantKumar2018}



Note that we have not yet implemented this extension in SMPyBandits \cite{SMPyBandits}, but we are interested to do it, and it is one of the major future work left on our library\footnote{See the issue ticket open at \href{https://github.com/SMPyBandits/SMPyBandits/issues/120}{\texttt{GitHub.com/SMPyBandits/SMPyBandits/issues/120}}}.


% ----------------------------------------------------------------------------
\subsection{Modeling more closely a real wireless network}
\label{sub:5:moreRealisticModels}

\cite{NaparstekCohen17}

Distributed Learning for Channel Allocation Over a Shared Spectrum
S.M. Zafaruddin, Ilai Bistritz, Amir Leshem, Dusit Niyato
https://arxiv.org/pdf/1902.06353
\cite{Zafaruddin2019}


% ----------------------------------------------------------------------------
\subsection{Real-world demonstrations to validate some models}
\label{sub:5:USRPdemos}

Similarly to our demonstration of multi-armed bandit learning in an IoT network that we presented in the previous Chapter~\ref{chapter:4} in Section~\ref{sec:4:gnuradio},
we review in this section some related works, dating back from $2016$, who proposed a similar approach.
All the similar works that we are aware of also used USRP boards, and implemented the demonstration using either the Simulink and MATLAB softwares, or the GNU Radio software like we did \cite{Besson2018ICT}.

\TODOL{Present the model, quote the main papers, explain the difficulty and how XXX algorithm solves it!}

\cite{Darak16}

\cite{modiDemo2016}

\cite{kumar2016two}

\cite{KumarYadav2018}

\cite{SawantKumar2018}

\cite{JoshiKumar2018}


% ----------------------------------------------------------------------------
\subsection{Towards non-stationary multi-players MAB models}
\label{sub:5:towardsNonStationaryModels}

\TODOL{Present the model, quote the main papers, explain the difficulty and how XXX algorithm solves it!}

\cite{WeiSrivastava18Distributed}

\cite{AlaturLevyKrause19}


% ----------------------------------------------------------------------------
\section{Conclusion}
\label{sec:5:conclusion}

In this chapter, we saw...

Future works include...



% ----------------------------------------------------------------------------
\section{Appendix}
\label{sec:5:appendix}

We include here some missing proofs and additional simulation results for this Chapter.

% ----------------------------------------------------------------------------
\input{2-Chapters/5-Chapter/ALT_2018__MPBandits.git/mpBandits_appendix.tex}

