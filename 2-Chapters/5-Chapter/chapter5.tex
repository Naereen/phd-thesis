%!TEX root = ../PhD_thesis__Lilian_Besson

\chapter[Multi-Player Multi-Armed Bandits Models]{Multi-Player Multi-Armed Bandits Models and Algorithms}
\label{chapter:5}
\minitoc


\paragraph{Abstract}

In this Chapter~\ref{chapter:5}, we are interested in a more formal approach to the decentralized learning problem presented in Chapter~\ref{chapter:4}.
We consider $M$ identical dynamic IoT devices, communicating with a unique gateway, in $K$ orthogonal channels and in a acknowledgment-based wireless protocol slotted in time.
A perfect time and frequency synchronization is assumed,
and some \iid{} background traffic is assumed to be non-uniformly in the $K$ channels.
As before, if two or more devices decide to use the same channel at the same time, a \emph{collision} arises and none of sent uplink packet can be received by the gateway.
%
Such networks can be modelled using a decentralized multi-player multi-armed bandit problem, where arms are channels and players are dynamic IoT devices.
Unfortunately, it is very hard to formally analyze the IoT network model we presented in Chapter~\ref{chapter:4}, mainly because of the random activation process of all the dynamic learning devices, Even if there is many identical bandit algorithms learning independently and in a decentralized way, the difficulty mainly comes from the fact that all of them are only communicating at some (random) time steps, and at each time step even the number of communicating devices is random and varies a lot.

Mainly for these two reasons, for this Chapter~\ref{chapter:5} we prefer to only consider the easier case of at most $M \leq K$ devices, communicating at each time step.
Each device will use a Multi-Armed Bandit algorithm, to maximize its number of successful communications, by using the received acknowledgement \Ack{} as a (random) binary reward at each time step.
%
We start by reviewing previous works on multi-player MAB models, which all considered the easier case of \emph{sensing feedback}.
Each time frame is separated as before in a \emph{sensing} phase (during which the device senses for the background traffic),
an \emph{uplink} phase (during which the device sends a packet to the gateway if it sensed the chosen channel to be free),
and a \emph{downlink} phase (during which it waits for an \Ack{} from the gateway).
In this first model, the binary reward is $1$ only if the channel was sensed to be free of background traffic \textbf{and} if \Ack{} was received, and the device has access to both information.
For this first model, we present two algorithms, \RandTopM{} and \MCTopM, based on the combination of an efficient MAB index policy (we chose \klUCB) and a smart orthogonalization procedure, based on a random hoping procedure called Musical Chair.
Like in previous works, we consider the centralized system regret (multi-player regret), or simply referred to as regret.
We start by showing an improved asymptotical regret lower-bound for any algorithm of a certain class, including previous solutions such as \rhoRand{} and our two solutions..
We then analyze our \MCTopM{} algorithm and we show that its regret upper-bound asymptotically matches the improved lower-bound, with a regret bounded by $\bigO{\log(T)}$ for a game at horizon $T$.
We also present extensive numerical simulations that show that our proposal outperforms all previous solutions and is much more efficient in this easier model of sensing feedback with a fixed and known number of players $M$ accessing $K \geq M$ channels.

We are also interested to consider the harder model where devices do not have access to the two feedback information (sensing and \Ack) but only have access to the \Ack.
Our article \cite{Besson2018ALT} was the first to propose this ``no sensing'' model, for which we only proposed an heuristic, the naive \Selfish{} strategy as it was already used in Chapter~\ref{chapter:4}.
Even if empirical simulations showed that \Selfish-\klUCB{} performs very well, it is a mistake to only consider mean regret, as we find on numerical simulations as well as formal derivation on a simple example of $K=M=2$ that the \Selfish{} heuristic can has a linear regret with a low probability (and so asymptotically it has linear regret and fails to solve the no sensing multi-player MAB problem).
We did not propose any other efficient algorithm, but our work presented in April~$2018$ inspired some other articles \cite{LugosiMehrabian18,BoursierPerchet18}.
Two articles confirmed our finding that \Selfish{} has a linear regret, and they all proposed new algorithms.
We do not include numerical simulations to compare all of them, but we present in details the current state-of-the-art of research on multi-player MAB models without sensing.

Finally, another interesting extension of the studied models is to consider different channels utility for each devices. It is a very interesting extension of our model, as for instance the devices could be located on different parts of a building or a field and suffer from different mean qualities of access to each channels.
This extension is not studied per say but we quickly review the existing literature, consisting in the two recent works \cite{Bistritz18,KaufmannAbbas19}, and we discuss the possible real-world usage of the existing algorithms.


\TODOL{Get bibtex of the articles by Sumit J. Darak, and talk correctly about them!}


\vfill{}

\paragraph{Publications}

This chapter is mainly based on the following publication: \cite{Besson2018ALT}.


\newpage
% Write miniTOC just after the title
\graphicspath{{2-Chapters/5-Chapter/Images/}}
\graphicspath{{2-Chapters/5-Chapter/ALT_2018__MPBandits.git/figures/}}

% ----------------------------------------------------------------------------
\input{2-Chapters/5-Chapter/ALT_2018__MPBandits.git/mpBandits.tex}


% ----------------------------------------------------------------------------
% ----------------------------------------------------------------------------
\section{Literature review of many extensions of our models}
\label{sec:5:literatureReviewOtherModels}
% ----------------------------------------------------------------------------

Before concluding this chapter, we give a literature review of the many extensions of the three models presented above in Section~\ref{sec:5:model}.
We wanted to highlight that the community was quite active on research on multi-player bandits in the last $10$ years, but especially active since last spring $2018$.
For instance, our article \cite{Besson2018ALT} was the first to propose the ``no sensing'' case, and it was studied by (at least) three independent group of researchers since its publication in April $2018$ \cite{BoursierPerchet18,LugosiMehrabian18,AvnerMannor18}.
Another example is the model with different arm means among players, who was studied in \cite{Anandkumar10,Kalathil12} and then more recently in \cite{Bistritz18,KaufmannAbbas19}.
%
By following actively the research on multi-player bandit models since the last three years, we believe to be able to give an exhaustive literature review, but it is of course possible that we missed an important research work. In this case, it is our mistake and not an intentional choice as we included all the different extensions of our models that we were aware of.

Many extensions of the simpler model of multi-player bandits have been considered.
The number of players $M$ can be fixed but initially unknown to the players, the sensing information can be absent (like our \modeltrois{} presented above), there could be communications between players (happening at each time step or only occasionally), or $M$ could evolve over time to model the possibility of arrivals or departures of players (\ie, connexions and disconnections of devices in a wireless networks).
Also inspired by real-world wireless networks, the arm means may vary among players, for instance to model players located at different distance of the gateway, and we can also consider networks with some ``jammers'' whose purpose is not to communicate to the gateway in a collaborative way, like the $M$ devices, but to interfere with the communications of the $M$ devices.

Finally, we can also be interested by models where $M$ stays fixed, but the environment evolves, for instance with abrupt changes in the means of arms. This last model makes a good connexion between this chapter and the next one, and an exciting future work is to further study this model in order to tackle this kind of problems by merging our proposals for stationary multi-player bandits and for non-stationary single-player bandits.

\TODOL{
    Pour chaque extension présentée ci dessous, je veux suivre la même organisation :

    1. j'explique l'extension, et en quoi c'est difficile (= en quoi notre approche échoue si on fait rien de plus),

    2. j'explique les travaux existants, et les résultats qu'ils obtiennent e.g. en terme de bornes de regret

    3. si j'ai déjà le code pour, je peux montrer UNE simulation
}


% ----------------------------------------------------------------------------
\subsection{Unknown (fixed) number of players}
\label{sub:5:unknownNumberOfPlayers}

In the model we presented above, we assumed the number of players $M$ to be fixed in time during the learning process.
Without removing this hypothesis (we study this case below in Section~\ref{sub:5:arrivalDepartures}), it is interesting to remark that we made no hypothesis on whether the players can known this value $M$ or not.
Our proposals, \RandTopM{} and \MCTopM, both assume to know $M$ beforehand, like it was done for our main inspiration \RhoRand.


\paragraph{Performance of our proposals for a wrong value of $M$?}
%
We can start by asking whether our most efficient proposal, \MCTopM-\klUCB, is still efficient if it uses a value $M'$ different than the real number of players $M$.
Even though we did not include numerical simulations for this case in Section~\ref{sec:5:experiments} above, we did some tests, which confirmed two disappointing results that were also proved analytically.
First, the three algorithms (\RhoRand, \RandTopM{} and \MCTopM) give linear regret if they are using a value $M'$ strictly smaller than $M$ (and if $\mu^*_{M-1} > \mu^*_M$), as players will converge to play about $T-\bigO{\log(T)}$ times the $M-1$ best arms, leading to at least one collision most of the time, and thus a linear regret.
Second, if the $M$ players falsely use a value $M' > M$, then there is also a certain (fixed) probability to achieve a linear regret. Indeed, imagine one player ($M=1$) running \MCTopM{} with the false knowledge of $M'=2$, and if it learned to accurately identify the set of the $2$ best arms, then the \MCTopM{} orthogonalization scheme can make it play the worst of the two arms, and as $M=1$ this player will never encounter any collision, thus playing this suboptimal arm for about $T-\bigO{\log(T)}$ times, also leading to linear regret.


\paragraph{Interpretation of the hypothesis of knowning $M$ for real-world networks.}
%
% \TODOL{Explain that it does make less sense to know $M$}
One could criticize our approach if we study a wireless networks with no central coordination from the gateway,
in particular where the devices cannot be assigned to a channel or be assigned a unique ID by the gateway when they first log in the network.
Then in such networks, if one wants to apply an algorithm like \MCTopM-\klUCB,
the $M$ devices need to known their number $M$, and have to receive it from the gateway, as it is the only part of this example of network which knowns $M$.
It seems unrealistic to ask the gateway to send the fixed value $M$ to each device, and not a unique ID to each device, for instance $\mathrm{id}^j\in\{1,\dots,K\}$.
%
% \TODOL{Explain why the problem is MUCH easier if we have before hand these unique ID!}
If the $M$ players each have a unique ID, then a simple explore-then-commit algorithm running on top of a round-Robin phase can achieve order-optimal regret. Without giving more details, we let the interested refer to what is is explained for the RR-SW-UCB\# algorithm in \cite{WeiSrivastava18Distributed} and \cite{DarakHanawal18,JoshiKumar2018,KumarDarak2019}.


\paragraph{Literature review}

Some works studied the same model as our model \modeldeux{} ``with sensing'', under the hypothesis that players do not know in the value of $M$.
As illustrated above, if we consider algorithms building on the same ideas as \RhoRand{} or \MCTopM, it seems mandatory to first build an estimate of the value of $M$ then run the initial algorithm that assumed a perfect knowledge of $M$.
Two possible directions exist to estimate on the fly this number.

The first idea comes from \cite{Anandkumar11}, and the intuition behind it is quite simple, even if the mathematical derivations are not.
All players will build an estimate $\hat{M}^j(t)$ of the number of player, that start by $\hat{M}^j(0)=1$. As soon as one collision is observed, a player knows that $M\geq2$, so $\hat{M}^j(t+1) = 2$.
Then, based on probabilistic computations on the expected number of collisions if there were $m$ players, all following the same strategy (\eg, \RhoRand{} in the case of ), and because the formula is simple to compute for different $m$, the authors proposed a statistical test of the hypothesis $M \leq \hat{M}^j(t)$ against $M > \hat{M}^j(t)$ which is expressed as a simple comparison of the current number of collisions (since last update of $\hat{M}^j(t)$) against a threshold.
If a player observed ``too many'' collisions (that are unlikely to be caused by only $\hat{M}^j(t) - 1$ other players), then she increases her current estimate $\hat{M}^j(t+1) = \hat{M}^j(t) + 1$.

The second idea comes from \cite{Rosenski16}, and suppose to know beforehand both the horizon $T$ and a certain measure of the difficulty of the problem (\ie, a lower bound on the minimal gap between two means).
If all the $M$ players start to play for a long enough time $T_0$ uniformly at random among all the $K$ arms, then the (expected) number of collisions observed $\E[\cC^j_{T_0}]$ by any player $j$ is a (relatively simple) function of $M$. By knowing $K$ and $T_0$ and if all players use the same mechanism, then they can invert the formula to obtain the most likely estimate of $M$ which explained the observations of $\cC^j_{T_0}$ collisions.
This second approach works empirically very fine, but it requires a fine tuning of the stopping time $T_0$.
Rosenski et al. studies the performance of their algorithm with high-probability bounds \cite{Rosenski16}, and the tuning of $T_0$ they propose depends on prior knowledge on the problem.
%
For this reason, we are not fond of this approach, as this hypothesis is quite unrealistic if one wants to apply this kind of algorithms for real-world wireless networks.
We note that all the following works assume some sort of prior knowledge on the difficulty of the problem:
\cite{kumar2017channel,KumarYadav2018,SawantKumar2018,JoshiKumar2018,DarakHanawal18,KumarDarak2019,Tibrewal2019}.


\paragraph{Extension of our proposals to learn the value of $M$?}

Similarly to what was proposed for the \rhoRandEst{} algorithm in \cite{Anandkumar11}, we could have worked on proposing and analyzing an extension of our proposals that could efficiently learn the value of $M$ the number of player.
We implemented this \rhoRandEst{} policy in our library SMPyBandits \cite{SMPyBandits}, as well as this mechanism for our proposals \RandTopM{} and \MCTopM.

\TODOL{Include a simulation to show the behavior of \rhoRand{} vs \rhoRandEst, and \RandTopM{} and \MCTopM{} with prior knowledge of $M$ vs the ``estimate'' versions.}



% ----------------------------------------------------------------------------
\subsection{Without sensing information}
\label{sub:5:withoutSensing}

We presented in Section~\ref{sec:5:model} the model \modeltrois, without sensing information. Players can only observe $Y_k(t)$, the product of the \iid{} random sensing information from arm $k$ and of the no-collision indicator.
This model seems harder than the model with sensing information, and even though our proposal \Selfish{} works fine in simulations (in terms of average regret), we proved that it can yield linear regret.
In our paper \cite{Besson2018ALT} as well as the previous sections, it was left as a future work to know if an algorithm can achieve sub-linear regret in this harder model without sensing information.

\TODOL{Present the model, quote the main papers, explain the difficulty and how XXX algorithm solves it!}

Inspired by our article \cite{Besson2018ALT},
Boursier and Perchet studied this question in the summer $2018$ following its publication \cite{BoursierPerchet18}.
Their answer is that the model without sensing information is essentially not harder than the model with sensing information.
%
Similarly, Lugosi and Mehrabian studied the same problem \cite{LugosiMehrabian18}.
In both works, the authors detail algorithms that are unfortunately suboptimal empirically, but give in both cases a logarithmic regret if $M\leqK$ players all independently implement the proposed algorithm.

\paragraph{The ``communication trick''}
%
\TODOL{Write about half a page on the article \cite{BoursierPerchet18}.}


\paragraph{Estimating collisions through uniform exploration}
%
\cite{LugosiMehrabian18} gives a first algorithm with expected regret of $\bigO{M K \log(T) / \Delta^2}$, if $\Delta = \mu^*_{M} - \mu^*_{M+1} \neq 0$, achieving a better dependency regarding $M$ when compared to our result (our bound is $\bigO{M^3}$) but at the cost of (much) larger constants hidden in the $\cO$ notation, and a non-fully explicit algorithms which usually obtain bad (or worse) empirical performance.
Then they study an interesting extension that works also if $\Delta = 0$ or if $\Delta$ is so close than the bound in $1/\Delta^2$ becomes useless for ``small horizons''. This behavior is well known for classical bandits, where bounds of the form $\log(T)/\Delta^2$ become worse than a linear regret $T$ if $\Delta$ is very small (\ie, $\Delta\ll \sqrt{\log(T)/T}$).
For this extension, their proposed algorithm is proved to achieve $\bigO{K^2 M (\log(T))^2 / \mu + K M \min(\sqrt{T \log(T)}, \log(T)/\Delta')}$ regret, if $\mu$ is a lower-bound on $\mu^*_{M}$ and $\Delta' = \max(\Delta, \min\{|\mu^*_M-\mu^*_i| : \mu^*_M > \mu^*_i \})$ (that both have to be known beforehand by the algorithm).
%
The proposed algorithms in \cite{LugosiMehrabian18} are all based on the Musical Chair algorithm from \cite{Rosenski16}, and the curious reader should read their Algorithm~2 (page 15) for more details.

The results presented in \cite{LugosiMehrabian18} are based on some hypotheses, for instance the tuning they propose for the length $g$ of the uniform exploration phase in the beginning of their ``Musical Chair''-like algorithm is based on a prior knowledge of the horizon $T$.
In Section~4 they explain how to relax the assumptions of their results.
For the same example, the ``doubling trick'' technique can be used to obtain a regret upper bound for an algorithm unaware of the value of $T$, within a constant multiplicative factor of the upper bound given for the algorithm aware of $T$ (Section~4.1). Because the regret bound is of the form $\bigO{\sqrt{T} \log(T)}$, a simple ``doubling trick'' of increasing horizons of lengths $T_i = 2^i$ works well, as proposed in \cite{CesaLugosi06} and as studied in depth in our article \cite{Besson2018DoublingTricks}.
They also study in Section~4.3 an extension of the model for the case with more players than arms, but we do not give more details on this aspect. Their definition of a centralized regret for this case is an interesting and natural generalization of our definition, and they also proposed an algorithm achieving $\bigO{M K \log(T) \exp(4M/K) / \Delta^2}$ regret in this case.
Finally, they also proposed in Section~4.4 an extension of their algorithm to estimate the number of players $M$, which is analyzed as for the Musical Chair algorithm in \cite{Rosenski16}, and also achieve logarithmic regret.
Note that it is significantly harder to estimate $M$ without collision information, and Algorithm~3 in page 24 is quite complex.
We did not implement it, and it is an interesting future work to run some numerical simulations in order to validate it empirically.


\paragraph{About \Selfish-\UCB{} inefficiency.}
%
It is also proved in Appendix~A of \cite{BoursierPerchet18} that \Selfish-\UCB{} has a linear regret, in the theoretical case, with a very neat argument from number theory (using Lindemann-Weierstrass theorem). As we conjectured, there is a gap between the theoretical result and its practical consequence, as the Theorem~4 they gave is only valid for real-valued number, and not for hardware-represented floating point number.
Their proof is supporting what we illustrated in Figure~\ref{fig:5:oneGameTree_SelfishKLUCB}.
Their argument is essentially to prove that for \Selfish-powered players using the simple \UCB-indexes, with a probability $p$ at time $t$ (both independent from $T$), two players might have the same number of pulls and the same observed rewards for each arm. In that case, the two players would pull the exact same arms and thus collide for a long time, until they reach a ``tie breaking point'' where they could choose different arms thanks to a random tie breaking rule (\eg, if two values of their \UCB{} indexes are the same, the $\argmax$ is a uniform random choice among the two arms).
They prove that it is unlikely to encounter any of these ``tie breaking points'', in theory if the \UCB{} indexes are real-valued number (that can be rational or irrational).


\subsubsection{Additionnal numerical simulations}

\TODOL{Do some basic simulations and show !}


% ----------------------------------------------------------------------------
\subsection{With communication or coordination between players}
\label{sub:5:withCommunicationOrCoordination}

In all the models of IoT networks considered in this thesis, we assume that the different IoT devices \emph{cannot communicate} with each other, and can only communicate with a unique gateway.
Of course, this hypothesis can be removed, and in practice in some families of wireless networks, communication between players are possible.
Note that this extension is not considering a graph of distributed agents all playing cooperatively to solve a unique bandit game, like it is studied in the ``graph bandit'' problem.
What we are interested here is an extension of our model where players can send some bits to one or all the other players, at every time steps or some time steps. Clearly, the problem is easier by allowing communication, and it is quite immediate to see that the communication capacity between players must be limited otherwise the problem becomes trivial to solve.
Indeed, imagine that at each time step $t$, all the $M$ players could share an unbounded number of bits with the other players, at no cost, even if this hypothesis is clearly unrealistic for wireless networks.
Then they can share all their observations, and they can all run the same multi-plays MAB algorithm \cite{Anantharam87a} (in a deterministic way), like for instance the extension of Thompson sampling for multi-plays studied in \cite{Komiyama15}, or extensions of \KLUCB{} from \cite{Luedtke16}.
In this setting, a logarithmic regret is obtained easily, and the regret upper-bound of the two aforementioned algorithms asymptotically achieve the lower-bound from \cite{Anantharam87a}.

Thus an interesting extension is to limit the communication between players, either to a small number of bits, or just one bit, 
This extension is studied in \cite{Avner16,AvnerMannor18}, where they essentially propose an efficient algorithm for this problem, that also reach a logarithmic regret when $M \leq K$ players are running independently the same algorithm.

\TODOL{Present the model, quote the main papers, explain the difficulty and how XXX algorithm solves it!}


% ----------------------------------------------------------------------------
\subsection{Arrival and departures of players}
\label{sub:5:arrivalDepartures}

\TODOL{Present the model, quote the main papers, explain the difficulty and how XXX algorithm solves it!}

\cite{KumarYadav2018}

Distributed Learning and Stable Orthogonalization in Ad-Hoc Networks with Heterogeneous Channels
Sumit J Darak, Manjesh K. Hanawal
https://arxiv.org/pdf/1812.11651
\cite{DarakHanawal18}


% ----------------------------------------------------------------------------
\subsection{With different arm utilities among players}
\label{sub:5:withDifferentMeansAmongPlayers}

\TODOL{Present the model, quote the main papers, explain the difficulty and how XXX algorithm solves it!}

\cite{Bistritz18}

\cite{KaufmannAbbas19}


Distributed Learning and Stable Orthogonalization in Ad-Hoc Networks with Heterogeneous Channels
Sumit J Darak, Manjesh K. Hanawal
https://arxiv.org/pdf/1812.11651
\cite{DarakHanawal18}


Distributed Learning and Optimal Assignment in Multiplayer Heterogeneous Networks
H Tibrewal, S Patchala, MK Hanawal
https://arxiv.org/pdf/1901.03868.pdf
\cite{Tibrewal2019}
``The channel characteristics are unknown  and  could  be  different  for  each  user  (heterogeneous)''


% ----------------------------------------------------------------------------
\subsection{With malicious jammers}
\label{sub:5:withDifferentMeansAmongPlayers}

\TODOL{Present the model, quote the main papers, explain the difficulty and how XXX algorithm solves it!}

Learning to Coordinate in a Decentralized Cognitive Radio Network in Presence of Jammers
Suneet Sawant, Rohit Kumar, Manjesh K. Hanawal, Sumit J. Darak 
https://arxiv.org/abs/1803.06810
\cite{SawantKumar2018}


% ----------------------------------------------------------------------------
\subsection{Modeling more closely a real wireless network}
\label{sub:5:moreRealisticModels}

\cite{NaparstekCohen17}

Distributed Learning for Channel Allocation Over a Shared Spectrum
S.M. Zafaruddin, Ilai Bistritz, Amir Leshem, Dusit Niyato
https://arxiv.org/pdf/1902.06353
\cite{Zafaruddin2019}


% ----------------------------------------------------------------------------
\subsection{Real-world demonstrations to validate some models}
\label{sub:5:USRPdemos}

Similarly to our demonstration of multi-armed bandit learning in an IoT network that we presented in the previous Chapter~\ref{chapter:4} in Section~\ref{sec:4:gnuradio},
we review in this section some related works, dating back from $2016$, who proposed a similar approach.
All the similar works that we are aware of also used USRP boards, and implemented the demonstration using either the Simulink and MATLAB softwares, or the GNU Radio software like we did \cite{Besson2018ICT}.

\TODOL{Present the model, quote the main papers, explain the difficulty and how XXX algorithm solves it!}

\cite{Darak16}

\cite{modiDemo2016}

\cite{kumar2016two}

\cite{KumarYadav2018}

\cite{SawantKumar2018}

\cite{JoshiKumar2018}


% ----------------------------------------------------------------------------
\subsection{Towards non-stationary multi-players MAB models}
\label{sub:5:towardsNonStationaryModels}

\TODOL{Present the model, quote the main papers, explain the difficulty and how XXX algorithm solves it!}

\cite{WeiSrivastava18Distributed}

\cite{AlaturLevyKrause19}


% ----------------------------------------------------------------------------
\section{Conclusion}
\label{sec:5:conclusion}

In this chapter, we saw...

Future works include...



% ----------------------------------------------------------------------------
\section{Appendix}
\label{sec:5:appendix}

We include here some missing proofs and additional simulation results for this Chapter.

% ----------------------------------------------------------------------------
\input{2-Chapters/5-Chapter/ALT_2018__MPBandits.git/mpBandits_appendix.tex}

