%!TEX root = ../PhD_thesis__Lilian_Besson

\chapter[Multi-Player Multi-Armed Bandits Models]{Multi-Player Multi-Armed Bandits Models and Algorithms}
\label{chapter:5}
\minitoc


\paragraph{Abstract}

In this Chapter~\ref{chapter:5}, we are interested in a more formal approach to the decentralized learning problem presented in Chapter~\ref{chapter:4}.
We consider $M$ identical dynamic IoT devices, communicating with a unique gateway, in $K$ orthogonal channels and in a acknowledgment-based wireless protocol slotted in time.
A perfect time and frequency synchronization is assumed,
and some \iid{} background traffic is assumed to be non-uniformly in the $K$ channels.
As before, if two or more devices decide to use the same channel at the same time, a \emph{collision} arises and none of sent uplink packet can be received by the gateway.
%
Such networks can be modelled using a decentralized multi-player multi-armed bandit problem, where arms are channels and players are dynamic IoT devices.
Unfortunately, it is very hard to formally analyze the IoT network model we presented in Chapter~\ref{chapter:4}, mainly because of the random activation process of all the dynamic learning devices, Even if there is many identical bandit algorithms learning independently and in a decentralized way, the difficulty mainly comes from the fact that all of them are only communicating at some (random) time steps, and at each time step even the number of communicating devices is random and varies a lot.

Mainly for these two reasons, for this Chapter~\ref{chapter:5} we prefer to only consider the easier case of at most $M \leq K$ devices, communicating at each time step.
Each device will use a Multi-Armed Bandit algorithm, to maximize its number of successful communications, by using the received acknowledgement \Ack{} as a (random) binary reward at each time step.
%
We start by reviewing previous works on multi-player MAB models, which all considered the easier case of \emph{sensing feedback}.
Each time frame is separated as before in a \emph{sensing} phase (during which the device senses for the background traffic),
an \emph{uplink} phase (during which the device sends a packet to the gateway if it sensed the chosen channel to be free),
and a \emph{downlink} phase (during which it waits for an \Ack{} from the gateway).
In this first model, the binary reward is $1$ only if the channel was sensed to be free of background traffic \textbf{and} if \Ack{} was received, and the device has access to both information.
For this first model, we present two algorithms, \RandTopM{} and \MCTopM, based on the combination of an efficient MAB index policy (we chose \klUCB) and a smart orthogonalization procedure, based on a random hoping procedure called Musical Chair.
Like in previous works, we consider the centralized system regret (multi-player regret), or simply referred to as regret.
We start by showing an improved asymptotical regret lower-bound for any algorithm of a certain class, including previous solutions such as \rhoRand{} and our two solutions..
We then analyze our \MCTopM{} algorithm and we show that its regret upper-bound asymptotically matches the improved lower-bound, with a regret bounded by $\bigO{\log(T)}$ for a game at horizon $T$.
We also present extensive numerical simulations that show that our proposal outperforms all previous solutions and is much more efficient in this easier model of sensing feedback with a fixed and known number of players $M$ accessing $K \geq M$ channels.

We are also interested to consider the harder model where devices do not have access to the two feedback information (sensing and \Ack) but only have access to the \Ack.
Our article \cite{Besson2018ALT} was the first to propose this ``no sensing'' model, for which we only proposed an heuristic, the naive \Selfish{} strategy as it was already used in Chapter~\ref{chapter:4}.
Even if empirical simulations showed that \Selfish-\klUCB{} performs very well, it is a mistake to only consider mean regret, as we find on numerical simulations as well as formal derivation on a simple example of $K=M=2$ that the \Selfish{} heuristic can has a linear regret with a low probability (and so asymptotically it has linear regret and fails to solve the no sensing multi-player MAB problem).
We did not propose any other efficient algorithm, but our work presented in April~$2018$ inspired some other articles \cite{LugosiMehrabian18,BoursierPerchet18}.
Two articles confirmed our finding that \Selfish{} has a linear regret, and they all proposed new algorithms.
We do not include numerical simulations to compare all of them, but we present in details the current state-of-the-art of research on multi-player MAB models without sensing.

Finally, another interesting extension of the studied models is to consider different channels utility for each devices. It is a very interesting extension of our model, as for instance the devices could be located on different parts of a building or a field and suffer from different mean qualities of access to each channels.
This extension is not studied per say but we quickly review the existing literature, consisting in the two recent works \cite{Bistritz18,KaufmannAbbas19}, and we discuss the possible real-world usage of the existing algorithms.


\TODOL{Get bibtex of the articles by Sumit J. Darak, and talk correctly about them!}


\vfill{}

\paragraph{Publications}

This chapter is mainly based on the following publication: \cite{Besson2018ALT}.


\newpage
% Write miniTOC just after the title
\graphicspath{{2-Chapters/5-Chapter/Images/}}
\graphicspath{{2-Chapters/5-Chapter/ALT_2018__MPBandits.git/figures/}}

% ----------------------------------------------------------------------------
\input{2-Chapters/5-Chapter/ALT_2018__MPBandits.git/mpBandits.tex}


% ----------------------------------------------------------------------------
% ----------------------------------------------------------------------------
\section{Literature review of many extensions of our models}
\label{sec:5:literatureReviewOtherModels}
% ----------------------------------------------------------------------------

We conclude this chapter with a literature review of the many extensions of the three models presented above in Section~\ref{sec:5:model}.
We wanted to highlight that the community was quite active on research on multi-player bandits in the last $10$ years, but especially active since last spring $2018$.
For instance, our article \cite{Besson2018ALT} was the first to propose the ``no sensing'' case, and it was studied by (at least) three independent group of researchers since its publication in April $2018$ \cite{BoursierPerchet18,LugosiMehrabian18,AvnerMannor18}.
Another example is the model with different ear means among players, who was studied first by \cite{Bistritz18} and shortly after by \cite{KaufmannAbbas19}.
%
By following actively the research on multi-player bandit models since the last three years, we believe to be able to give an exhaustive literature review, but it is of course possible that we missed an important research work. In this case, it is our mistake and not an intentional choice as we included all the different extensions of our models that we were aware of.

% \TODOL{Just a small sentence saying that we present here all the extensions of our models that we are aware of blabla}

Many extensions of the simpler model of multi-player bandits have been considered.
The number of players $M$ can be fixed but initially unknown to the players, the sensing information can be absent (like our \modeltrois{} presented above), there could be communications between players (happening at each time step or only occasionally), or $M$ could evolve over time to model the possibility of arrivals or departures of players (\ie, connexions and disconnections of devices in a wireless networks).
Also inspired by real-world wireless networks, the arm means may vary among players, for instance to model players located at different distance of the gateway, and we can also consider networks with some ``jammers'' whose purpose is not to communicate to the gateway in a collaborative way, like the $M$ devices, but to interfere with the communications of the $M$ devices.

Finally, we can also be interested by models where $M$ stays fixed, but the environment evolves, for instance with abrupt changes in the means of arms. This last model makes a good connexion between this chapter and the next one, and an exciting future work is to further study this model in order to tackle this kind of problems by merging our proposals for stationary multi-player bandits and for non-stationary single-player bandits.

\TODOL{I also need to include additional simulations from the latest research articles, and discussions about the new works that took inspiration in our paper.}

\TODOL{
    Pour chaque extension au modèle ci dessous, je veux suivre la même organisation :

    1. j'explique l'extension, et en quoi c'est difficile (= en quoi notre approche échoue si on fait rien de plus),
    2. j'explique les travaux existants, et les résultats qu'ils obtiennent e.g. en terme de bornes de regret
    3. si j'ai déjà le code pour, je peux montrer UNE simulation
}


% ----------------------------------------------------------------------------
\subsection{Unknown (fixed) number of players}
\label{sub:5:unknownNumberOfPlayers}

In the model we presented above, we assumed the number of players $M$ to be fixed in time during the learning process.
Without removing this hypothesis (we study this case below in Section~\ref{sub:5:arrivalDepartures}), it is interesting to remark that we made no hypothesis on whether the players can known this value $M$ or not.
Our proposals, \RandTopM{} and \MCTopM, both assume to know $M$ beforehand, like it was done for our inspirational algorithms \RhoRand{} and \MusicalChair.


\paragraph{Performance of our proposals for a wrong value of $M$?}
%
We can start by asking whether our efficient proposal, \MCTopM-\klUCB, is still efficient if it uses a value $M'$ different than the real number of players $M$.
Even though we did not include numerical simulations for this case in Section~\ref{sec:5:experiments} above, we did some tests, and we observed two disappointing results.
First, the three algorithms (\RhoRand, \RandTopM{} and \MCTopM) give linear regret if they are using a value $M'$ strictly smaller than $M$ (as soon as $\mu^*_{M-1} > \mu^*_M$), as players will converge to play ($T-\bigO{\log(T)}$ times) the $M-1$ best arms, leading to at least one collision and thus a linear regret.
Second, if they are using $M' > M$, then there is also a certain probability to achieve a linear regret. Indeed, imagine one player ($M=1$) running \MCTopM{} with the false knowledge of $M'=2$, and if it learned to accurately identify the set of the $2$ best arms, then the \MCTopM{} orthogonalization scheme can make it play the worst of the two arms, and as $M=1$ this player will never encounter any collision, thus playing this suboptimal arm for $T-\bigO{\log(T)}$ times, leading to linear regret.


\paragraph{Physical interpretation of the hypothesis of knowning $M$.}
%
% \TODOL{Explain that it does make less sense to know $M$}
One criticism that could be made about our approach is the fact that if we study a wireless networks with no central coordination from the gateway,
in particular where the devices cannot be assigned to a channel or be assigned a unique ID by the gateway when they first log in the network.
Then in such networks, if one wants to apply an algorithm like \MCTopM-\klUCB,
the $M$ devices need to known their number $M$, and have to receive it from the gateway, as it is the only part of this example of network which knowns $M$.
It seems unrealistic to ask the gateway to send the fixed value $M$ to each device, and not a unique ID to each device, for instance $\mathrm{id}^j\in\{1,\dots,K\}$.

\TODOL{Explain why the problem is MUCH easier if we have before hand these unique ID!}


\paragraph{Literature review}

Some works studied the same model as our model \modeldeux{} ``with sensing'', under the hypothesis that players do not know in the value of $M$.


\paragraph{Extension of our proposals to learn the value of $M$?}

Similarly to what was proposed for the \rhoRandEst{} algorithm in \cite{Anandkumar11}, we could have worked on proposing and analyzing an extension of our proposals that could efficiently learn the value of $M$ the number of player.
Two possible directions exist to estimate on the fly this number.

The first idea comes from \cite{Anandkumar11}, and is quite simple. All players will build an estimate $\hat{M}^j(t)$ of the number of player, that start by $\hat{M}^j(0)=1$. As soon as one collision is observed, a player knows that $M\geq2$, so $\hat{M}^j(t+1) = 2$.
Then, based on probabilistic computations on the expected number of collisions if there were $m$ players, all following the same strategy (\eg, \RhoRand{} in the case of ), and because the formula is simple to compute for different $m$, the authors proposed a statistical test of the hypothesis $M \leq \hat{M}^j(t)$ against $M > \hat{M}^j(t)$ which is expressed as a simple comparison of the current number of collisions (since last update of $\hat{M}^j(t)$) against a threshold.
If a player observed ``too many'' collisions (that are unlikely to be caused by only $\hat{M}^j(t) - 1$ other players), then she increases her current estimate $\hat{M}^j(t+1) = \hat{M}^j(t) + 1$.
We implemented this \rhoRandEst{} policy in our library SMPyBandits \cite{SMPyBandits}, as well as this mechanism for our proposals \RandTopM{} and \MCTopM.

The second idea comes from \cite{Rosenski16}, and suppose to know beforehand both the horizon $T$ and a certain measure of the difficulty of the problem (\ie, a lower bound on the minimal gap between two means).
If all the $M$ players start to play for a long enough time $T_0$ uniformly at random among all the $K$ arms, then the (expected) number of collisions observed $\E[\cC^j_{T_0}]$ by any player $j$ is a (relatively simple) function of $M$. By knowing $K$ and $T_0$ and if all players use the same mechanism, then they can invert the formula to obtain the most likely estimate of $M$ which explained the observations of $\cC^j_{T_0}$ collisions.
This second approach works empirically very fine, but it requires a fine tuning of the stopping time $T_0$.
Rosenski et al. studies the performance of their algorithm with high-probability bounds \cite{Rosenski16}, and the tuning of $T_0$ they propose depends on prior knowledge on the problem.

\TODOL{Include a simulation to show the behavior of \rhoRand{} vs \rhoRandEst, and \RandTopM{} and \MCTopM{} with prior knowledge of $M$ vs the ``estimate'' versions.}



% ----------------------------------------------------------------------------
\subsection{Without sensing information}
\label{sub:5:withoutSensing}

We presented in Section~\ref{sec:5:model} the model \modeltrois, without sensing information. Players can only observe $Y_k(t)$, the product of the \iid{} random sensing information from arm $k$ and of the no-collision indicator.
This model is clearly harder than the model with sensing information, and even though our proposal \Selfish{} works fine in simulations (in terms of average regret), we proved that it can yield linear regret.
In our paper \cite{Besson2018ALT} as well as the previous sections, it was left as a future work to know if an algorithm can achieve sub-linear regret in this harder model without sensing information.

\TODOL{Present the model, quote the main papers, explain the difficulty and how XXX algorithm solves it!}

Inspired by our article \cite{Besson2018ALT}, Boursier and Perchet studied this question in the summer $2018$ following its publication \cite{BoursierPerchet18}.
Their answer is that the model without sensing information is essentially not harder than with sensing information.

\paragraph{About \Selfish-\UCB{} inefficiency.}
%
They also prove in Appendix~A \cite{BoursierPerchet18} that \Selfish-\UCB{} has a linear regret, in the theoretical case, with a very neat argument from number theory (using Lindemann-Weierstrass theorem). As we conjectured, there is a gap between the theoretical result and its practical consequence, as the Theorem~4 they gave is only valid for real-valued number, and not for hardware-represented floating point number.
Their proof is supporting what we illustrated in Figure~\ref{fig:5:oneGameTree_SelfishKLUCB}.
Their argument is essentially to prove that for \Selfish-powered players using the simple \UCB-indexes, with a probability $p$ at time $t$ (both independent from $T$), two players might ahve the same number of pulls and the same observed rewards for each arm. In that case, the two players would pull the exact same arms and thus collide for a long time, until they reach a ``tie breaking point'' where they could choose different arms thanks to a random tie breaking rule (\eg, if two values of their \UCB{} indexes are the same, the $\argmax$ is a uniform random choice among the two arms).
They prove that it is unlikely to encounter any of these ``tie breaking points'', in theory if the \UCB{} indexes are real-valued number (that can be rational or irrational).


\subsubsection{Additionnal numerical simulations}

\TODOL{Do some basic simulations and show !}


% ----------------------------------------------------------------------------
\subsection{With communication or coordination between players}
\label{sub:5:withCommunicationOrCoordination}

\TODOL{Present the model, quote the main papers, explain the difficulty and how XXX algorithm solves it!}

Multi-user Communication Networks: A Coordinated Multi-armed Bandit Approach
O Avner, S Mannor - arXiv preprint arXiv:1808.04875, 2018 - arxiv.org
\cite{AvnerMannor18}


% ----------------------------------------------------------------------------
\subsection{Arrival and departures of players}
\label{sub:5:arrivalDepartures}

\TODOL{Present the model, quote the main papers, explain the difficulty and how XXX algorithm solves it!}

\cite{KumarYadav2018}

Distributed Learning and Stable Orthogonalization in Ad-Hoc Networks with Heterogeneous Channels
Sumit J Darak, Manjesh K. Hanawal
https://arxiv.org/pdf/1812.11651
\cite{DarakHanawal18}


% ----------------------------------------------------------------------------
\subsection{With different arm utilities among players}
\label{sub:5:withDifferentMeansAmongPlayers}

\TODOL{Present the model, quote the main papers, explain the difficulty and how XXX algorithm solves it!}

\cite{Bistritz18}

\cite{KaufmannAbbas19}


Distributed Learning and Stable Orthogonalization in Ad-Hoc Networks with Heterogeneous Channels
Sumit J Darak, Manjesh K. Hanawal
https://arxiv.org/pdf/1812.11651
\cite{DarakHanawal18}


Distributed Learning and Optimal Assignment in Multiplayer Heterogeneous Networks
H Tibrewal, S Patchala, MK Hanawal
https://arxiv.org/pdf/1901.03868.pdf
\cite{Tibrewal2019}
``The channel characteristics are unknown  and  could  be  different  for  each  user  (heterogeneous)''


% ----------------------------------------------------------------------------
\subsection{With malicious jammers}
\label{sub:5:withDifferentMeansAmongPlayers}

\TODOL{Present the model, quote the main papers, explain the difficulty and how XXX algorithm solves it!}

Learning to Coordinate in a Decentralized Cognitive Radio Network in Presence of Jammers
Suneet Sawant, Rohit Kumar, Manjesh K. Hanawal, Sumit J. Darak 
https://arxiv.org/abs/1803.06810
\cite{SawantKumar2018}


% ----------------------------------------------------------------------------
\subsection{Modeling more closely a real wireless network}
\label{sub:5:moreRealisticModels}

\cite{NaparstekCohen17}

Distributed Learning for Channel Allocation Over a Shared Spectrum
S.M. Zafaruddin, Ilai Bistritz, Amir Leshem, Dusit Niyato
https://arxiv.org/pdf/1902.06353
\cite{Zafaruddin2019}


% ----------------------------------------------------------------------------
\subsection{Real-world demonstrations to validate some models}
\label{sub:5:USRPdemos}

Similarly to our demonstration of multi-armed bandit learning in an IoT network that we presented in the previous Chapter~\ref{chapter:4} in Section~\ref{sec:4:gnuradio},
we review in this section some related works, dating back from $2016$, who proposed a similar approach.
All the similar works that we are aware of also used USRP boards, and implemented the demonstration using either the Simulink and MATLAB softwares, or the GNU Radio software like we did \cite{Besson2018ICT}.

\TODOL{Present the model, quote the main papers, explain the difficulty and how XXX algorithm solves it!}

\cite{Darak16}

\cite{modiDemo2016}

\cite{kumar2016two}

\cite{KumarYadav2018}

\cite{SawantKumar2018}

\cite{JoshiKumar2018}


% ----------------------------------------------------------------------------
\subsection{Towards non-stationary multi-players MAB models}
\label{sub:5:towardsNonStationaryModels}

\TODOL{Present the model, quote the main papers, explain the difficulty and how XXX algorithm solves it!}

\cite{WeiSrivastava18Distributed}

\cite{AlaturLevyKrause19}


% ----------------------------------------------------------------------------
\section{Conclusion}
\label{sec:5:conclusion}

In this chapter, we saw...

Future works include...



% ----------------------------------------------------------------------------
\section{Appendix}
\label{sec:5:appendix}

We include here some missing proofs and additional simulation results for this Chapter.

% ----------------------------------------------------------------------------
\input{2-Chapters/5-Chapter/ALT_2018__MPBandits.git/mpBandits_appendix.tex}

